{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09062f7",
   "metadata": {},
   "source": [
    "## **Crawling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35e5eb",
   "metadata": {},
   "source": [
    "### Gamebrott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re \n",
    "\n",
    "from bs4 import XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "SITEMAP_INDEX_URL = \"https://gamebrott.com/sitemap_index.xml\"\n",
    "URL_OUTPUT_FILE = \"../data/crawled_urls.txt\" \n",
    "LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "PORTAL_NAME = \"Gamebrott\"\n",
    "\n",
    "output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def log_to_file(message):\n",
    "    \"\"\"Fungsi untuk menulis pesan log ke file dengan timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    print(log_entry.strip())\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "def get_sitemap_number(url):\n",
    "    \"\"\"Mengekstrak nomor dari URL sitemap untuk sorting yang benar.\"\"\"\n",
    "    match = re.search(r'post-sitemap(\\d+)\\.xml', url)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def crawl_xml_sitemap():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk crawling sitemap XML, dengan logging dan penyimpanan append.\n",
    "    \"\"\"\n",
    "    log_to_file(f\"===== Memulai Proses Crawling {PORTAL_NAME} =====\")\n",
    "    \n",
    "    newly_found_urls = []\n",
    "    \n",
    "    try:\n",
    "        log_to_file(f\"Mengambil sitemap index dari: {SITEMAP_INDEX_URL}\")\n",
    "        response_main = requests.get(SITEMAP_INDEX_URL, headers=HEADERS, timeout=15)\n",
    "        response_main.raise_for_status()\n",
    "\n",
    "        # Gunakan \"lxml-xml\" atau \"xml\" dengan lxml sebagai parser\n",
    "        # Jika masih error, gunakan \"html.parser\" sebagai fallback\n",
    "        try:\n",
    "            soup_main = BeautifulSoup(response_main.content, \"lxml-xml\")\n",
    "        except:\n",
    "            log_to_file(\"  -> Mencoba parser alternatif...\")\n",
    "            soup_main = BeautifulSoup(response_main.content, \"html.parser\")\n",
    "        \n",
    "        all_sitemap_links = [loc.text for loc in soup_main.find_all('loc')]\n",
    "        \n",
    "        if not all_sitemap_links:\n",
    "            log_to_file(\"ERROR: Tidak ada tag <loc> yang ditemukan di sitemap index.\")\n",
    "            return\n",
    "\n",
    "        log_to_file(f\"Total sitemap ditemukan di index: {len(all_sitemap_links)}\")\n",
    "        \n",
    "        post_sitemap_links = [url for url in all_sitemap_links if 'post-sitemap' in url]\n",
    "        log_to_file(f\"Menyaring sitemap... Ditemukan {len(post_sitemap_links)} link yang mengandung 'post-sitemap'.\")\n",
    "\n",
    "        post_sitemap_links.sort(key=get_sitemap_number)\n",
    "        \n",
    "        sitemaps_to_process = post_sitemap_links[:3]\n",
    "        log_to_file(f\"Membatasi proses hanya untuk {len(sitemaps_to_process)} sitemap pertama.\")\n",
    "        \n",
    "        for i, sitemap_url in enumerate(sitemaps_to_process, 1):\n",
    "            log_to_file(f\"({i}/{len(sitemaps_to_process)}) Memproses sitemap: {sitemap_url}\")\n",
    "            try:\n",
    "                response_post = requests.get(sitemap_url, headers=HEADERS, timeout=15)\n",
    "                response_post.raise_for_status()\n",
    "\n",
    "                # Gunakan parser yang sama\n",
    "                try:\n",
    "                    soup_post = BeautifulSoup(response_post.content, \"lxml-xml\")\n",
    "                except:\n",
    "                    soup_post = BeautifulSoup(response_post.content, \"html.parser\")\n",
    "                \n",
    "                article_links_in_page = []\n",
    "                url_blocks = soup_post.find_all('url')\n",
    "                \n",
    "                for block in url_blocks:\n",
    "                    loc_tag = block.find('loc')\n",
    "                    if loc_tag:\n",
    "                        article_links_in_page.append(loc_tag.text)\n",
    "                \n",
    "                log_to_file(f\"  -> Ditemukan {len(article_links_in_page)} link artikel (gambar diabaikan).\")\n",
    "                newly_found_urls.extend(article_links_in_page)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                log_to_file(f\"  -> ERROR: Gagal mengambil atau memproses {sitemap_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_to_file(f\"FATAL ERROR: Gagal mengambil sitemap index utama. Proses dihentikan. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    if newly_found_urls:\n",
    "        log_to_file(f\"Menyimpan {len(newly_found_urls)} link baru ke file {URL_OUTPUT_FILE}...\")\n",
    "        try:\n",
    "            with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                for url in newly_found_urls:\n",
    "                    # Menggunakan variabel PORTAL_NAME\n",
    "                    f.write(f\"{PORTAL_NAME};{url}\\n\")\n",
    "            log_to_file(\"Penyimpanan link berhasil.\")\n",
    "        except IOError as e:\n",
    "            log_to_file(f\"ERROR: Gagal menulis ke file {URL_OUTPUT_FILE}: {e}\")\n",
    "    else:\n",
    "        log_to_file(\"Tidak ada link baru yang ditemukan pada sesi crawling ini.\")\n",
    "    \n",
    "    log_to_file(f\"Total link yang didapat pada sesi ini: {len(newly_found_urls)}\")\n",
    "    log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_xml_sitemap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5990a3",
   "metadata": {},
   "source": [
    "### Kotakgames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "START_YEAR = 2025\n",
    "END_YEAR = 2025  # Sama dengan START_YEAR untuk hanya mengambil tahun 2025\n",
    "MAX_LINKS = 500\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "URL_OUTPUT_FILE = \"../data/crawled_urls.txt\"\n",
    "LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "PORTAL_NAME = \"Kotakgame\"\n",
    "\n",
    "output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def log_to_file(message):\n",
    "    # Gunakan format yang kompatibel dengan Windows dan Linux\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    print(log_entry.strip())\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "def get_last_page(year_base_url):\n",
    "    \"\"\"Fungsi ini sekarang menerima base URL untuk tahun tertentu.\"\"\"\n",
    "    first_page_url = year_base_url + \"1/\"\n",
    "    try:\n",
    "        log_to_file(f\"  -> Mencari halaman terakhir dari: {first_page_url}\")\n",
    "        response = requests.get(first_page_url, headers=HEADERS, timeout=15, verify=False)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        last_page_link = soup.find('a', class_='prevnext', text='LAST »')\n",
    "        \n",
    "        if last_page_link and last_page_link.has_attr('href'):\n",
    "            href = last_page_link['href']\n",
    "            page_number = int(href.strip('/').split('/')[-1])\n",
    "            log_to_file(f\"  -> Halaman terakhir ditemukan: {page_number}\")\n",
    "            return page_number\n",
    "        else:\n",
    "            log_to_file(\"  -> WARNING: Link 'LAST »' tidak ditemukan. Mengasumsikan hanya ada 1 halaman.\")\n",
    "            return 1\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_to_file(f\"  -> ERROR: Tidak bisa mengakses halaman pertama untuk tahun ini. Error: {e}\")\n",
    "        return 0 # Mengembalikan 0 jika gagal, untuk dilewati\n",
    "    except (ValueError, IndexError):\n",
    "        log_to_file(\"  -> FATAL ERROR: Gagal mem-parsing nomor halaman terakhir dari link.\")\n",
    "        return 0\n",
    "\n",
    "def crawl_kotakgame_multi_year(max_links=None):\n",
    "    \"\"\"\n",
    "    Crawl artikel dari Kotakgame.com\n",
    "    \n",
    "    Args:\n",
    "        max_links (int, optional): Jumlah maksimal link yang ingin diambil.\n",
    "                                   Jika None, akan mengambil semua link.\n",
    "    \"\"\"\n",
    "    log_to_file(f\"===== Memulai Proses Crawling {PORTAL_NAME} (Tahun {START_YEAR}-{END_YEAR}) =====\")\n",
    "    if max_links:\n",
    "        log_to_file(f\"Target: Mengambil maksimal {max_links} link\")\n",
    "    \n",
    "    grand_total_urls = []\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        # Cek apakah sudah mencapai limit\n",
    "        if max_links and len(grand_total_urls) >= max_links:\n",
    "            log_to_file(f\"Sudah mencapai limit {max_links} link. Menghentikan crawling.\")\n",
    "            break\n",
    "            \n",
    "        log_to_file(f\"--- Memproses Tahun {year} ---\")\n",
    "        \n",
    "        # Base URL sekarang dinamis berdasarkan tahun\n",
    "        base_url_for_year = f\"https://www.kotakgame.com/berita/index/{year}/0/0/\"\n",
    "        \n",
    "        last_page = get_last_page(base_url_for_year)\n",
    "        if last_page == 0:\n",
    "            log_to_file(f\"Melewati tahun {year} karena gagal mendapatkan info halaman.\")\n",
    "            continue # Lanjut ke tahun berikutnya\n",
    "\n",
    "        urls_this_year = []\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            # Cek limit sebelum memproses halaman baru\n",
    "            if max_links and len(grand_total_urls) >= max_links:\n",
    "                log_to_file(f\"    -> Sudah mencapai limit {max_links} link. Melewati halaman berikutnya.\")\n",
    "                break\n",
    "                \n",
    "            page_url = f\"{base_url_for_year}{page_num}/\"\n",
    "            log_to_file(f\"    ({page_num}/{last_page}) Memproses halaman: {page_url}\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(page_url, headers=HEADERS, timeout=15, verify=False)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                article_links_found = soup.select('div#contenta div.detailfeature h4 a')\n",
    "                \n",
    "                page_urls = []\n",
    "                for link_tag in article_links_found:\n",
    "                    if link_tag.has_attr('href'):\n",
    "                        relative_url = link_tag['href']\n",
    "                        full_url = f\"https://www.kotakgame.com{relative_url}\"\n",
    "                        page_urls.append(full_url)\n",
    "                \n",
    "                # Batasi jumlah URL yang diambil jika sudah mendekati limit\n",
    "                if max_links:\n",
    "                    remaining_links = max_links - len(grand_total_urls)\n",
    "                    if remaining_links < len(page_urls):\n",
    "                        page_urls = page_urls[:remaining_links]\n",
    "                        log_to_file(f\"      -> Membatasi ke {remaining_links} link untuk mencapai target {max_links}\")\n",
    "                \n",
    "                log_to_file(f\"      -> Ditemukan {len(page_urls)} link artikel.\")\n",
    "                urls_this_year.extend(page_urls)\n",
    "                grand_total_urls.extend(page_urls)\n",
    "                \n",
    "                # Cek apakah sudah mencapai limit setelah menambah URL\n",
    "                if max_links and len(grand_total_urls) >= max_links:\n",
    "                    log_to_file(f\"      -> Target {max_links} link tercapai!\")\n",
    "                    break\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                log_to_file(f\"      -> ERROR: Gagal mengambil {page_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        log_to_file(f\"--- Selesai Tahun {year}, ditemukan {len(urls_this_year)} link ---\")\n",
    "\n",
    "    if grand_total_urls:\n",
    "        unique_urls = sorted(list(set(grand_total_urls)))\n",
    "        log_to_file(f\"Menyimpan total {len(unique_urls)} link unik dari semua tahun ke file {URL_OUTPUT_FILE}...\")\n",
    "        try:\n",
    "            with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                for url in unique_urls:\n",
    "                    f.write(f\"{PORTAL_NAME};{url}\\n\")\n",
    "            log_to_file(\"Penyimpanan link berhasil.\")\n",
    "        except IOError as e:\n",
    "            log_to_file(f\"ERROR: Gagal menulis ke file {URL_OUTPUT_FILE}: {e}\")\n",
    "    else:\n",
    "        log_to_file(\"Tidak ada link baru yang ditemukan pada sesi crawling ini.\")\n",
    "    \n",
    "    log_to_file(f\"Total link yang didapat pada sesi ini: {len(grand_total_urls)}\")\n",
    "    log_to_file(f\"Total link unik yang disimpan: {len(unique_urls) if grand_total_urls else 0}\")\n",
    "    log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Panggil dengan parameter max_links\n",
    "    # Ubah nilai MAX_LINKS sesuai kebutuhan, atau set None untuk mengambil semua\n",
    "    crawl_kotakgame_multi_year(max_links=MAX_LINKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6c385",
   "metadata": {},
   "source": [
    "### Indogamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad329f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time \n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "BASE_URL = \"https://indogamers.com/\"\n",
    "CATEGORIES = ['guides', 'pc', 'console', 'mobile'] \n",
    "URL_OUTPUT_FILE = \"../data/crawled_urls.txt\"\n",
    "LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "MAX_LINKS = 500 \n",
    "\n",
    "output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def log_to_file(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    print(log_entry.strip())\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "def crawl_indogamers_deep(max_links=None):\n",
    "    \"\"\"\n",
    "    Crawl artikel dari Indogamers.com\n",
    "    \n",
    "    Args:\n",
    "        max_links (int, optional): Jumlah maksimal link yang ingin diambil.\n",
    "                                   Jika None, akan mengambil semua link.\n",
    "    \"\"\"\n",
    "    log_to_file(\"===== Memulai Proses Crawling Mendalam Indogamers.com =====\")\n",
    "    if max_links:\n",
    "        log_to_file(f\"Target: Mengambil maksimal {max_links} link\")\n",
    "    \n",
    "    total_urls_found_session = 0\n",
    "\n",
    "    for category in CATEGORIES:\n",
    "        # Cek apakah sudah mencapai limit\n",
    "        if max_links and total_urls_found_session >= max_links:\n",
    "            log_to_file(f\"Sudah mencapai limit {max_links} link. Menghentikan crawling.\")\n",
    "            break\n",
    "            \n",
    "        log_to_file(f\"Memulai kategori: '{category}'\")\n",
    "        page_num = 1\n",
    "        urls_per_category = 0\n",
    "        \n",
    "        while True:\n",
    "            # Cek limit sebelum memproses halaman baru\n",
    "            if max_links and total_urls_found_session >= max_links:\n",
    "                log_to_file(f\"  -> Sudah mencapai limit {max_links} link. Melewati kategori '{category}'.\")\n",
    "                break\n",
    "                \n",
    "            page_url = f\"{BASE_URL}{category}?page={page_num}\"\n",
    "            log_to_file(f\"  -> Memproses halaman: {page_url}\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(page_url, headers=HEADERS, timeout=20, verify=False)\n",
    "                \n",
    "                if response.status_code == 404:\n",
    "                    log_to_file(f\"    -> Halaman tidak ditemukan (404). Akhir dari kategori '{category}'.\")\n",
    "                    break\n",
    "\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                \n",
    "                selector = \"div[class*='article_recent__'] div[class*='article_recent_desc__'] h1 a\"\n",
    "                link_tags = soup.select(selector)\n",
    "                \n",
    "                if not link_tags:\n",
    "                    log_to_file(f\"    -> Tidak ada artikel ditemukan. Akhir dari kategori '{category}'.\")\n",
    "                    break\n",
    "                \n",
    "                page_urls = [tag['href'] for tag in link_tags if tag.has_attr('href')]\n",
    "                \n",
    "                # Batasi jumlah URL yang diambil jika sudah mendekati limit\n",
    "                if max_links:\n",
    "                    remaining_links = max_links - total_urls_found_session\n",
    "                    if remaining_links < len(page_urls):\n",
    "                        page_urls = page_urls[:remaining_links]\n",
    "                        log_to_file(f\"    -> Membatasi ke {remaining_links} link untuk mencapai target {max_links}\")\n",
    "                \n",
    "                log_to_file(f\"    -> Ditemukan {len(page_urls)} link artikel.\")\n",
    "                \n",
    "                with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                    for url in page_urls:\n",
    "                        f.write(f\"Indogamers;{url}\\n\")\n",
    "                \n",
    "                urls_per_category += len(page_urls)\n",
    "                total_urls_found_session += len(page_urls)\n",
    "                \n",
    "                # Cek apakah sudah mencapai limit setelah menambah URL\n",
    "                if max_links and total_urls_found_session >= max_links:\n",
    "                    log_to_file(f\"    -> Target {max_links} link tercapai!\")\n",
    "                    break\n",
    "                \n",
    "                page_num += 1 \n",
    "                time.sleep(1) \n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                log_to_file(f\"    -> ERROR: Gagal mengambil {page_url}: {e}. Mencoba lagi dalam 5 detik...\")\n",
    "                time.sleep(5)\n",
    "                continue \n",
    "        \n",
    "        log_to_file(f\"Selesai kategori '{category}'. Total link ditemukan: {urls_per_category}\")\n",
    "\n",
    "    log_to_file(f\"Total link yang didapat pada sesi ini: {total_urls_found_session}\")\n",
    "    log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Panggil dengan parameter max_links\n",
    "    # Ubah nilai MAX_LINKS sesuai kebutuhan, atau set None untuk mengambil semua\n",
    "    crawl_indogamers_deep(max_links=MAX_LINKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c8ef6",
   "metadata": {},
   "source": [
    "### JagatPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re \n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "SITEMAP_INDEX_URL = \"https://jagatplay.com/sitemap.html\"\n",
    "URL_OUTPUT_FILE = \"../data/crawled_urls.txt\"\n",
    "LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "PORTAL_NAME = \"Jagatplay\"\n",
    "MAX_LINKS = 500  # Jumlah maksimal link yang ingin diambil\n",
    "\n",
    "output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def log_to_file(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    print(log_entry.strip())\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "def get_jagatplay_sitemap_number(url):\n",
    "    \"\"\"Mengekstrak nomor dari URL sitemap untuk sorting yang benar.\"\"\"\n",
    "    match = re.search(r'post-sitemap(\\d+)\\.html', url)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def crawl_jagatplay(max_links=None):\n",
    "    \"\"\"\n",
    "    Crawl artikel dari Jagatplay.com\n",
    "    \n",
    "    Args:\n",
    "        max_links (int, optional): Jumlah maksimal link yang ingin diambil.\n",
    "                                   Jika None, akan mengambil semua link.\n",
    "    \"\"\"\n",
    "    log_to_file(f\"===== Memulai Proses Crawling {PORTAL_NAME} =====\")\n",
    "    if max_links:\n",
    "        log_to_file(f\"Target: Mengambil maksimal {max_links} link\")\n",
    "    \n",
    "    all_urls_found = []\n",
    "    \n",
    "    try:\n",
    "        log_to_file(f\"Mengambil sitemap index dari: {SITEMAP_INDEX_URL}\")\n",
    "        response_main = requests.get(SITEMAP_INDEX_URL, headers=HEADERS, timeout=15, verify=False)\n",
    "        response_main.raise_for_status()\n",
    "        soup_main = BeautifulSoup(response_main.content, \"html.parser\")\n",
    "        \n",
    "        sitemap_tags = soup_main.select('tr > td > a')\n",
    "        all_sitemap_links = [tag['href'] for tag in sitemap_tags if tag.has_attr('href')]\n",
    "        \n",
    "        if not all_sitemap_links:\n",
    "            log_to_file(\"ERROR: Tidak ada link sitemap yang ditemukan di halaman index.\")\n",
    "            return\n",
    "\n",
    "        log_to_file(f\"Total sitemap ditemukan di index: {len(all_sitemap_links)}\")\n",
    "        \n",
    "        post_sitemap_links_numbered = [url for url in all_sitemap_links if re.search(r'post-sitemap\\d+\\.html', url)]\n",
    "        \n",
    "        post_sitemap_links_numbered.sort(key=get_jagatplay_sitemap_number)\n",
    "        \n",
    "        sitemap_limit = 11\n",
    "        sitemaps_to_process = [\n",
    "            url for url in post_sitemap_links_numbered \n",
    "            if get_jagatplay_sitemap_number(url) <= sitemap_limit\n",
    "        ]\n",
    "        \n",
    "        log_to_file(f\"Menyaring sitemap... Akan memproses {len(sitemaps_to_process)} sitemap (hingga post-sitemap{sitemap_limit}).\")\n",
    "\n",
    "        for i, sitemap_url in enumerate(sitemaps_to_process, 1):\n",
    "            # Cek apakah sudah mencapai limit\n",
    "            if max_links and len(all_urls_found) >= max_links:\n",
    "                log_to_file(f\"Sudah mencapai limit {max_links} link. Menghentikan crawling.\")\n",
    "                break\n",
    "                \n",
    "            log_to_file(f\"({i}/{len(sitemaps_to_process)}) Memproses sitemap: {sitemap_url}\")\n",
    "            try:\n",
    "                response_post = requests.get(sitemap_url, headers=HEADERS, timeout=15, verify=False)\n",
    "                response_post.raise_for_status()\n",
    "\n",
    "                # Gunakan lxml-xml dengan fallback ke html.parser\n",
    "                try:\n",
    "                    soup_post = BeautifulSoup(response_post.content, \"lxml-xml\")\n",
    "                except:\n",
    "                    soup_post = BeautifulSoup(response_post.content, \"html.parser\")\n",
    "                \n",
    "                article_links = []\n",
    "                url_blocks = soup_post.find_all('url')\n",
    "                for block in url_blocks:\n",
    "                    loc_tag = block.find('loc')\n",
    "                    if loc_tag:\n",
    "                        article_links.append(loc_tag.text)\n",
    "                \n",
    "                # Batasi jumlah URL yang diambil jika sudah mendekati limit\n",
    "                if max_links:\n",
    "                    remaining_links = max_links - len(all_urls_found)\n",
    "                    if remaining_links < len(article_links):\n",
    "                        article_links = article_links[:remaining_links]\n",
    "                        log_to_file(f\"  -> Membatasi ke {remaining_links} link untuk mencapai target {max_links}\")\n",
    "                \n",
    "                log_to_file(f\"  -> Ditemukan {len(article_links)} link artikel.\")\n",
    "                all_urls_found.extend(article_links)\n",
    "                \n",
    "                # Cek apakah sudah mencapai limit setelah menambah URL\n",
    "                if max_links and len(all_urls_found) >= max_links:\n",
    "                    log_to_file(f\"  -> Target {max_links} link tercapai!\")\n",
    "                    break\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                log_to_file(f\"  -> ERROR: Gagal mengambil atau memproses {sitemap_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_to_file(f\"FATAL ERROR: Gagal mengambil sitemap index utama. Proses dihentikan. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    if all_urls_found:\n",
    "        unique_urls = sorted(list(set(all_urls_found)))\n",
    "        log_to_file(f\"Menyimpan total {len(unique_urls)} link unik ke file {URL_OUTPUT_FILE}...\")\n",
    "        try:\n",
    "            with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                for url in unique_urls:\n",
    "                    f.write(f\"{PORTAL_NAME};{url}\\n\")\n",
    "            log_to_file(\"Penyimpanan link berhasil.\")\n",
    "        except IOError as e:\n",
    "            log_to_file(f\"ERROR: Gagal menulis ke file {URL_OUTPUT_FILE}: {e}\")\n",
    "    else:\n",
    "        log_to_file(\"Tidak ada link baru yang ditemukan pada sesi crawling ini.\")\n",
    "    \n",
    "    log_to_file(f\"Total link yang didapat pada sesi ini: {len(all_urls_found)}\")\n",
    "    log_to_file(f\"Total link unik yang disimpan: {len(unique_urls) if all_urls_found else 0}\")\n",
    "    log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Panggil dengan parameter max_links\n",
    "    # Ubah nilai MAX_LINKS sesuai kebutuhan, atau set None untuk mengambil semua\n",
    "    crawl_jagatplay(max_links=MAX_LINKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e109c7",
   "metadata": {},
   "source": [
    "## **Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Comment\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "\n",
    "# Nonaktifkan pesan peringatan SSL\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "CRAWLED_URL_FILE = \"../data/crawled_urls.txt\"\n",
    "OUTPUT_CSV_FILE = \"../data/scraped_articles.csv\"\n",
    "LOG_FILE = \"../data/scrape_logs.txt\"\n",
    "\n",
    "PORTALS_TO_SCRAPE = [\n",
    "    \"Gamebrott\",\n",
    "    \"Kotakgame\",\n",
    "    \"Indogamers\",\n",
    "    \"Jagatplay\"\n",
    "]\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV_FILE), exist_ok=True)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def log_to_file(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    print(log_entry.strip())\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "def read_all_urls_to_scrape(filepath, portal_names):\n",
    "    log_to_file(f\"Membaca SEMUA URL dari {filepath}...\")\n",
    "    all_tasks = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if ';' in line:\n",
    "                    portal, url = line.strip().split(';', 1)\n",
    "                    if portal in portal_names:\n",
    "                        all_tasks.append((portal, url))\n",
    "        log_to_file(f\"Total URL target ditemukan: {len(all_tasks)}\")\n",
    "        return all_tasks\n",
    "    except FileNotFoundError:\n",
    "        log_to_file(f\"ERROR: File {filepath} tidak ditemukan.\")\n",
    "        return []\n",
    "\n",
    "def get_already_scraped_urls(filepath):\n",
    "    \"\"\"Membaca CSV dan mengembalikan set URL yang sudah di-scrape.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        if 'url' in df.columns:\n",
    "            return set(df['url'])\n",
    "        else:\n",
    "            log_to_file(\"WARNING: Kolom 'url' tidak ditemukan di CSV. Tidak bisa melanjutkan. Harap hapus file CSV lama atau tambahkan kolom 'url'.\")\n",
    "            return set()\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        log_to_file(f\"Error saat membaca CSV yang ada: {e}. Mengasumsikan file kosong.\")\n",
    "        return set()\n",
    "\n",
    "def extract_text_with_links(element):\n",
    "    \"\"\"Ekstrak teks dari elemen termasuk tag <a>, lalu bersihkan HTML belakangan.\"\"\"\n",
    "    if not element:\n",
    "        return 'N/A'\n",
    "    \n",
    "    # Ambil semua paragraph\n",
    "    paragraphs = element.find_all('p')\n",
    "    content_parts = []\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        # Ambil teks dengan mempertahankan <a> tag sementara\n",
    "        text = p.get_text(separator=' ', strip=True)\n",
    "        if text:\n",
    "            content_parts.append(text)\n",
    "    \n",
    "    return '\\n'.join(content_parts) if content_parts else 'N/A'\n",
    "\n",
    "def scrape_kotakgame_article(url, soup):\n",
    "    \"\"\"Scrape artikel dari Kotakgame\"\"\"\n",
    "    title = 'N/A'\n",
    "    thumbnail_url = 'N/A'\n",
    "    publish_date = 'N/A'\n",
    "    content = 'N/A'\n",
    "    \n",
    "    # Judul - ada di class bagiankiri > h3.judulh3\n",
    "    bagian_kiri = soup.select_one('.bagiankiri')\n",
    "    if bagian_kiri:\n",
    "        title_tag = bagian_kiri.select_one('h3.judulh3')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "    \n",
    "    # Thumbnail - ada di wrapimg > img src\n",
    "    thumb_tag = soup.select_one('.wrapimg img')\n",
    "    if thumb_tag and thumb_tag.has_attr('src'):\n",
    "        relative_url = thumb_tag['src']\n",
    "        thumbnail_url = f\"https://www.kotakgame.com{relative_url}\" if relative_url.startswith('/') else relative_url\n",
    "    \n",
    "    # Tanggal - ada di boxwidget > boxcreate > span.txtcreate2\n",
    "    date_span = soup.select_one('.boxwidget .boxcreate .txtcreate2')\n",
    "    if date_span:\n",
    "        publish_date = date_span.get_text(strip=True)\n",
    "    \n",
    "    # Konten - ada di isinewsp, ambil semua <p>\n",
    "    content_div = soup.select_one('.isinewsp')\n",
    "    if content_div:\n",
    "        content = extract_text_with_links(content_div)\n",
    "    \n",
    "    return {\n",
    "        \"judul\": title,\n",
    "        \"konten\": content,\n",
    "        \"tanggal_terbit\": publish_date,\n",
    "        \"url_thumbnail\": thumbnail_url\n",
    "    }\n",
    "\n",
    "def scrape_indogamers_article(url, soup):\n",
    "    title_tag = soup.select_one('h1[class*=\"style_article__title__\"]')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "    thumb_tag = soup.select_one('div[class*=\"style_image__article__\"] img')\n",
    "    if thumb_tag and thumb_tag.has_attr('srcset'):\n",
    "        last_url_part = thumb_tag['srcset'].split(',')[-1].strip()\n",
    "        relative_url = last_url_part.split(' ')[0]\n",
    "        thumbnail_url = f\"https://indogamers.com{relative_url}\"\n",
    "    else: thumbnail_url = 'N/A'\n",
    "    date_container = soup.select_one('div[class*=\"style_author__box__\"]')\n",
    "    publish_date = 'N/A'\n",
    "    if date_container:\n",
    "        all_spans = date_container.find_all('span')\n",
    "        for span in all_spans:\n",
    "            span_text = span.get_text(strip=True)\n",
    "            if re.search(r'\\b(Senin|Selasa|Rabu|Kamis|Jumat|Sabtu|Minggu)\\b', span_text):\n",
    "                publish_date = span_text.split(',')[0].strip()\n",
    "                break\n",
    "    # content_div = soup.find('article[class*=style_content__article__]')\n",
    "    content_div = soup.find('article', class_=re.compile(r'style_content__article___'))\n",
    "    content = content_div.decode_contents() if content_div else 'N/A'\n",
    "    # content = 'N/A'\n",
    "    # if content_div:\n",
    "    #     paragraphs = content_div.find_all('p')\n",
    "    #     content = '\\n'.join([p.get_text(strip=True) for p in paragraphs if not p.has_attr('class') or 'caption' not in ''.join(p['class'])])\n",
    "    return {\"judul\": title, \"konten\": content, \"tanggal_terbit\": publish_date, \"url_thumbnail\": thumbnail_url}\n",
    "\n",
    "def scrape_gamebrott_article(url, soup):\n",
    "    \"\"\"Scrape artikel dari Gamebrott\"\"\"\n",
    "    title = 'N/A'\n",
    "    thumbnail_url = 'N/A'\n",
    "    publish_date = 'N/A'\n",
    "    content = 'N/A'\n",
    "    \n",
    "    # Judul - class post-wrapper > h1.jeg_post_title\n",
    "    post_wrapper = soup.select_one('.post-wrapper')\n",
    "    if post_wrapper:\n",
    "        title_tag = post_wrapper.select_one('h1.jeg_post_title')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "    \n",
    "    # Thumbnail - class jeg_featured.featured_image > thumbnail-container > img src\n",
    "    thumb_tag = soup.select_one('.jeg_featured.featured_image .thumbnail-container img')\n",
    "    if thumb_tag and thumb_tag.has_attr('src'):\n",
    "        thumbnail_url = thumb_tag['src']\n",
    "    \n",
    "    # Tanggal - div.jeg_meta_container > div.jeg_meta_date > a\n",
    "    date_tag = soup.select_one('.jeg_meta_container .jeg_meta_date a')\n",
    "    if date_tag:\n",
    "        publish_date = date_tag.get_text(strip=True)\n",
    "    \n",
    "    # Konten - div.entry-content.no-share > div.content-inner.jeg_link_underline > p\n",
    "    content_div = soup.select_one('.entry-content.no-share .content-inner.jeg_link_underline')\n",
    "    if content_div:\n",
    "        content = extract_text_with_links(content_div)\n",
    "    \n",
    "    return {\n",
    "        \"judul\": title,\n",
    "        \"konten\": content,\n",
    "        \"tanggal_terbit\": publish_date,\n",
    "        \"url_thumbnail\": thumbnail_url\n",
    "    }\n",
    "\n",
    "def scrape_jagatplay_article(url, soup):\n",
    "    \"\"\"Scrape artikel dari Jagatplay\"\"\"\n",
    "    title = 'N/A'\n",
    "    thumbnail_url = 'N/A'\n",
    "    publish_date = 'N/A'\n",
    "    content = 'N/A'\n",
    "    \n",
    "    # Cari div#mainContent dulu\n",
    "    main_content = soup.select_one('div#mainContent')\n",
    "    if not main_content:\n",
    "        main_content = soup  # Fallback ke soup utama\n",
    "    \n",
    "    # Judul - div.jgpost__header > h1\n",
    "    header = main_content.select_one('.jgpost__header')\n",
    "    if header:\n",
    "        title_tag = header.select_one('h1')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "    \n",
    "    # Thumbnail - class jgpost__feat-img, ambil background url dari style\n",
    "    feat_img = main_content.select_one('.jgpost__feat-img')\n",
    "    if feat_img and feat_img.has_attr('style'):\n",
    "        style = feat_img['style']\n",
    "        match = re.search(r\"url\\(['\\\"]?(.*?)['\\\"]?\\)\", style)\n",
    "        if match:\n",
    "            thumbnail_url = match.group(1)\n",
    "    \n",
    "    # Tanggal - div.jgpost__content > div.jgauthor.breakout > div.jgauthor__posted > div\n",
    "    author_posted = main_content.select_one('.jgpost__content .jgauthor.breakout .jgauthor__posted')\n",
    "    if author_posted:\n",
    "        # Cari div yang berisi tanggal (biasanya div terakhir atau yang tidak punya tag <a>)\n",
    "        divs = author_posted.find_all('div', recursive=False)\n",
    "        for div in divs:\n",
    "            if not div.find('a'):  # Div tanpa link biasanya berisi tanggal\n",
    "                publish_date = div.get_text(strip=True)\n",
    "                break\n",
    "    \n",
    "    # Konten - div.jgpost__content > p\n",
    "    content_div = main_content.select_one('.jgpost__content')\n",
    "    if content_div:\n",
    "        content = extract_text_with_links(content_div)\n",
    "    \n",
    "    return {\n",
    "        \"judul\": title,\n",
    "        \"konten\": content,\n",
    "        \"tanggal_terbit\": publish_date,\n",
    "        \"url_thumbnail\": thumbnail_url\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    log_to_file(\"===== Memulai Proses Scraping Skala Penuh (Mode Resume + Real-time Save) =====\")\n",
    "    \n",
    "    all_tasks = read_all_urls_to_scrape(CRAWLED_URL_FILE, PORTALS_TO_SCRAPE)\n",
    "    \n",
    "    header = ['id_dokumen', 'sumber', 'url', 'judul', 'konten', 'tanggal_terbit', 'url_thumbnail']\n",
    "    \n",
    "    already_scraped_urls = get_already_scraped_urls(OUTPUT_CSV_FILE)\n",
    "    if already_scraped_urls:\n",
    "        log_to_file(f\"Ditemukan {len(already_scraped_urls)} URL yang sudah diproses. Akan melanjutkan.\")\n",
    "    \n",
    "    doc_id_counter = len(already_scraped_urls) + 1\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_CSV_FILE) or not already_scraped_urls:\n",
    "        log_to_file(f\"File {OUTPUT_CSV_FILE} tidak ada atau kosong. Membuat file baru dengan header.\")\n",
    "        pd.DataFrame(columns=header).to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "        doc_id_counter = 1\n",
    "    \n",
    "    total_urls_to_process = len(all_tasks)\n",
    "    newly_scraped_count = 0\n",
    "    \n",
    "    for i, (portal, url) in enumerate(all_tasks):\n",
    "        # Lewati URL yang sudah ada\n",
    "        if url in already_scraped_urls:\n",
    "            continue\n",
    "            \n",
    "        log_to_file(f\"  ({i+1}/{total_urls_to_process}) Scraping: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=15, verify=False)\n",
    "            if response.status_code != 200:\n",
    "                log_to_file(f\"    -> Gagal mengakses (Status: {response.status_code})\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            data = None\n",
    "            if portal == \"Gamebrott\":\n",
    "                data = scrape_gamebrott_article(url, soup)\n",
    "            elif portal == \"Kotakgame\":\n",
    "                data = scrape_kotakgame_article(url, soup)\n",
    "            elif portal == \"Indogamers\":\n",
    "                data = scrape_indogamers_article(url, soup)\n",
    "            elif portal == \"Jagatplay\":\n",
    "                data = scrape_jagatplay_article(url, soup)\n",
    "            \n",
    "            if data:\n",
    "                data['id_dokumen'] = f\"doc_{doc_id_counter:05d}\"\n",
    "                data['sumber'] = portal\n",
    "                data['url'] = url\n",
    "                \n",
    "                df_row = pd.DataFrame([data])\n",
    "                df_row = df_row[header]\n",
    "                \n",
    "                df_row.to_csv(OUTPUT_CSV_FILE, mode='a', index=False, header=False)\n",
    "                \n",
    "                log_to_file(f\"    -> Berhasil: {data['judul'][:50]}...\")\n",
    "                \n",
    "                doc_id_counter += 1\n",
    "                newly_scraped_count += 1\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            log_to_file(f\"    -> Terjadi error kritis saat scraping {url}: {e}\")\n",
    "\n",
    "    log_to_file(f\"Scraping selesai. Total {newly_scraped_count} artikel BARU berhasil disimpan ke {OUTPUT_CSV_FILE}.\")\n",
    "    log_to_file(f\"Total artikel keseluruhan: {doc_id_counter - 1}\")\n",
    "    log_to_file(f\"Proses selesai.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d951d",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f690435",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b79c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview Dataframe\n",
      "  id_dokumen     sumber                                                url  \\\n",
      "0  doc_00001  Gamebrott                        https://gamebrott.com/blog/   \n",
      "1  doc_00002  Gamebrott  https://gamebrott.com/microsoft-investigasi-pe...   \n",
      "2  doc_00003  Gamebrott  https://gamebrott.com/amd-klarifikasi-soal-soc...   \n",
      "3  doc_00004  Gamebrott  https://gamebrott.com/detail-trailer-fallout-s...   \n",
      "4  doc_00005  Gamebrott  https://gamebrott.com/kisah-clair-obscur-exped...   \n",
      "\n",
      "                                               judul  \\\n",
      "0                                                NaN   \n",
      "1  Microsoft Investigasi Penyebab Masalah SSD, Be...   \n",
      "2  AMD Klarifikasi Masalah Socket AM5 Terbakar, S...   \n",
      "3  Gamescom 2025 — Detail Trailer Fallout Season ...   \n",
      "4  Kisah Clair Obscur Expedition 33, Mulai dari A...   \n",
      "\n",
      "                                              konten   tanggal_terbit  \\\n",
      "0                                                NaN              NaN   \n",
      "1  Beberapa hari lalu, permasalahan SSD rusak men...  21 Agustus 2025   \n",
      "2  AMD akhirnya buka suara mengenai socket proses...  21 Agustus 2025   \n",
      "3  Detail Trailer Fallout Season 2 – Trailer TV S...  20 Agustus 2025   \n",
      "4  Kisah Clair Obscur: Expedition 33 —  Game perd...  20 Agustus 2025   \n",
      "\n",
      "                                       url_thumbnail  \n",
      "0                                                NaN  \n",
      "1  https://gamebrott.com/wp-content/uploads/2025/...  \n",
      "2  https://gamebrott.com/wp-content/uploads/2025/...  \n",
      "3  https://gamebrott.com/wp-content/uploads/2025/...  \n",
      "4  https://gamebrott.com/wp-content/uploads/2025/...  \n",
      "Statistik Deskriptif Untuk Dataframe Articles\n",
      "       id_dokumen     sumber  \\\n",
      "count        2029       2029   \n",
      "unique       1831          4   \n",
      "top     doc_00448  Gamebrott   \n",
      "freq            2        601   \n",
      "\n",
      "                                                      url  \\\n",
      "count                                                2029   \n",
      "unique                                               1831   \n",
      "top     https://gamebrott.com/director-expedition-33-g...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                                  judul  \\\n",
      "count                                              2020   \n",
      "unique                                             1822   \n",
      "top     OpenAI Rilis GPT-5, Bisa Nalar Seperti Manusia?   \n",
      "freq                                                  2   \n",
      "\n",
      "                                                   konten  tanggal_terbit  \\\n",
      "count                                                2020            2020   \n",
      "unique                                               1822             549   \n",
      "top     OpenAI baru saja mengenalkan AI model terbaru ...  4 Agustus 2025   \n",
      "freq                                                    2              36   \n",
      "\n",
      "                                            url_thumbnail  \n",
      "count                                                2020  \n",
      "unique                                               1722  \n",
      "top     https://indogamers.com/_next/image?url=https%3...  \n",
      "freq                                                    5  \n",
      "Tipe Data Dataframe Articles\n",
      "id_dokumen        object\n",
      "sumber            object\n",
      "url               object\n",
      "judul             object\n",
      "konten            object\n",
      "tanggal_terbit    object\n",
      "url_thumbnail     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_article = pd.read_csv('../data/scraped_articles.csv')\n",
    "print(\"Preview Dataframe\")\n",
    "print(df_article.head())\n",
    "\n",
    "print(\"Statistik Deskriptif Untuk Dataframe Articles\")\n",
    "print(df_article.describe())\n",
    "\n",
    "print(\"Tipe Data Dataframe Articles\")\n",
    "print(df_article.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d704",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbca2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_article.isnull().sum())\n",
    "\n",
    "df_null = df_article.dropna()\n",
    "print(f\"Jumlah baris sebelum dihandling: {df_article.shape[0]}\")\n",
    "print(f\"Jumlah baris setelah dihandling: {df_null.shape[0]}\")\n",
    "\n",
    "df_null.isnull().sum()\n",
    "df_article = df_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4dfc5",
   "metadata": {},
   "source": [
    "### Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b81c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47ed4a",
   "metadata": {},
   "source": [
    "### Date Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sources = df_article['sumber'].unique()\n",
    "sample_rows = []\n",
    "\n",
    "for source in unique_sources:\n",
    "    sample = df_article[df_article['sumber'] == source].head(1)\n",
    "    sample_rows.append(sample)\n",
    "\n",
    "if sample_rows:\n",
    "    temp_df = pd.concat(sample_rows)\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(temp_df[['sumber', 'tanggal_terbit']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb56deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_article \n",
    "\n",
    "# Dictionary bulan Inggris ke Indonesia\n",
    "bulan_dict = {\n",
    "    'January': 'Januari', 'February': 'Februari', 'March': 'Maret', 'April': 'April',\n",
    "    'May': 'Mei', 'June': 'Juni', 'July': 'Juli', 'August': 'Agustus', 'September': 'September',\n",
    "    'October': 'Oktober', 'November': 'November', 'December':'Desember',\n",
    "    'Jan': 'Januari', 'Feb': 'Februari', 'Mar': 'Maret', 'Apr': 'April', 'May': 'Mei',\n",
    "    'Jun': 'Juni', 'Jul': 'Juli', 'Aug': 'Agustus', 'Sep': 'September', 'Oct': 'Oktober',\n",
    "    'Nov': 'November', 'Dec':'Desember','Agu':'Agustus','Okt':'Oktober','Des':'Desember'\n",
    "}\n",
    "\n",
    "# Pembersihan dan konversi\n",
    "def convert_date(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'N/A'\n",
    "    \n",
    "    original_text = text.strip()\n",
    "\n",
    "    if '|' in original_text:\n",
    "        text = original_text.split('|', 1)[-1].strip()\n",
    "    \n",
    "    if re.search(r'(?i)\\b\\d+\\s+(Hari|Jam)\\s+yang\\s+lalu\\b', text):\n",
    "        return '20 November 2025'\n",
    "    \n",
    "    # Buang hari & jam jika ada\n",
    "    text = re.sub(r'(?i)\\b(Minggu|Senin|Selasa|Rabu|Kamis|Jumat|Sabtu),?\\s*', '', text)\n",
    "    text = re.sub(r'\\b(Minggu|Senin|Selasa|Rabu|Kamis|Jumat|Sabtu)\\s*', '', text)\n",
    "    text = re.sub(r'\\d{2}:\\d{2}$', '', text)\n",
    "    \n",
    "    # Ubah 'July 31, 2012' ke '31 July 2012'\n",
    "    m = re.match(r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', text.strip())\n",
    "    if m:\n",
    "        text = f\"{m.group(2)} {m.group(1)} {m.group(3)}\"\n",
    "    \n",
    "    # Ubah ke list lalu ganti bulan\n",
    "    parts = text.strip().split()\n",
    "    if len(parts) >= 3:\n",
    "        hari, bulan_raw, tahun = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        return text.strip()\n",
    "    # Ganti bulan\n",
    "    bulan = bulan_dict.get(bulan_raw, bulan_raw)\n",
    "    # Format ulang\n",
    "    return f\"{hari} {bulan} {tahun}\"\n",
    "\n",
    "df_article['tanggal_terbit_normalized'] = df_article['tanggal_terbit'].apply(convert_date)\n",
    "\n",
    "unique_sources = df_article['sumber'].unique()\n",
    "sample_rows = []\n",
    "\n",
    "for source in unique_sources:\n",
    "    sample = df_article[df_article['sumber'] == source].head(1)\n",
    "    sample_rows.append(sample)\n",
    "\n",
    "if sample_rows:\n",
    "    temp_df = pd.concat(sample_rows)\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(temp_df[['sumber', 'tanggal_terbit', 'tanggal_terbit_normalized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_article_cleaned = df_article.drop('tanggal_terbit', axis=1)\n",
    "df_article_cleaned = df_article_cleaned.rename(columns={'tanggal_terbit_normalized': 'tanggal_terbit'})\n",
    "\n",
    "# Kamus untuk mengubah tanggal_terbit dari format object menjadi datetime\n",
    "ID_TO_EN_MAP = {\n",
    "    'Januari': 'January',\n",
    "    'Februari': 'February',\n",
    "    'Maret': 'March',\n",
    "    'April': 'April',\n",
    "    'Mei': 'May',\n",
    "    'Juni': 'June',\n",
    "    'Juli': 'July',\n",
    "    'Agustus': 'August',\n",
    "    'September': 'September',\n",
    "    'Oktober': 'October',\n",
    "    'November': 'November',\n",
    "    'Desember': 'December'\n",
    "}\n",
    "\n",
    "def convert_indo_date_to_datetime(date_str):\n",
    "    \"\"\"Mengubah string tanggal Indo (15 Agustus 2025) ke datetime object.\"\"\"\n",
    "    if not isinstance(date_str, str) or date_str == 'N/A':\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Terjemahkan nama bulan di dalam string ke Inggris\n",
    "    # Contoh: \"15 Agustus 2025\" -> \"15 August 2025\"\n",
    "    date_str_en = date_str\n",
    "    for id_month, en_month in ID_TO_EN_MAP.items():\n",
    "        if id_month in date_str:\n",
    "            date_str_en = date_str.replace(id_month, en_month)\n",
    "            break\n",
    "            \n",
    "    # Sekarang Pandas bisa membacanya dengan mudah\n",
    "    try:\n",
    "        return pd.to_datetime(date_str_en, format='%d %B %Y')\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# 2. Terapkan ke kolom baru 'timestamp'\n",
    "print(\"Sedang membuat kolom timestamp...\")\n",
    "df_article_cleaned['timestamp'] = df_article_cleaned['tanggal_terbit'].apply(convert_indo_date_to_datetime)\n",
    "\n",
    "# 3. Cek Hasilnya\n",
    "print(\"\\nCek tipe data:\")\n",
    "print(df_article_cleaned.dtypes)\n",
    "\n",
    "print(\"\\nContoh data:\")\n",
    "# Tampilkan kolom tanggal (string indo) dan timestamp (datetime) berdampingan\n",
    "display(df_article_cleaned[['tanggal_terbit', 'timestamp']].head())\n",
    "\n",
    "# Opsional: Cek apakah masih ada NaT (selain yang memang N/A)\n",
    "jumlah_nat = df_article_cleaned['timestamp'].isna().sum()\n",
    "print(f\"\\nJumlah baris yang gagal dikonversi (NaT): {jumlah_nat}\")\n",
    "timestamp_nat = df_article_cleaned['timestamp'].isna()\n",
    "failed_rows = df_article_cleaned[timestamp_nat]\n",
    "\n",
    "if not failed_rows.empty:\n",
    "    print(f\"\\nDitemukan {len(failed_rows)} baris yang gagal dikonversi menjadi datetime (NaT).\")\n",
    "    print(\"Berikut adalah sampel dari baris-baris yang gagal tersebut:\")\n",
    "    \n",
    "    # 'tanggal_terbit' adalah kolom yang paling penting untuk dianalisis\n",
    "    display(failed_rows[['sumber', 'tanggal_terbit', 'timestamp']])\n",
    "    \n",
    "    print(\"\\n--- Analisis Frekuensi Format Tanggal yang Gagal ---\")\n",
    "    print(\"Berikut adalah format-format tanggal unik yang paling sering menyebabkan kegagalan:\")\n",
    "    \n",
    "    display(failed_rows['tanggal_terbit'].value_counts().head(20))\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSelamat! Tidak ada baris dengan nilai NaT di kolom 'timestamp'.\")\n",
    "\n",
    "print(df_article_cleaned.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "TODAY = datetime(2025, 11, 10)\n",
    "\n",
    "ID_TO_EN_MAP = {\n",
    "    'Januari': 'January', 'Februari': 'February', 'Maret': 'March', 'April': 'April',\n",
    "    'Mei': 'May', 'Juni': 'June', 'Juli': 'July', 'Agustus': 'August',\n",
    "    'September': 'September', 'Oktober': 'October', 'November': 'November', 'Desember': 'December'\n",
    "}\n",
    "\n",
    "def convert_to_datetime(date_str):\n",
    "    if not isinstance(date_str, str) or date_str.lower() == 'n/a':\n",
    "        return pd.NaT\n",
    "    \n",
    "    match = re.search(r'(\\d+)\\s+Hari yang', date_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        days_ago = int(match.group(1))\n",
    "        calculated_date = TODAY - timedelta(days=days_ago)\n",
    "        return calculated_date\n",
    "    \n",
    "    date_str_en = date_str\n",
    "    for id_month, en_month in ID_TO_EN_MAP.items():\n",
    "        if id_month in date_str:\n",
    "            date_str_en = date_str.replace(id_month, en_month)\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(date_str_en, format='%d %B %Y')\n",
    "    except Exception:\n",
    "        # Jika gagal, kembalikan NaT\n",
    "        print(f\"Gagal mem-parsing tanggal absolut: '{date_str}'\")\n",
    "        return pd.NaT\n",
    "\n",
    "print(\"--- DataFrame SEBELUM konversi tanggal relatif ---\")\n",
    "df_article_cleaned.head()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Terapkan fungsi konversi baru untuk membuat kolom 'timestamp'\n",
    "print(\"Mengonversi semua format tanggal ke tipe datetime...\")\n",
    "df_article_cleaned['timestamp'] = df_article_cleaned['tanggal_terbit'].apply(convert_to_datetime)\n",
    "\n",
    "def format_to_indonesian_str(dt_object):\n",
    "    if pd.isna(dt_object):\n",
    "        return 'N/A'\n",
    "    english_date_str = dt_object.strftime('%d %B %Y')\n",
    "    for en_month, id_month in ID_TO_EN_MAP.items():\n",
    "        english_date_str = english_date_str.replace(en_month, id_month)\n",
    "    return english_date_str\n",
    "\n",
    "# Timpa kolom 'tanggal_terbit' yang lama dengan format yang sudah konsisten\n",
    "df_article_cleaned['tanggal_terbit'] = df_article_cleaned['timestamp'].apply(format_to_indonesian_str)\n",
    "\n",
    "print(\"Konversi selesai.\")\n",
    "print(\"\\n--- DataFrame SETELAH konversi ---\")\n",
    "df_article_cleaned.head()\n",
    "print(\"\\nTipe data akhir:\")\n",
    "df_article_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article_cleaned\n",
    "df_article.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a25ec",
   "metadata": {},
   "source": [
    "### Content Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4b17b",
   "metadata": {},
   "source": [
    "#### Create Whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f22ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Daftar kata kerja/umum yang HARUS DIKELUARKAN dari WHITELIST agar bisa di-stemming\n",
    "WORDS_TO_EXCLUDE_FROM_WHITELIST = set([\n",
    "    \"bawa\", \"beli\", \"bepergian\", \"beredar\", \"berencana\", \"berhasil\", \"berkurang\", \"bermain\", \"beruntunnya\", \n",
    "    \"diam\", \"dibatalkan\", \"dicap\", \"diduga\", \"digelar\", \"dihapus\", \"dihentikan\", \"dikembangkan\", \"dikenalkan\", \n",
    "    \"dilengkapi\", \"dirilis\", \"dirumorkan\", \"disebar\", \"diskon\", \"ditangkap\", \"ditolak\", \"ditunda\", \"diumumkan\", \n",
    "    \"diungkap\", \"habiskan\", \"hadir\", \"hadirkan\", \"kecanduan\", \"kecipratan\", \"kejar\", \"kelarkan\", \"kesalahan\", \n",
    "    \"ketahuan\", \"ketahui\", \"keuntungan\", \"melengkapi\", \"meluncur\", \"memainkannya\", \"memanjat\", \"memilih\",\n",
    "    \"menanggapinya\", \"menawarkan\", \"mencapai\", \"mencekam\", \"mengalami\", \"mengambil\", \"mengatasi\", \"mengerti\", \n",
    "    \"menghabiskan\", \"menghilang\", \"meningkat\", \"menjaga\", \"merawat\", \"merusak\", \"muncul\", \"paham\", \"pakai\", \n",
    "    \"pamer\", \"pamerkan\", \"perbincangan\", \"percaya\", \"perdana\", \"performa\", \"perilisan\", \"perlihatkan\", \n",
    "    \"perusahaan\", \"terbakar\", \"terbang\", \"tercepat\", \"terhentikan\", \"terinspirasi\", \"terjual\", \"terlaris\", \n",
    "    \"tetapkan\", \"tambahkan\", \"tanggapi\", \"umumkan\",\n",
    "    \n",
    "    # Tambahan lain yang bersifat umum (dari contoh sebelumnya)\n",
    "    \"cara\", \"tips\", \"trik\", \"terbaru\", \"akan\", \"bikin\", \"terbaik\", \"terlengkap\", \"wajib\", \"dibuka\", \"dimainkan\", \n",
    "    \"gratis\", \"ketagihan\", \"seru\", \"lupa\", \"coba\", \"dicoba\", \"keren\", \"mainkan\", \"banget\", \"lengkap\",\n",
    "    \"pembayaran\", \"pemecatan\", \"pemerintahan\", \"pengalaman\", \"tanggal\", \"director\", \"nggak\", \"orang\", \"dirumorkan\",\n",
    "    \"ps\", \"dirilis\"\n",
    "])\n",
    "\n",
    "\n",
    "def clean_for_ngram(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "df_article['judul_clean'] = df_article['judul'].apply(clean_for_ngram)\n",
    "\n",
    "# Menggunakan Stopwords Gabungan (Indonesia dan Inggris)\n",
    "STOPWORDS_COMBINED = set(stopwords.words('indonesian')) | set(stopwords.words('english'))\n",
    "\n",
    "def get_top_ngrams(corpus, n=2, top_k=500):\n",
    "    \"\"\"Fungsi untuk mendapatkan N-gram yang paling sering muncul.\"\"\"\n",
    "    \n",
    "    # Inisialisasi CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words=list(STOPWORDS_COMBINED))\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    word_counts = X.sum(axis=0)\n",
    "    word_freq = [(word, word_counts[0, idx]) \n",
    "                 for word, idx in vectorizer.vocabulary_.items()]\n",
    "    \n",
    "    word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    return word_freq[:top_k]\n",
    "\n",
    "# Ekstrak N-gram\n",
    "top_bigrams = get_top_ngrams(df_article['judul_clean'], n=2, top_k=500)\n",
    "top_trigrams = get_top_ngrams(df_article['judul_clean'], n=3, top_k=500)\n",
    "\n",
    "print(\"### Top 20 Bigram ###\")\n",
    "for phrase, count in top_bigrams[:20]:\n",
    "    print(f\"'{phrase}' (Count: {count})\")\n",
    "\n",
    "print(\"\\n### Top 20 Trigram ###\")\n",
    "for phrase, count in top_trigrams[:20]:\n",
    "    print(f\"'{phrase}' (Count: {count})\")\n",
    "\n",
    "# 1. Kumpulkan semua kata kandidat dari N-gram\n",
    "potential_whitelist_words = set()\n",
    "\n",
    "for phrase, count in top_bigrams:\n",
    "    for word in phrase.split():\n",
    "        potential_whitelist_words.add(word)\n",
    "\n",
    "for phrase, count in top_trigrams:\n",
    "    for word in phrase.split():\n",
    "        potential_whitelist_words.add(word)\n",
    "        \n",
    "# 2. Tambahkan Jargon/Akronim Hardcoded (sebelum filtering)\n",
    "potential_whitelist_words.update([\n",
    "    \"the\", \"of\", \"and\", \"or\", \"in\", \"with\", \n",
    "    \"buff\", \"nerf\", \"meta\", \"patch\", \"dlc\", \"rts\", \"fps\", \"moba\", \"rpg\",\n",
    "    \"pc\", \"ps5\", \"xbox\", \"nintendo\", \"steam\", \"mobile\", \"e3\" # Penting untuk akronim/entitas\n",
    "])\n",
    "\n",
    "print(f\"\\nTotal kata unik (token) sebelum filtering: {len(potential_whitelist_words)}\")\n",
    "\n",
    "# 3. FILTERING FINAL: Hanya kata yang TIDAK termasuk WORDS_TO_ALLOW_STEMMING\n",
    "final_whitelist_words = [\n",
    "    word for word in potential_whitelist_words if word not in WORDS_TO_EXCLUDE_FROM_WHITELIST\n",
    "]\n",
    "\n",
    "# Nama file yang akan digunakan untuk menyimpan whitelist\n",
    "FILE_PATH = '../data/whitelist.txt'\n",
    "\n",
    "with open(FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "    for word in final_whitelist_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"\\n✅ Whitelist bersih (total {len(final_whitelist_words)} kata) berhasil disimpan ke '{FILE_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bed402",
   "metadata": {},
   "source": [
    "#### Create Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92764462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import os \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# =======================================================\n",
    "# 1. FUNGSI DAN MUAT KOMPONEN\n",
    "# =======================================================\n",
    "\n",
    "# 🚨 Penambahan try-except untuk download NLTK stopwords\n",
    "try:\n",
    "    stopwords.words('indonesian')\n",
    "except LookupError:\n",
    "    print(\"⏳ NLTK stopwords Bahasa Indonesia belum diunduh. Sedang mengunduh...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"✅ Pengunduhan NLTK stopwords selesai.\")\n",
    "\n",
    "def load_words_set(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Peringatan: File {file_path} tidak ditemukan. Menggunakan set kosong.\")\n",
    "        return set()\n",
    "\n",
    "# Asumsi lokasi file Whitelist\n",
    "WHITELIST_FILE_PATH = '../data/whitelist.txt'\n",
    "WHITELIST = load_words_set(WHITELIST_FILE_PATH)\n",
    "\n",
    "# Menggunakan Stoplist Bahasa Indonesia dari NLTK\n",
    "NLTK_ID_STOPWORDS = set(stopwords.words('indonesian'))\n",
    "STANDARD_STOPWORDS = NLTK_ID_STOPWORDS \n",
    "\n",
    "# --- Kata-kata yang TIDAK BOLEH ADA di Final Stoplist ---\n",
    "WORDS_TO_ALLOW_STEMMING = set([\n",
    "    \"bawa\", \"beli\", \"bepergian\", \"beredar\", \"berencana\", \"berhasil\", \"berkurang\", \"bermain\", \"beruntunnya\", \n",
    "    \"diam\", \"dibatalkan\", \"dicap\", \"diduga\", \"digelar\", \"dihapus\", \"dihentikan\", \"dikembangkan\", \"dikenalkan\", \n",
    "    \"dilengkapi\", \"dirilis\", \"dirumorkan\", \"disebar\", \"diskon\", \"ditangkap\", \"ditolak\", \"ditunda\", \"diumumkan\", \n",
    "    \"diungkap\", \"habiskan\", \"hadir\", \"hadirkan\", \"kecanduan\", \"kecipratan\", \"kejar\", \"kelarkan\", \"kesalahan\", \n",
    "    \"ketahuan\", \"ketahui\", \"keuntungan\", \"melengkapi\", \"meluncur\", \"memainkannya\", \"memanjat\", \"memilih\",\n",
    "    \"menanggapinya\", \"menawarkan\", \"mencapai\", \"mencekam\", \"mengalami\", \"mengambil\", \"mengatasi\", \"mengerti\", \n",
    "    \"menghabiskan\", \"menghilang\", \"meningkat\", \"menjaga\", \"merawat\", \"merusak\", \"muncul\", \"paham\", \"pakai\", \n",
    "    \"pamer\", \"pamerkan\", \"perbincangan\", \"percaya\", \"perdana\", \"performa\", \"perilisan\", \"perlihatkan\", \n",
    "    \"perusahaan\", \"terbakar\", \"terbang\", \"tercepat\", \"terhentikan\", \"terinspirasi\", \"terjual\", \"terlaris\", \n",
    "    \"tetapkan\", \"tambahkan\", \"tanggapi\", \"umumkan\"\n",
    "])\n",
    "\n",
    "\n",
    "CURATED_ADDITIONAL_STOPWORDS = set([\n",
    "    \"baca\", \"juga\", \"berita\", \"lain\", \"kembali\", \"lanjut\", \n",
    "    \"sob\", \"gaes\", \"yuk\", \"dunia\", \"maya\", \"segera\", \"download\", \n",
    "    \"jangan\", \"lupa\", \"contoh\", \"terkait\", \"halaman\", \"akhir\", \"artikel\"\n",
    "])\n",
    "\n",
    "# =======================================================\n",
    "# 2. PEMBENTUKAN FINAL STOPLIST\n",
    "# =======================================================\n",
    "\n",
    "# 1. Gabungkan semua kata yang ingin dihapus (STANDAR NLTK ID + CURATED NOISE)\n",
    "all_stopwords_to_remove = STANDARD_STOPWORDS | CURATED_ADDITIONAL_STOPWORDS\n",
    "\n",
    "# 2. Kurangi dengan WHITELIST (untuk melindungi entitas/jargon)\n",
    "FINAL_STOPLIST = all_stopwords_to_remove - WHITELIST\n",
    "\n",
    "# 3. KOREKSI TAMBAHAN: Pastikan semua kata kerja yang diizinkan untuk stemming TIDAK masuk ke stoplist\n",
    "FINAL_STOPLIST = FINAL_STOPLIST - WORDS_TO_ALLOW_STEMMING\n",
    "\n",
    "print(f\"\\nTotal Stopwords Kandidat (NLTK ID + Tambahan): {len(all_stopwords_to_remove)}\")\n",
    "print(f\"Total Kata Kunci di Whitelist: {len(WHITELIST)}\")\n",
    "print(f\"Total Kata Kerja yang Diizinkan Stemming: {len(WORDS_TO_ALLOW_STEMMING)}\")\n",
    "print(f\"✅ Total kata di FINAL STOPLIST: {len(FINAL_STOPLIST)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- PENYIMPANAN FINAL STOPLIST KE FILE TEKS ---\n",
    "FINAL_STOPLIST_FILE_PATH = '../data/final_stopwords.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(FINAL_STOPLIST_FILE_PATH)):\n",
    "    os.makedirs(os.path.dirname(FINAL_STOPLIST_FILE_PATH))\n",
    "\n",
    "with open(FINAL_STOPLIST_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(list(FINAL_STOPLIST)): \n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"✅ FINAL STOPLIST berhasil disimpan ke '{FINAL_STOPLIST_FILE_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20430d3",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "df_article\n",
    "\n",
    "WHITELIST_FILE_PATH = \"../data/whitelist.txt\"\n",
    "FINAL_STOPLIST_PATH = \"../data/final_stopwords.txt\"\n",
    "CHECKPOINT_FILE_PATH = \"../data/preprocessing_checkpoint.txt\"\n",
    "OUTPUT_CSV_FILE = \"../data/processed_data.csv\"\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def load_file(file_path):\n",
    "    \"\"\"Memuat set kata dari file teks.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\" Peringatan: File {file_path} tidak ditemukan. Menggunakan set kosong.\")\n",
    "        return set()\n",
    "    \n",
    "WHITELIST = load_words_set(WHITELIST_FILE_PATH)\n",
    "FINAL_STOPLIST = load_words_set(FINAL_STOPLIST_FILE_PATH)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    tokens_filtered = [word for word in tokens if word not in FINAL_STOPLIST]\n",
    "\n",
    "    stemmed_tokens = []\n",
    "    for word in tokens_filtered:\n",
    "        if any(char.isalpha() for char in word) and any(char.isdigit() for char in word):\n",
    "            stemmed_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word.isdigit():\n",
    "            stemmed_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in WHITELIST:\n",
    "            stemmed_tokens.append(word)\n",
    "            continue # Tambahkan continue agar tidak jatuh ke stemming di 'else'\n",
    "        \n",
    "        stemmed_tokens.append(stemmer.stem(word))\n",
    "    \n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "if 'konten_processed' not in df_article.columns:\n",
    "    df_article['konten_processed'] = np.nan\n",
    "\n",
    "total_rows = len(df_article)\n",
    "\n",
    "start_index = 0\n",
    "if os.path.exists(CHECKPOINT_FILE_PATH):\n",
    "    with open(CHECKPOINT_FILE_PATH, 'r') as f:\n",
    "        try:\n",
    "            start_index = int(f.read().strip())\n",
    "        except ValueError:\n",
    "            start_index = 0\n",
    "\n",
    "if start_index >= total_rows:\n",
    "    print(\"Preprocessing selesai\")\n",
    "else:\n",
    "    print(f\"\\n--- Memulai Proses Preprocessing Kolom 'konten_separated' ---\")\n",
    "    print(f\"Melanjutkan dari baris ke-{start_index} dari {total_rows}...\")\n",
    "\n",
    "    for i in tqdm(range(start_index, total_rows),\n",
    "                  initial=start_index,\n",
    "                  total=total_rows,\n",
    "                  desc=\"Preprocessing Konten\"):\n",
    "    \n",
    "        if pd.isna(df_article.iloc[i]['konten_processed']):\n",
    "            text_to_process = df_article.iloc[i]['konten']\n",
    "            processed_text = preprocess_text(text_to_process)\n",
    "            df_article.iloc[i, df_article.columns.get_loc('konten_processed')] = processed_text\n",
    "\n",
    "            if (i + 1) % 1000 == 0 or i == total_rows - 1:\n",
    "                with open(CHECKPOINT_FILE_PATH, 'w') as f:\n",
    "                    f.write(str(i + 1))\n",
    "    \n",
    "    print(\"SELESAI\")\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE_PATH) and df_article['konten_processed'].notna().all():\n",
    "    os.remove(CHECKPOINT_FILE_PATH)\n",
    "    print(\"File checkpoint dihapus.\")\n",
    "\n",
    "if 'konten' in df_article.columns and len(df_article) > 0:\n",
    "    print(\"\\n--- Contoh Perbandingan Hasil ---\")\n",
    "    \n",
    "    for i in range(min(5, total_rows)):\n",
    "        print(f\"\\nBaris ke-{i}:\")\n",
    "        print(f\"Konten Asli: {str(df_article.iloc[i]['konten'])[:100]}...\")\n",
    "        print(f\"Konten Diproses: {df_article.iloc[i]['konten_processed']}\")\n",
    "else:\n",
    "    print(\"DataFrame sampel kosong atau kolom 'konten' tidak ada.\")\n",
    "\n",
    "# Ekstrak DataFrame yang sudah diproses ke CSV baru\n",
    "df_article.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "print(f\"\\nDataFrame telah diekstrak ke '{OUTPUT_CSV_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4874f47",
   "metadata": {},
   "source": [
    "## **Indexing and Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MODEL_DIR = \"../models/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "BM25_MODEL_PATH = os.path.join(MODEL_DIR, 'bm25_model.pkl')\n",
    "CORPUS_DF_PATH = os.path.join(MODEL_DIR, 'df_corpus.pkl')\n",
    "\n",
    "SBERT_MODEL_PATH = os.path.join(MODEL_DIR, 'sbert_model.pkl')\n",
    "SBERT_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'sbert_embeddings.npy')\n",
    "\n",
    "df = pd.read_csv('../data/processed_data.csv')\n",
    "df = df.drop_duplicates(subset=['konten_processed'], keep='first')\n",
    "\n",
    "# BM25\n",
    "\n",
    "# Tokenizing corpus\n",
    "tokenized_corpus = df['konten_processed'].apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "print(\"BM25 Indexing Selesai.\")\n",
    "print(f\"Total dokumen terindeks oleh BM25: {len(tokenized_corpus)}\")\n",
    "\n",
    "print(\"\\nEXPORTING BM25 MODEL\")\n",
    "try:\n",
    "    with open(BM25_MODEL_PATH, \"wb\") as f:\n",
    "        pickle.dump(bm25, f)\n",
    "    print(f\"Model BM25 berhasil disimpan ke: {BM25_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan BM25: {e}\")\n",
    "\n",
    "try:\n",
    "    df.to_pickle(CORPUS_DF_PATH)\n",
    "    print(f\"DataFrame Corpus berhasil disimpan ke: {CORPUS_DF_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan DataFrame: {e}\")\n",
    "\n",
    "\n",
    "# SBERT\n",
    "model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "corpus_texts = df['konten'].astype(str).tolist()\n",
    "\n",
    "corpus_embeddings = sbert_model.encode(\n",
    "    corpus_texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "\n",
    "corpus_embeddings = corpus_embeddings.cpu().numpy()\n",
    "print(\"S-BERT Indexing (Embeddings) Selesai.\")\n",
    "print(\"\\nEXPORTING SBERT MODEL\")\n",
    "\n",
    "try:\n",
    "    np.save(SBERT_EMBEDDINGS_PATH, corpus_embeddings)\n",
    "    print(f\"Corpus Embeddings berhasil disimpan ke: {SBERT_EMBEDDINGS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan Embeddings: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(SBERT_MODEL_PATH, 'wb') as f:\n",
    "        pickle.dump(model_name, f)\n",
    "    print(f\"Nama Model SBERT berhasil disimpan ke: {SBERT_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan nama model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da0350",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4604866",
   "metadata": {},
   "source": [
    "##### Step 1: Data Cleaning and Sampling (100 Random Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996bc709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Sebelum cleaning: 2029 baris\n",
      "Setelah drop missing/konten pendek: 2020 baris\n",
      "Setelah drop duplikat: 1822 baris\n",
      "Membersihkan HTML...\n",
      "\n",
      "=== SELESAI CLEANING! ===\n",
      "Dataset disimpan: ../evaluation/df_corpus_clean.csv\n",
      "Jumlah dokumen final: 1822\n",
      "  id_dokumen                                        judul_clean  \\\n",
      "1  doc_00002  Microsoft Investigasi Penyebab Masalah SSD, Be...   \n",
      "2  doc_00003  AMD Klarifikasi Masalah Socket AM5 Terbakar, S...   \n",
      "3  doc_00004  Gamescom 2025 — Detail Trailer Fallout Season ...   \n",
      "4  doc_00005  Kisah Clair Obscur Expedition 33, Mulai dari A...   \n",
      "5  doc_00006  Developer Larang Konten Leak Infinity Nikki Di...   \n",
      "\n",
      "                                        konten_clean  \n",
      "1  Beberapa hari lalu, permasalahan SSD rusak men...  \n",
      "2  AMD akhirnya buka suara mengenai socket proses...  \n",
      "3  Detail Trailer Fallout Season 2 – Trailer TV S...  \n",
      "4  Kisah Clair Obscur: Expedition 33 — Game perda...  \n",
      "5  Leak Infinity Nikki – Pada bulan Mei 2025 lalu...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# === Load dataset mentah ===\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../data/scraped_articles.csv')\n",
    "\n",
    "print(f\"Sebelum cleaning: {len(df)} baris\")\n",
    "\n",
    "# === Pastikan id_dokumen tidak hilang ===\n",
    "if 'id_dokumen' not in df.columns:\n",
    "    raise ValueError(\"ERROR: Dataset tidak punya kolom 'id_dokumen'. Harus ada untuk IR!\")\n",
    "\n",
    "# === Buang baris tanpa konten ===\n",
    "df = df[df['konten'].notna() & (df['konten'].str.strip() != '')]\n",
    "df = df[df['konten'].apply(lambda x: len(str(x).strip()) > 50)]\n",
    "\n",
    "print(f\"Setelah drop missing/konten pendek: {len(df)} baris\")\n",
    "\n",
    "# === Buang duplikat berdasarkan URL jika ada, kalau tidak berdasarkan konten ===\n",
    "if 'url' in df.columns:\n",
    "    df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
    "else:\n",
    "    df.drop_duplicates(subset=['konten'], keep='first', inplace=True)\n",
    "\n",
    "print(f\"Setelah drop duplikat: {len(df)} baris\")\n",
    "\n",
    "# === Fungsi bersihkan HTML secara agresif ===\n",
    "def clean_html(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Parsing normal\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Hapus script & style\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    # Ambil teks\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    # Hapus whitespace berlebihan\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Membersihkan HTML...\")\n",
    "df['konten_clean'] = df['konten'].apply(clean_html)\n",
    "df['judul_clean'] = df['judul'].apply(clean_html) if 'judul' in df.columns else \"\"\n",
    "\n",
    "# === Buang judul kosong jika diinginkan (opsional) ===\n",
    "df = df[df['judul_clean'].str.strip() != \"\"]\n",
    "\n",
    "# === Simpan hanya kolom penting & ID lama ===\n",
    "final_df = df[['id_dokumen', 'judul_clean', 'konten_clean']]\n",
    "\n",
    "# === Simpan ===\n",
    "output_path_csv = '../evaluation/df_corpus_clean.csv'\n",
    "final_df.to_csv(output_path_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n=== SELESAI CLEANING! ===\")\n",
    "print(\"Dataset disimpan:\", output_path_csv)\n",
    "print(\"Jumlah dokumen final:\", len(final_df))\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a0b475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_dokumen</th>\n",
       "      <th>judul_clean</th>\n",
       "      <th>konten_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>doc_00557</td>\n",
       "      <td>Film Street Fighter Umumkan Cast dan Tanggal T...</td>\n",
       "      <td>Saatnya masuk ke arena pertarungan! Legendary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>doc_01751</td>\n",
       "      <td>Preview Hitman – Absolution: Kesan Pertama yan...</td>\n",
       "      <td>Setelah sempat terlupakan eksistensinya dari i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>doc_00497</td>\n",
       "      <td>Aplikasi Ini Bantu Migrasi Data dari Windows 1...</td>\n",
       "      <td>Meski sudah mulai banyak digunakan, nyatanya t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>doc_00735</td>\n",
       "      <td>BNPT Sebut Kelompok Radikal Rekrut Anak Muda L...</td>\n",
       "      <td>Perilaku kejahatan dan terorisme dapat terjadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>doc_00920</td>\n",
       "      <td>Rekomendasi Spesifikasi Smartphone Untuk Delta...</td>\n",
       "      <td>Indogamers.com - Delta Force Mobile merupakan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>doc_00631</td>\n",
       "      <td>Marvel Tokon Akan Hadir di TGS 2025, Akan Pame...</td>\n",
       "      <td>Sejak pertama kali diumumkan, Marvel Tokon: Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>doc_01044</td>\n",
       "      <td>Panduan Jalur Top Lane di Honor of Kings, Stra...</td>\n",
       "      <td>Indogamers.com - Top Lane, atau sering disebut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>doc_00611</td>\n",
       "      <td>Rockstar Jamin Perilisan GTA VI Akan Jadi Peri...</td>\n",
       "      <td>Antusiasme terhadap Grand Theft Auto VI (GTA 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>doc_00680</td>\n",
       "      <td>[TGS 2025] Interview Ryōsuke Horii Director Ya...</td>\n",
       "      <td>Yakuza Kiwami 3 jadi game yang dinanti banyak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>doc_00604</td>\n",
       "      <td>Petualangan Klasik Kembali! DRAGON QUEST VII: ...</td>\n",
       "      <td>Penggemar serial DRAGON QUEST di Indonesia, be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_dokumen                                        judul_clean  \\\n",
       "555   doc_00557  Film Street Fighter Umumkan Cast dan Tanggal T...   \n",
       "1741  doc_01751  Preview Hitman – Absolution: Kesan Pertama yan...   \n",
       "297   doc_00497  Aplikasi Ini Bantu Migrasi Data dari Windows 1...   \n",
       "733   doc_00735  BNPT Sebut Kelompok Radikal Rekrut Anak Muda L...   \n",
       "910   doc_00920  Rekomendasi Spesifikasi Smartphone Untuk Delta...   \n",
       "629   doc_00631  Marvel Tokon Akan Hadir di TGS 2025, Akan Pame...   \n",
       "1034  doc_01044  Panduan Jalur Top Lane di Honor of Kings, Stra...   \n",
       "609   doc_00611  Rockstar Jamin Perilisan GTA VI Akan Jadi Peri...   \n",
       "678   doc_00680  [TGS 2025] Interview Ryōsuke Horii Director Ya...   \n",
       "602   doc_00604  Petualangan Klasik Kembali! DRAGON QUEST VII: ...   \n",
       "\n",
       "                                           konten_clean  \n",
       "555   Saatnya masuk ke arena pertarungan! Legendary ...  \n",
       "1741  Setelah sempat terlupakan eksistensinya dari i...  \n",
       "297   Meski sudah mulai banyak digunakan, nyatanya t...  \n",
       "733   Perilaku kejahatan dan terorisme dapat terjadi...  \n",
       "910   Indogamers.com - Delta Force Mobile merupakan ...  \n",
       "629   Sejak pertama kali diumumkan, Marvel Tokon: Fi...  \n",
       "1034  Indogamers.com - Top Lane, atau sering disebut...  \n",
       "609   Antusiasme terhadap Grand Theft Auto VI (GTA 6...  \n",
       "678   Yakuza Kiwami 3 jadi game yang dinanti banyak ...  \n",
       "602   Penggemar serial DRAGON QUEST di Indonesia, be...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/df_corpus_clean.csv\")\n",
    "\n",
    "def sample_random_subset(df, n=50, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    sample_df = df.sample(n=n)\n",
    "    sample_df = sample_df[['id_dokumen', 'judul_clean', 'konten_clean']]\n",
    "    return sample_df\n",
    "\n",
    "sample_berita = sample_random_subset(df, n=100)\n",
    "sample_berita.to_csv('../evaluation/sample_berita_100.csv', index=False)\n",
    "sample_berita.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adac416",
   "metadata": {},
   "source": [
    "##### Step 2: Generate Query using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700dba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai generate queries (Mode: High Token & Retry)...\n",
      "✅ Berhasil doc index 0\n",
      "✅ Berhasil doc index 1\n",
      "✅ Berhasil doc index 2\n",
      "✅ Berhasil doc index 3\n",
      "✅ Berhasil doc index 4\n",
      "✅ Berhasil doc index 5\n",
      "✅ Berhasil doc index 6\n",
      "✅ Berhasil doc index 7\n",
      "✅ Berhasil doc index 8\n",
      "✅ Berhasil doc index 9\n",
      "✅ Berhasil doc index 10\n",
      "✅ Berhasil doc index 11\n",
      "✅ Berhasil doc index 12\n",
      "✅ Berhasil doc index 13\n",
      "✅ Berhasil doc index 14\n",
      "✅ Berhasil doc index 15\n",
      "✅ Berhasil doc index 16\n",
      "✅ Berhasil doc index 17\n",
      "✅ Berhasil doc index 18\n",
      "✅ Berhasil doc index 19\n",
      "✅ Berhasil doc index 20\n",
      "✅ Berhasil doc index 21\n",
      "✅ Berhasil doc index 22\n",
      "✅ Berhasil doc index 23\n",
      "✅ Berhasil doc index 24\n",
      "✅ Berhasil doc index 25\n",
      "✅ Berhasil doc index 26\n",
      "✅ Berhasil doc index 27\n",
      "✅ Berhasil doc index 28\n",
      "✅ Berhasil doc index 29\n",
      "✅ Berhasil doc index 30\n",
      "✅ Berhasil doc index 31\n",
      "✅ Berhasil doc index 32\n",
      "✅ Berhasil doc index 33\n",
      "✅ Berhasil doc index 34\n",
      "✅ Berhasil doc index 35\n",
      "✅ Berhasil doc index 36\n",
      "✅ Berhasil doc index 37\n",
      "✅ Berhasil doc index 38\n",
      "✅ Berhasil doc index 39\n",
      "✅ Berhasil doc index 40\n",
      "✅ Berhasil doc index 41\n",
      "✅ Berhasil doc index 42\n",
      "✅ Berhasil doc index 43\n",
      "✅ Berhasil doc index 44\n",
      "✅ Berhasil doc index 45\n",
      "✅ Berhasil doc index 46\n",
      "✅ Berhasil doc index 47\n",
      "✅ Berhasil doc index 48\n",
      "✅ Berhasil doc index 49\n",
      "✅ Berhasil doc index 50\n",
      "✅ Berhasil doc index 51\n",
      "✅ Berhasil doc index 52\n",
      "✅ Berhasil doc index 53\n",
      "✅ Berhasil doc index 54\n",
      "✅ Berhasil doc index 55\n",
      "✅ Berhasil doc index 56\n",
      "✅ Berhasil doc index 57\n",
      "✅ Berhasil doc index 58\n",
      "✅ Berhasil doc index 59\n",
      "✅ Berhasil doc index 60\n",
      "✅ Berhasil doc index 61\n",
      "✅ Berhasil doc index 62\n",
      "✅ Berhasil doc index 63\n",
      "✅ Berhasil doc index 64\n",
      "✅ Berhasil doc index 65\n",
      "✅ Berhasil doc index 66\n",
      "✅ Berhasil doc index 67\n",
      "✅ Berhasil doc index 68\n",
      "✅ Berhasil doc index 69\n",
      "✅ Berhasil doc index 70\n",
      "✅ Berhasil doc index 71\n",
      "✅ Berhasil doc index 72\n",
      "✅ Berhasil doc index 73\n",
      "✅ Berhasil doc index 74\n",
      "✅ Berhasil doc index 75\n",
      "✅ Berhasil doc index 76\n",
      "✅ Berhasil doc index 77\n",
      "✅ Berhasil doc index 78\n",
      "✅ Berhasil doc index 79\n",
      "✅ Berhasil doc index 80\n",
      "✅ Berhasil doc index 81\n",
      "✅ Berhasil doc index 82\n",
      "✅ Berhasil doc index 83\n",
      "✅ Berhasil doc index 84\n",
      "✅ Berhasil doc index 85\n",
      "✅ Berhasil doc index 86\n",
      "✅ Berhasil doc index 87\n",
      "✅ Berhasil doc index 88\n",
      "✅ Berhasil doc index 89\n",
      "✅ Berhasil doc index 90\n",
      "✅ Berhasil doc index 91\n",
      "✅ Berhasil doc index 92\n",
      "✅ Berhasil doc index 93\n",
      "✅ Berhasil doc index 94\n",
      "✅ Berhasil doc index 95\n",
      "✅ Berhasil doc index 96\n",
      "✅ Berhasil doc index 97\n",
      "✅ Berhasil doc index 98\n",
      "✅ Berhasil doc index 99\n",
      "\n",
      "Selesai! Total query: 401\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=2048, \n",
    ")\n",
    "\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "]\n",
    "\n",
    "# Baca CSV\n",
    "try:\n",
    "    df = pd.read_csv('../evaluation/sample_berita_100.csv')\n",
    "except:\n",
    "    print(\"File CSV tidak ada, membuat dummy data...\")\n",
    "    df = pd.DataFrame({'id_dokumen':['d1'], 'judul_clean':['Tes'], 'konten_clean':['Isi tes']})\n",
    "\n",
    "queries_json = []\n",
    "qid_counter = 1\n",
    "\n",
    "print(\"Mulai generate queries (Mode: High Token & Retry)...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    judul = row.get('judul_clean', '')\n",
    "    konten = row.get('konten_clean', '')\n",
    "    \n",
    "    if not judul or not konten:\n",
    "        continue\n",
    "\n",
    "    prompt_text = (\n",
    "        f\"Context: Berita game berjudul '{judul}' dan kontennya {konten}.\\n\"\n",
    "        f\"Task: Buatkan total 4 query pencarian singkat yang manusia biasa mungkin gunakan untuk mencari berita ini.\\n\"\n",
    "        f\"Aturan: Jangan salin judul secara utuh, variasikan tipe query (1 buah query literal (pakai kata kunci yang sangat mirip dengan judul), 1 buah query parafrasa yang menggunakan sinonim atau deskripsi singkat, 1 buah query yang mengandung typo, singkatan, atau ejaan tidak baku, 1 buah query yang bersifat informatif/pertanyaan (bentuk apa, kapan, benarkah, dll)), query harus alami seperti pencarian manusia, tidak terlalu panjang (maks 6 kata)\"\n",
    "        f\"Format: Hanya list teks query, tanpa nomor, pisahkan dengan baris baru.\"\n",
    "    )\n",
    "\n",
    "    max_retries = 3\n",
    "    success = False\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt_text,\n",
    "                generation_config=generation_config,\n",
    "                safety_settings=safety_settings\n",
    "            )\n",
    "            \n",
    "            if response.candidates and response.candidates[0].content.parts:\n",
    "                # Ambil teks langsung dari parts\n",
    "                raw_text = response.candidates[0].content.parts[0].text\n",
    "                \n",
    "                # Proses teks\n",
    "                lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "                for q in lines:\n",
    "                    q_clean = q.lstrip('1234567890.- ').strip()\n",
    "                    if q_clean: # Pastikan tidak string kosong\n",
    "                        queries_json.append({\n",
    "                            \"qid\": f\"q{qid_counter}\",\n",
    "                            \"query_text\": q_clean,\n",
    "                            \"source_doc_id\": row['id_dokumen']\n",
    "                        })\n",
    "                        qid_counter += 1\n",
    "                \n",
    "                print(f\"✅ Berhasil doc index {idx}\")\n",
    "                success = True\n",
    "                break \n",
    "            \n",
    "            else:\n",
    "                print(f\"⚠️ Percobaan {attempt+1}: Kosong. Reason: {response.candidates[0].finish_reason}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Percobaan {attempt+1} Gagal (Index {idx}): {e}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"💀 GAGAL TOTAL index {idx} setelah {max_retries} kali coba.\")\n",
    "\n",
    "    # Delay antar dokumen (bukan antar retry)\n",
    "    time.sleep(3) \n",
    "\n",
    "# Simpan hasil\n",
    "with open('../evaluation/generated_queries.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(queries_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSelesai! Total query: {len(queries_json)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d557f",
   "metadata": {},
   "source": [
    "##### Step 3: Run Retrieval Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8308c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menyiapkan Preprocessing...\n",
      "\n",
      "Loading Model & Resources...\n",
      "✅ Loaded 401 queries.\n",
      "✅ Loaded Corpus DataFrame: 1822 docs.\n",
      "✅ Loaded BM25 Model.\n",
      "✅ Loaded S-BERT Embeddings shape: torch.Size([1822, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c1c842b6-5d61-4ce3-9267-c4d07f3977ce)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded S-BERT Model: paraphrase-multilingual-mpnet-base-v2\n",
      "\n",
      "Memulai Retrieval Top-20...\n",
      "Processed 20 queries...\n",
      "Processed 40 queries...\n",
      "Processed 60 queries...\n",
      "Processed 80 queries...\n",
      "Processed 100 queries...\n",
      "Processed 120 queries...\n",
      "Processed 140 queries...\n",
      "Processed 160 queries...\n",
      "Processed 180 queries...\n",
      "Processed 200 queries...\n",
      "Processed 220 queries...\n",
      "Processed 240 queries...\n",
      "Processed 260 queries...\n",
      "Processed 280 queries...\n",
      "Processed 300 queries...\n",
      "Processed 320 queries...\n",
      "Processed 340 queries...\n",
      "Processed 360 queries...\n",
      "Processed 380 queries...\n",
      "Processed 400 queries...\n",
      "\n",
      "✅ Selesai! Hasil pooling disimpan di '../evaluation/retrieval_pool.json'\n",
      "Total Query: 401\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASI PATH\n",
    "# ==========================================\n",
    "MODEL_DIR = \"../models/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "EVAL_DIR = \"../evaluation/\"\n",
    "\n",
    "# Path Model\n",
    "BM25_MODEL_PATH = os.path.join(MODEL_DIR, 'bm25_model.pkl')\n",
    "CORPUS_DF_PATH = os.path.join(MODEL_DIR, 'df_corpus.pkl') # Dataframe referensi\n",
    "SBERT_MODEL_PATH = os.path.join(MODEL_DIR, 'sbert_model.pkl')\n",
    "SBERT_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'sbert_embeddings.npy')\n",
    "\n",
    "# Path Helper Preprocessing (Sesuai kode kamu)\n",
    "WHITELIST_FILE_PATH = os.path.join(DATA_DIR, \"whitelist.txt\")\n",
    "FINAL_STOPLIST_FILE_PATH = os.path.join(DATA_DIR, \"final_stopwords.txt\")\n",
    "\n",
    "# Input & Output\n",
    "QUERY_FILE = os.path.join(EVAL_DIR, \"generated_queries.json\")\n",
    "OUTPUT_FILE = os.path.join(EVAL_DIR, \"retrieval_pool.json\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. SETUP PREPROCESSING (COPY DARI KODEMU)\n",
    "# ==========================================\n",
    "print(\"Menyiapkan Preprocessing...\")\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def load_words_set(file_path):\n",
    "    \"\"\"Memuat set kata dari file teks.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Peringatan: File {file_path} tidak ditemukan. Menggunakan set kosong.\")\n",
    "        return set()\n",
    "\n",
    "# Load Stopwords & Whitelist\n",
    "WHITELIST = load_words_set(WHITELIST_FILE_PATH)\n",
    "FINAL_STOPLIST = load_words_set(FINAL_STOPLIST_FILE_PATH)\n",
    "\n",
    "def preprocess_text_for_bm25(text):\n",
    "    \"\"\"\n",
    "    Fungsi ini SAMA PERSIS dengan yang kamu pakai saat indexing.\n",
    "    Digunakan khusus untuk query yang masuk ke BM25.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # Cleaning dasar\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Filter Stopwords\n",
    "    tokens_filtered = [word for word in tokens if word not in FINAL_STOPLIST]\n",
    "\n",
    "    stemmed_tokens = []\n",
    "    for word in tokens_filtered:\n",
    "        # Logika Stemming Selektif (Sesuai kodemu)\n",
    "        if any(char.isalpha() for char in word) and any(char.isdigit() for char in word):\n",
    "            stemmed_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word.isdigit():\n",
    "            stemmed_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in WHITELIST:\n",
    "            stemmed_tokens.append(word)\n",
    "            continue \n",
    "        \n",
    "        stemmed_tokens.append(stemmer.stem(word))\n",
    "    \n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD MODEL & DATA\n",
    "# ==========================================\n",
    "print(\"\\nLoading Model & Resources...\")\n",
    "\n",
    "# A. Load Queries\n",
    "with open(QUERY_FILE, 'r', encoding='utf-8') as f:\n",
    "    queries_data = json.load(f)\n",
    "print(f\"✅ Loaded {len(queries_data)} queries.\")\n",
    "\n",
    "# B. Load DataFrame Corpus (Untuk Mapping ID)\n",
    "try:\n",
    "    df_corpus = pd.read_pickle(CORPUS_DF_PATH)\n",
    "    df_corpus = df_corpus.reset_index(drop=True)\n",
    "    all_doc_ids = df_corpus['id_dokumen'].tolist()\n",
    "    print(f\"✅ Loaded Corpus DataFrame: {len(df_corpus)} docs.\")\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"Gagal load {CORPUS_DF_PATH}. Error: {e}\")\n",
    "\n",
    "# C. Load BM25\n",
    "with open(BM25_MODEL_PATH, \"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "print(\"✅ Loaded BM25 Model.\")\n",
    "\n",
    "# D. Load S-BERT Embeddings\n",
    "try:\n",
    "    corpus_embeddings = np.load(SBERT_EMBEDDINGS_PATH)\n",
    "    corpus_embeddings = torch.from_numpy(corpus_embeddings)\n",
    "    print(f\"✅ Loaded S-BERT Embeddings shape: {corpus_embeddings.shape}\")\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"Gagal load embedding. Error: {e}\")\n",
    "\n",
    "# E. Load S-BERT Model\n",
    "with open(SBERT_MODEL_PATH, 'rb') as f:\n",
    "    model_name = pickle.load(f) # Biasanya string nama modelnya\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "print(f\"✅ Loaded S-BERT Model: {model_name}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. LOOP RETRIEVAL & POOLING\n",
    "# ==========================================\n",
    "\n",
    "retrieval_pool = {} \n",
    "TOP_K = 20 # Ambil Top-20 dari masing-masing metode\n",
    "\n",
    "print(f\"\\nMemulai Retrieval Top-{TOP_K}...\")\n",
    "\n",
    "for i, q_item in enumerate(queries_data):\n",
    "    qid = q_item['qid']\n",
    "    raw_query = q_item['query_text'] # Query asli manusia (untuk S-BERT)\n",
    "    \n",
    "    # --- A. RETRIEVAL BM25 (Pakai Preprocessing) ---\n",
    "    bm25_query_text = preprocess_text_for_bm25(raw_query)\n",
    "    # 2. Split token\n",
    "    tokenized_query = bm25_query_text.split()\n",
    "    # 3. Get Scores & Sort\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    top_n_indices = np.argsort(doc_scores)[::-1][:TOP_K]\n",
    "    # 4. Map ke ID Dokumen\n",
    "    bm25_doc_ids = [all_doc_ids[idx] for idx in top_n_indices]\n",
    "\n",
    "    # --- B. RETRIEVAL S-BERT (Pakai Raw Query) ---\n",
    "    query_emb = sbert_model.encode(raw_query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_emb, corpus_embeddings, top_k=TOP_K)[0]\n",
    "    sbert_doc_ids = [all_doc_ids[hit['corpus_id']] for hit in hits]\n",
    "\n",
    "    # --- C. POOLING ---\n",
    "    combined_candidates = list(set(bm25_doc_ids + sbert_doc_ids))\n",
    "    \n",
    "    retrieval_pool[qid] = {\n",
    "        \"query_text\": raw_query,\n",
    "        \"processed_query_bm25\": bm25_query_text, # Debugging info\n",
    "        \"source_doc_id\": q_item['source_doc_id'],\n",
    "        \"candidates\": combined_candidates\n",
    "    }\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"Processed {i+1} queries...\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. SIMPAN HASIL\n",
    "# ==========================================\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(retrieval_pool, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Selesai! Hasil pooling disimpan di '{OUTPUT_FILE}'\")\n",
    "print(f\"Total Query: {len(retrieval_pool)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c1057",
   "metadata": {},
   "source": [
    "##### Step 4: Judging and Calculate Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5706bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menyiapkan Preprocessing...\n",
      "\n",
      "Loading Resources...\n",
      "Loading Cross-Encoder (Juri Evaluasi)...\n",
      "\n",
      "--- MULAI TAHAP JUDGING (401 queries) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1707e3e77b154009a755f2487299de92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging Candidates:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth tersimpan di ../evaluation/ground_truth.json\n",
      "\n",
      "--- MULAI PERHITUNGAN METRIK ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35fb048ea3d40b6b11707b3d1f0137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================\n",
      "HASIL EVALUASI INFORMATION RETRIEVAL (TOP-10)\n",
      "=================================================\n",
      "Jumlah Query: 401\n",
      "Ground Truth Method: Cross-Encoder (MS-MARCO)\n",
      "\n",
      "1. ALGORITMA BM25 (Lexical)\n",
      "   - Mean Precision@10 : 0.2566\n",
      "   - Mean Recall@10    : 0.8443\n",
      "   - Mean F1-Score@10  : 0.3378\n",
      "   - MAP (Mean AP)     : 0.7358\n",
      "\n",
      "2. ALGORITMA S-BERT (Semantic)\n",
      "   - Mean Precision@10 : 0.2002\n",
      "   - Mean Recall@10    : 0.6596\n",
      "   - Mean F1-Score@10  : 0.2616\n",
      "   - MAP (Mean AP)     : 0.5005\n",
      "=================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# ==========================================\n",
    "# 1. KONFIGURASI PATH\n",
    "# ==========================================\n",
    "MODEL_DIR = \"../models/\"\n",
    "DATA_DIR = \"../data/\"\n",
    "EVAL_DIR = \"../evaluation/\"\n",
    "\n",
    "# Path Data & Model\n",
    "BM25_MODEL_PATH = os.path.join(MODEL_DIR, 'bm25_model.pkl')\n",
    "CORPUS_DF_PATH = os.path.join(MODEL_DIR, 'df_corpus.pkl')\n",
    "SBERT_MODEL_PATH = os.path.join(MODEL_DIR, 'sbert_model.pkl')\n",
    "SBERT_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'sbert_embeddings.npy')\n",
    "\n",
    "# Helper Preprocessing\n",
    "WHITELIST_FILE_PATH = os.path.join(DATA_DIR, \"whitelist.txt\")\n",
    "FINAL_STOPLIST_FILE_PATH = os.path.join(DATA_DIR, \"final_stopwords.txt\")\n",
    "\n",
    "# Input & Output\n",
    "POOL_FILE = os.path.join(EVAL_DIR, 'retrieval_pool.json')        \n",
    "GROUND_TRUTH_FILE = os.path.join(EVAL_DIR, 'ground_truth.json')    \n",
    "FINAL_REPORT_FILE = os.path.join(EVAL_DIR, 'eval_result.txt')    \n",
    "\n",
    "# ==========================================\n",
    "# 2. SETUP PREPROCESSING (Harus ada untuk BM25)\n",
    "# ==========================================\n",
    "print(\"Menyiapkan Preprocessing...\")\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def load_words_set(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except: return set()\n",
    "\n",
    "WHITELIST = load_words_set(WHITELIST_FILE_PATH)\n",
    "FINAL_STOPLIST = load_words_set(FINAL_STOPLIST_FILE_PATH)\n",
    "\n",
    "def preprocess_text_for_bm25(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)\n",
    "    text = text.lower().replace(\"-\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = text.split()\n",
    "    tokens_filtered = [word for word in tokens if word not in FINAL_STOPLIST]\n",
    "    \n",
    "    stemmed_tokens = []\n",
    "    for word in tokens_filtered:\n",
    "        if any(char.isalpha() for char in word) and any(char.isdigit() for char in word):\n",
    "            stemmed_tokens.append(word)\n",
    "        elif word.isdigit() or word in WHITELIST:\n",
    "            stemmed_tokens.append(word)\n",
    "        else:\n",
    "            stemmed_tokens.append(stemmer.stem(word))\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD DATA & MODEL\n",
    "# ==========================================\n",
    "print(\"\\nLoading Resources...\")\n",
    "\n",
    "# Load Corpus\n",
    "df_corpus = pd.read_pickle(CORPUS_DF_PATH).reset_index(drop=True)\n",
    "all_doc_ids = df_corpus['id_dokumen'].tolist()\n",
    "df_corpus['full_text'] = df_corpus['judul'].fillna('') + \". \" + df_corpus['konten'].fillna('')\n",
    "docs_map = pd.Series(df_corpus.full_text.values, index=df_corpus.id_dokumen).to_dict()\n",
    "\n",
    "# Load Models\n",
    "with open(BM25_MODEL_PATH, \"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "\n",
    "corpus_embeddings = torch.from_numpy(np.load(SBERT_EMBEDDINGS_PATH))\n",
    "\n",
    "with open(SBERT_MODEL_PATH, 'rb') as f:\n",
    "    sbert_name = pickle.load(f)\n",
    "sbert_model = SentenceTransformer(sbert_name)\n",
    "\n",
    "# Load Cross-Encoder (Juri)\n",
    "print(\"Loading Cross-Encoder (Juri Evaluasi)...\")\n",
    "judge_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Load Retrieval Pool\n",
    "with open(POOL_FILE, 'r', encoding='utf-8') as f:\n",
    "    pool_data = json.load(f)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TAHAP A: JUDGING (MEMBUAT GROUND TRUTH)\n",
    "# ==========================================\n",
    "print(f\"\\n--- MULAI TAHAP JUDGING ({len(pool_data)} queries) ---\")\n",
    "ground_truth = {}\n",
    "\n",
    "for q_id, data in tqdm(pool_data.items(), desc=\"Judging Candidates\"):\n",
    "    query = data['query_text']\n",
    "    candidates = data['candidates'] # List ID dokumen\n",
    "    source_doc = data['source_doc_id']\n",
    "    \n",
    "    # pasangan [Query, Dokumen] untuk dinilai\n",
    "    pairs = []\n",
    "    valid_candidates = []\n",
    "    \n",
    "    for doc_id in candidates:\n",
    "        if doc_id in docs_map:\n",
    "            doc_text = docs_map[doc_id][:1000]\n",
    "            pairs.append([query, doc_text])\n",
    "            valid_candidates.append(doc_id)\n",
    "            \n",
    "    if not pairs:\n",
    "        ground_truth[q_id] = [source_doc]\n",
    "        continue\n",
    "\n",
    "    # Prediksi Skor\n",
    "    scores = judge_model.predict(pairs)\n",
    "    \n",
    "    # Filter Relevan (Threshold > 0.5)\n",
    "    relevant_docs = []\n",
    "    for doc_id, score in zip(valid_candidates, scores):\n",
    "        if score > 0.5:\n",
    "            relevant_docs.append(doc_id)\n",
    "            \n",
    "    # FORCE INCLUDE SOURCE DOC\n",
    "    # Dokumen asal pembuatan query WAJIB dianggap relevan (Known-Item assumption)\n",
    "    if source_doc not in relevant_docs:\n",
    "        relevant_docs.append(source_doc)\n",
    "        \n",
    "    ground_truth[q_id] = list(set(relevant_docs))\n",
    "\n",
    "# Simpan Ground Truth\n",
    "with open(GROUND_TRUTH_FILE, 'w') as f:\n",
    "    json.dump(ground_truth, f, indent=2)\n",
    "print(f\"Ground Truth tersimpan di {GROUND_TRUTH_FILE}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. TAHAP B: EVALUASI METRIK\n",
    "# ==========================================\n",
    "print(\"\\n--- MULAI PERHITUNGAN METRIK ---\")\n",
    "\n",
    "def calculate_metrics(retrieved_ids, true_ids, k=10):\n",
    "    \"\"\"Menghitung P@k, R@k, F1@k, AP@k\"\"\"\n",
    "    retrieved_k = retrieved_ids[:k]\n",
    "    true_set = set(true_ids)\n",
    "    \n",
    "    # Hits\n",
    "    hits = [1 if doc in true_set else 0 for doc in retrieved_k]\n",
    "    num_hits = sum(hits)\n",
    "    num_relevant = len(true_set)\n",
    "    \n",
    "    # Precision & Recall\n",
    "    precision = num_hits / k\n",
    "    recall = num_hits / num_relevant if num_relevant > 0 else 0\n",
    "    \n",
    "    # F1\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Average Precision (AP)\n",
    "    ap = 0.0\n",
    "    running_hits = 0\n",
    "    for i, is_hit in enumerate(hits):\n",
    "        if is_hit:\n",
    "            running_hits += 1\n",
    "            ap += running_hits / (i + 1)\n",
    "    \n",
    "    ap = ap / num_relevant if num_relevant > 0 else 0\n",
    "    return precision, recall, f1, ap\n",
    "\n",
    "# Storage hasil\n",
    "metrics_bm25 = {'p': [], 'r': [], 'f1': [], 'ap': []}\n",
    "metrics_sbert = {'p': [], 'r': [], 'f1': [], 'ap': []}\n",
    "K = 10\n",
    "\n",
    "for q_id, true_ids in tqdm(ground_truth.items(), desc=\"Calculating Metrics\"):\n",
    "    # Ambil teks query asli\n",
    "    raw_query = pool_data[q_id]['query_text']\n",
    "    \n",
    "    # --- 1. JALANKAN BM25 ULANG ---\n",
    "    bm25_query = preprocess_text_for_bm25(raw_query)\n",
    "    bm25_tokens = bm25_query.split()\n",
    "    bm25_scores = bm25.get_scores(bm25_tokens)\n",
    "    bm25_top_idx = np.argsort(bm25_scores)[::-1][:K]\n",
    "    bm25_res = [all_doc_ids[i] for i in bm25_top_idx]\n",
    "    \n",
    "    # --- 2. JALANKAN S-BERT ULANG ---\n",
    "    sbert_emb = sbert_model.encode(raw_query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(sbert_emb, corpus_embeddings, top_k=K)[0]\n",
    "    sbert_res = [all_doc_ids[hit['corpus_id']] for hit in hits]\n",
    "    \n",
    "    # --- 3. HITUNG SKOR ---\n",
    "    p, r, f, ap = calculate_metrics(bm25_res, true_ids, k=K)\n",
    "    metrics_bm25['p'].append(p)\n",
    "    metrics_bm25['r'].append(r)\n",
    "    metrics_bm25['f1'].append(f)\n",
    "    metrics_bm25['ap'].append(ap)\n",
    "    \n",
    "    p, r, f, ap = calculate_metrics(sbert_res, true_ids, k=K)\n",
    "    metrics_sbert['p'].append(p)\n",
    "    metrics_sbert['r'].append(r)\n",
    "    metrics_sbert['f1'].append(f)\n",
    "    metrics_sbert['ap'].append(ap)\n",
    "\n",
    "# ==========================================\n",
    "# 6. HASIL AKHIR\n",
    "# ==========================================\n",
    "result_text = f\"\"\"\n",
    "=================================================\n",
    "HASIL EVALUASI INFORMATION RETRIEVAL (TOP-{K})\n",
    "=================================================\n",
    "Jumlah Query: {len(ground_truth)}\n",
    "Ground Truth Method: Cross-Encoder (MS-MARCO)\n",
    "\n",
    "1. ALGORITMA BM25 (Lexical)\n",
    "   - Mean Precision@{K} : {np.mean(metrics_bm25['p']):.4f}\n",
    "   - Mean Recall@{K}    : {np.mean(metrics_bm25['r']):.4f}\n",
    "   - Mean F1-Score@{K}  : {np.mean(metrics_bm25['f1']):.4f}\n",
    "   - MAP (Mean AP)     : {np.mean(metrics_bm25['ap']):.4f}\n",
    "\n",
    "2. ALGORITMA S-BERT (Semantic)\n",
    "   - Mean Precision@{K} : {np.mean(metrics_sbert['p']):.4f}\n",
    "   - Mean Recall@{K}    : {np.mean(metrics_sbert['r']):.4f}\n",
    "   - Mean F1-Score@{K}  : {np.mean(metrics_sbert['f1']):.4f}\n",
    "   - MAP (Mean AP)     : {np.mean(metrics_sbert['ap']):.4f}\n",
    "=================================================\n",
    "\"\"\"\n",
    "\n",
    "print(result_text)\n",
    "\n",
    "# Simpan ke file\n",
    "with open(FINAL_REPORT_FILE, 'w') as f:\n",
    "    f.write(result_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
