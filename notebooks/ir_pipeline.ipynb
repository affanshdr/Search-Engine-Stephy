{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09062f7",
   "metadata": {},
   "source": [
    "## **Crawling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35e5eb",
   "metadata": {},
   "source": [
    "### Gamebrott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "# import re \n",
    "\n",
    "# from bs4 import XMLParsedAsHTMLWarning\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "# # --- Konfigurasi ---\n",
    "# SITEMAP_INDEX_URL = \"https://gamebrott.com/sitemap_index.xml\"\n",
    "# URL_OUTPUT_FILE = \"../data/crawled_urls.txt\" \n",
    "# LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "# PORTAL_NAME = \"Gamebrott\"\n",
    "\n",
    "# output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# HEADERS = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# def log_to_file(message):\n",
    "#     \"\"\"Fungsi untuk menulis pesan log ke file dengan timestamp.\"\"\"\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "#     print(log_entry.strip())\n",
    "#     with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "#         f.write(log_entry)\n",
    "\n",
    "# def get_sitemap_number(url):\n",
    "#     \"\"\"Mengekstrak nomor dari URL sitemap untuk sorting yang benar.\"\"\"\n",
    "#     match = re.search(r'post-sitemap(\\d+)\\.xml', url)\n",
    "#     return int(match.group(1)) if match else 0\n",
    "\n",
    "# def crawl_xml_sitemap():\n",
    "#     \"\"\"\n",
    "#     Fungsi utama untuk crawling sitemap XML, dengan logging dan penyimpanan append.\n",
    "#     \"\"\"\n",
    "#     log_to_file(f\"===== Memulai Proses Crawling {PORTAL_NAME} =====\")\n",
    "    \n",
    "#     newly_found_urls = []\n",
    "    \n",
    "#     try:\n",
    "#         log_to_file(f\"Mengambil sitemap index dari: {SITEMAP_INDEX_URL}\")\n",
    "#         response_main = requests.get(SITEMAP_INDEX_URL, headers=HEADERS, timeout=15)\n",
    "#         response_main.raise_for_status()\n",
    "\n",
    "#         # Gunakan \"lxml-xml\" atau \"xml\" dengan lxml sebagai parser\n",
    "#         # Jika masih error, gunakan \"html.parser\" sebagai fallback\n",
    "#         try:\n",
    "#             soup_main = BeautifulSoup(response_main.content, \"lxml-xml\")\n",
    "#         except:\n",
    "#             log_to_file(\"  -> Mencoba parser alternatif...\")\n",
    "#             soup_main = BeautifulSoup(response_main.content, \"html.parser\")\n",
    "        \n",
    "#         all_sitemap_links = [loc.text for loc in soup_main.find_all('loc')]\n",
    "        \n",
    "#         if not all_sitemap_links:\n",
    "#             log_to_file(\"ERROR: Tidak ada tag <loc> yang ditemukan di sitemap index.\")\n",
    "#             return\n",
    "\n",
    "#         log_to_file(f\"Total sitemap ditemukan di index: {len(all_sitemap_links)}\")\n",
    "        \n",
    "#         post_sitemap_links = [url for url in all_sitemap_links if 'post-sitemap' in url]\n",
    "#         log_to_file(f\"Menyaring sitemap... Ditemukan {len(post_sitemap_links)} link yang mengandung 'post-sitemap'.\")\n",
    "\n",
    "#         post_sitemap_links.sort(key=get_sitemap_number)\n",
    "        \n",
    "#         sitemaps_to_process = post_sitemap_links[:3]\n",
    "#         log_to_file(f\"Membatasi proses hanya untuk {len(sitemaps_to_process)} sitemap pertama.\")\n",
    "        \n",
    "#         for i, sitemap_url in enumerate(sitemaps_to_process, 1):\n",
    "#             log_to_file(f\"({i}/{len(sitemaps_to_process)}) Memproses sitemap: {sitemap_url}\")\n",
    "#             try:\n",
    "#                 response_post = requests.get(sitemap_url, headers=HEADERS, timeout=15)\n",
    "#                 response_post.raise_for_status()\n",
    "\n",
    "#                 # Gunakan parser yang sama\n",
    "#                 try:\n",
    "#                     soup_post = BeautifulSoup(response_post.content, \"lxml-xml\")\n",
    "#                 except:\n",
    "#                     soup_post = BeautifulSoup(response_post.content, \"html.parser\")\n",
    "                \n",
    "#                 article_links_in_page = []\n",
    "#                 url_blocks = soup_post.find_all('url')\n",
    "                \n",
    "#                 for block in url_blocks:\n",
    "#                     loc_tag = block.find('loc')\n",
    "#                     if loc_tag:\n",
    "#                         article_links_in_page.append(loc_tag.text)\n",
    "                \n",
    "#                 log_to_file(f\"  -> Ditemukan {len(article_links_in_page)} link artikel (gambar diabaikan).\")\n",
    "#                 newly_found_urls.extend(article_links_in_page)\n",
    "\n",
    "#             except requests.exceptions.RequestException as e:\n",
    "#                 log_to_file(f\"  -> ERROR: Gagal mengambil atau memproses {sitemap_url}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         log_to_file(f\"FATAL ERROR: Gagal mengambil sitemap index utama. Proses dihentikan. Error: {e}\")\n",
    "#         return\n",
    "\n",
    "#     if newly_found_urls:\n",
    "#         log_to_file(f\"Menyimpan {len(newly_found_urls)} link baru ke file {URL_OUTPUT_FILE}...\")\n",
    "#         try:\n",
    "#             with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "#                 for url in newly_found_urls:\n",
    "#                     # Menggunakan variabel PORTAL_NAME\n",
    "#                     f.write(f\"{PORTAL_NAME};{url}\\n\")\n",
    "#             log_to_file(\"Penyimpanan link berhasil.\")\n",
    "#         except IOError as e:\n",
    "#             log_to_file(f\"ERROR: Gagal menulis ke file {URL_OUTPUT_FILE}: {e}\")\n",
    "#     else:\n",
    "#         log_to_file(\"Tidak ada link baru yang ditemukan pada sesi crawling ini.\")\n",
    "    \n",
    "#     log_to_file(f\"Total link yang didapat pada sesi ini: {len(newly_found_urls)}\")\n",
    "#     log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     crawl_xml_sitemap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5990a3",
   "metadata": {},
   "source": [
    "### Kotakgames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# import urllib3\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# START_YEAR = 2025\n",
    "# END_YEAR = 2025  # Sama dengan START_YEAR untuk hanya mengambil tahun 2025\n",
    "# MAX_LINKS = 500\n",
    "\n",
    "# # --- Konfigurasi ---\n",
    "# URL_OUTPUT_FILE = \"../data/crawled_urls.txt\"\n",
    "# LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "# PORTAL_NAME = \"Kotakgame\"\n",
    "\n",
    "# output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# HEADERS = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# def log_to_file(message):\n",
    "#     # Gunakan format yang kompatibel dengan Windows dan Linux\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "#     print(log_entry.strip())\n",
    "#     with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "#         f.write(log_entry)\n",
    "\n",
    "# def get_last_page(year_base_url):\n",
    "#     \"\"\"Fungsi ini sekarang menerima base URL untuk tahun tertentu.\"\"\"\n",
    "#     first_page_url = year_base_url + \"1/\"\n",
    "#     try:\n",
    "#         log_to_file(f\"  -> Mencari halaman terakhir dari: {first_page_url}\")\n",
    "#         response = requests.get(first_page_url, headers=HEADERS, timeout=15, verify=False)\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#         last_page_link = soup.find('a', class_='prevnext', text='LAST »')\n",
    "        \n",
    "#         if last_page_link and last_page_link.has_attr('href'):\n",
    "#             href = last_page_link['href']\n",
    "#             page_number = int(href.strip('/').split('/')[-1])\n",
    "#             log_to_file(f\"  -> Halaman terakhir ditemukan: {page_number}\")\n",
    "#             return page_number\n",
    "#         else:\n",
    "#             log_to_file(\"  -> WARNING: Link 'LAST »' tidak ditemukan. Mengasumsikan hanya ada 1 halaman.\")\n",
    "#             return 1\n",
    "            \n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         log_to_file(f\"  -> ERROR: Tidak bisa mengakses halaman pertama untuk tahun ini. Error: {e}\")\n",
    "#         return 0 # Mengembalikan 0 jika gagal, untuk dilewati\n",
    "#     except (ValueError, IndexError):\n",
    "#         log_to_file(\"  -> FATAL ERROR: Gagal mem-parsing nomor halaman terakhir dari link.\")\n",
    "#         return 0\n",
    "\n",
    "# def crawl_kotakgame_multi_year(max_links=None):\n",
    "#     \"\"\"\n",
    "#     Crawl artikel dari Kotakgame.com\n",
    "    \n",
    "#     Args:\n",
    "#         max_links (int, optional): Jumlah maksimal link yang ingin diambil.\n",
    "#                                    Jika None, akan mengambil semua link.\n",
    "#     \"\"\"\n",
    "#     log_to_file(f\"===== Memulai Proses Crawling {PORTAL_NAME} (Tahun {START_YEAR}-{END_YEAR}) =====\")\n",
    "#     if max_links:\n",
    "#         log_to_file(f\"Target: Mengambil maksimal {max_links} link\")\n",
    "    \n",
    "#     grand_total_urls = []\n",
    "    \n",
    "#     for year in range(START_YEAR, END_YEAR + 1):\n",
    "#         # Cek apakah sudah mencapai limit\n",
    "#         if max_links and len(grand_total_urls) >= max_links:\n",
    "#             log_to_file(f\"Sudah mencapai limit {max_links} link. Menghentikan crawling.\")\n",
    "#             break\n",
    "            \n",
    "#         log_to_file(f\"--- Memproses Tahun {year} ---\")\n",
    "        \n",
    "#         # Base URL sekarang dinamis berdasarkan tahun\n",
    "#         base_url_for_year = f\"https://www.kotakgame.com/berita/index/{year}/0/0/\"\n",
    "        \n",
    "#         last_page = get_last_page(base_url_for_year)\n",
    "#         if last_page == 0:\n",
    "#             log_to_file(f\"Melewati tahun {year} karena gagal mendapatkan info halaman.\")\n",
    "#             continue # Lanjut ke tahun berikutnya\n",
    "\n",
    "#         urls_this_year = []\n",
    "#         for page_num in range(1, last_page + 1):\n",
    "#             # Cek limit sebelum memproses halaman baru\n",
    "#             if max_links and len(grand_total_urls) >= max_links:\n",
    "#                 log_to_file(f\"    -> Sudah mencapai limit {max_links} link. Melewati halaman berikutnya.\")\n",
    "#                 break\n",
    "                \n",
    "#             page_url = f\"{base_url_for_year}{page_num}/\"\n",
    "#             log_to_file(f\"    ({page_num}/{last_page}) Memproses halaman: {page_url}\")\n",
    "            \n",
    "#             try:\n",
    "#                 response = requests.get(page_url, headers=HEADERS, timeout=15, verify=False)\n",
    "#                 response.raise_for_status()\n",
    "                \n",
    "#                 soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#                 article_links_found = soup.select('div#contenta div.detailfeature h4 a')\n",
    "                \n",
    "#                 page_urls = []\n",
    "#                 for link_tag in article_links_found:\n",
    "#                     if link_tag.has_attr('href'):\n",
    "#                         relative_url = link_tag['href']\n",
    "#                         full_url = f\"https://www.kotakgame.com{relative_url}\"\n",
    "#                         page_urls.append(full_url)\n",
    "                \n",
    "#                 # Batasi jumlah URL yang diambil jika sudah mendekati limit\n",
    "#                 if max_links:\n",
    "#                     remaining_links = max_links - len(grand_total_urls)\n",
    "#                     if remaining_links < len(page_urls):\n",
    "#                         page_urls = page_urls[:remaining_links]\n",
    "#                         log_to_file(f\"      -> Membatasi ke {remaining_links} link untuk mencapai target {max_links}\")\n",
    "                \n",
    "#                 log_to_file(f\"      -> Ditemukan {len(page_urls)} link artikel.\")\n",
    "#                 urls_this_year.extend(page_urls)\n",
    "#                 grand_total_urls.extend(page_urls)\n",
    "                \n",
    "#                 # Cek apakah sudah mencapai limit setelah menambah URL\n",
    "#                 if max_links and len(grand_total_urls) >= max_links:\n",
    "#                     log_to_file(f\"      -> Target {max_links} link tercapai!\")\n",
    "#                     break\n",
    "\n",
    "#             except requests.exceptions.RequestException as e:\n",
    "#                 log_to_file(f\"      -> ERROR: Gagal mengambil {page_url}: {e}\")\n",
    "#                 continue\n",
    "        \n",
    "#         log_to_file(f\"--- Selesai Tahun {year}, ditemukan {len(urls_this_year)} link ---\")\n",
    "\n",
    "#     if grand_total_urls:\n",
    "#         unique_urls = sorted(list(set(grand_total_urls)))\n",
    "#         log_to_file(f\"Menyimpan total {len(unique_urls)} link unik dari semua tahun ke file {URL_OUTPUT_FILE}...\")\n",
    "#         try:\n",
    "#             with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "#                 for url in unique_urls:\n",
    "#                     f.write(f\"{PORTAL_NAME};{url}\\n\")\n",
    "#             log_to_file(\"Penyimpanan link berhasil.\")\n",
    "#         except IOError as e:\n",
    "#             log_to_file(f\"ERROR: Gagal menulis ke file {URL_OUTPUT_FILE}: {e}\")\n",
    "#     else:\n",
    "#         log_to_file(\"Tidak ada link baru yang ditemukan pada sesi crawling ini.\")\n",
    "    \n",
    "#     log_to_file(f\"Total link yang didapat pada sesi ini: {len(grand_total_urls)}\")\n",
    "#     log_to_file(f\"Total link unik yang disimpan: {len(unique_urls) if grand_total_urls else 0}\")\n",
    "#     log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Panggil dengan parameter max_links\n",
    "#     # Ubah nilai MAX_LINKS sesuai kebutuhan, atau set None untuk mengambil semua\n",
    "#     crawl_kotakgame_multi_year(max_links=MAX_LINKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6c385",
   "metadata": {},
   "source": [
    "### Indogamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad329f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "# import time \n",
    "# import urllib3\n",
    "\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# # --- Konfigurasi ---\n",
    "# BASE_URL = \"https://indogamers.com/\"\n",
    "# CATEGORIES = ['guides', 'pc', 'console', 'mobile'] \n",
    "# URL_OUTPUT_FILE = \"../data/crawled_urls.txt\"\n",
    "# LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "# MAX_LINKS = 500 \n",
    "\n",
    "# output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# HEADERS = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# def log_to_file(message):\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "#     print(log_entry.strip())\n",
    "#     with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "#         f.write(log_entry)\n",
    "\n",
    "# def crawl_indogamers_deep(max_links=None):\n",
    "#     \"\"\"\n",
    "#     Crawl artikel dari Indogamers.com\n",
    "    \n",
    "#     Args:\n",
    "#         max_links (int, optional): Jumlah maksimal link yang ingin diambil.\n",
    "#                                    Jika None, akan mengambil semua link.\n",
    "#     \"\"\"\n",
    "#     log_to_file(\"===== Memulai Proses Crawling Mendalam Indogamers.com =====\")\n",
    "#     if max_links:\n",
    "#         log_to_file(f\"Target: Mengambil maksimal {max_links} link\")\n",
    "    \n",
    "#     total_urls_found_session = 0\n",
    "\n",
    "#     for category in CATEGORIES:\n",
    "#         # Cek apakah sudah mencapai limit\n",
    "#         if max_links and total_urls_found_session >= max_links:\n",
    "#             log_to_file(f\"Sudah mencapai limit {max_links} link. Menghentikan crawling.\")\n",
    "#             break\n",
    "            \n",
    "#         log_to_file(f\"Memulai kategori: '{category}'\")\n",
    "#         page_num = 1\n",
    "#         urls_per_category = 0\n",
    "        \n",
    "#         while True:\n",
    "#             # Cek limit sebelum memproses halaman baru\n",
    "#             if max_links and total_urls_found_session >= max_links:\n",
    "#                 log_to_file(f\"  -> Sudah mencapai limit {max_links} link. Melewati kategori '{category}'.\")\n",
    "#                 break\n",
    "                \n",
    "#             page_url = f\"{BASE_URL}{category}?page={page_num}\"\n",
    "#             log_to_file(f\"  -> Memproses halaman: {page_url}\")\n",
    "            \n",
    "#             try:\n",
    "#                 response = requests.get(page_url, headers=HEADERS, timeout=20, verify=False)\n",
    "                \n",
    "#                 if response.status_code == 404:\n",
    "#                     log_to_file(f\"    -> Halaman tidak ditemukan (404). Akhir dari kategori '{category}'.\")\n",
    "#                     break\n",
    "\n",
    "#                 response.raise_for_status()\n",
    "#                 soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                \n",
    "#                 selector = \"div[class*='article_recent__'] div[class*='article_recent_desc__'] h1 a\"\n",
    "#                 link_tags = soup.select(selector)\n",
    "                \n",
    "#                 if not link_tags:\n",
    "#                     log_to_file(f\"    -> Tidak ada artikel ditemukan. Akhir dari kategori '{category}'.\")\n",
    "#                     break\n",
    "                \n",
    "#                 page_urls = [tag['href'] for tag in link_tags if tag.has_attr('href')]\n",
    "                \n",
    "#                 # Batasi jumlah URL yang diambil jika sudah mendekati limit\n",
    "#                 if max_links:\n",
    "#                     remaining_links = max_links - total_urls_found_session\n",
    "#                     if remaining_links < len(page_urls):\n",
    "#                         page_urls = page_urls[:remaining_links]\n",
    "#                         log_to_file(f\"    -> Membatasi ke {remaining_links} link untuk mencapai target {max_links}\")\n",
    "                \n",
    "#                 log_to_file(f\"    -> Ditemukan {len(page_urls)} link artikel.\")\n",
    "                \n",
    "#                 with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "#                     for url in page_urls:\n",
    "#                         f.write(f\"Indogamers;{url}\\n\")\n",
    "                \n",
    "#                 urls_per_category += len(page_urls)\n",
    "#                 total_urls_found_session += len(page_urls)\n",
    "                \n",
    "#                 # Cek apakah sudah mencapai limit setelah menambah URL\n",
    "#                 if max_links and total_urls_found_session >= max_links:\n",
    "#                     log_to_file(f\"    -> Target {max_links} link tercapai!\")\n",
    "#                     break\n",
    "                \n",
    "#                 page_num += 1 \n",
    "#                 time.sleep(1) \n",
    "                \n",
    "#             except requests.exceptions.RequestException as e:\n",
    "#                 log_to_file(f\"    -> ERROR: Gagal mengambil {page_url}: {e}. Mencoba lagi dalam 5 detik...\")\n",
    "#                 time.sleep(5)\n",
    "#                 continue \n",
    "        \n",
    "#         log_to_file(f\"Selesai kategori '{category}'. Total link ditemukan: {urls_per_category}\")\n",
    "\n",
    "#     log_to_file(f\"Total link yang didapat pada sesi ini: {total_urls_found_session}\")\n",
    "#     log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Panggil dengan parameter max_links\n",
    "#     # Ubah nilai MAX_LINKS sesuai kebutuhan, atau set None untuk mengambil semua\n",
    "#     crawl_indogamers_deep(max_links=MAX_LINKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c8ef6",
   "metadata": {},
   "source": [
    "### JagatPlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "# import re \n",
    "# import urllib3\n",
    "\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# # --- Konfigurasi ---\n",
    "# SITEMAP_INDEX_URL = \"https://jagatplay.com/sitemap.html\"\n",
    "# URL_OUTPUT_FILE = \"../data/crawled_urls.txt\"\n",
    "# LOG_FILE = \"../data/crawl_logs.txt\"\n",
    "# PORTAL_NAME = \"Jagatplay\"\n",
    "# MAX_LINKS = 500  # Jumlah maksimal link yang ingin diambil\n",
    "\n",
    "# output_dir = os.path.dirname(URL_OUTPUT_FILE)\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# HEADERS = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# def log_to_file(message):\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "#     print(log_entry.strip())\n",
    "#     with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "#         f.write(log_entry)\n",
    "\n",
    "# def get_jagatplay_sitemap_number(url):\n",
    "#     \"\"\"Mengekstrak nomor dari URL sitemap untuk sorting yang benar.\"\"\"\n",
    "#     match = re.search(r'post-sitemap(\\d+)\\.html', url)\n",
    "#     return int(match.group(1)) if match else 0\n",
    "\n",
    "# def crawl_jagatplay(max_links=None):\n",
    "#     \"\"\"\n",
    "#     Crawl artikel dari Jagatplay.com\n",
    "    \n",
    "#     Args:\n",
    "#         max_links (int, optional): Jumlah maksimal link yang ingin diambil.\n",
    "#                                    Jika None, akan mengambil semua link.\n",
    "#     \"\"\"\n",
    "#     log_to_file(f\"===== Memulai Proses Crawling {PORTAL_NAME} =====\")\n",
    "#     if max_links:\n",
    "#         log_to_file(f\"Target: Mengambil maksimal {max_links} link\")\n",
    "    \n",
    "#     all_urls_found = []\n",
    "    \n",
    "#     try:\n",
    "#         log_to_file(f\"Mengambil sitemap index dari: {SITEMAP_INDEX_URL}\")\n",
    "#         response_main = requests.get(SITEMAP_INDEX_URL, headers=HEADERS, timeout=15, verify=False)\n",
    "#         response_main.raise_for_status()\n",
    "#         soup_main = BeautifulSoup(response_main.content, \"html.parser\")\n",
    "        \n",
    "#         sitemap_tags = soup_main.select('tr > td > a')\n",
    "#         all_sitemap_links = [tag['href'] for tag in sitemap_tags if tag.has_attr('href')]\n",
    "        \n",
    "#         if not all_sitemap_links:\n",
    "#             log_to_file(\"ERROR: Tidak ada link sitemap yang ditemukan di halaman index.\")\n",
    "#             return\n",
    "\n",
    "#         log_to_file(f\"Total sitemap ditemukan di index: {len(all_sitemap_links)}\")\n",
    "        \n",
    "#         post_sitemap_links_numbered = [url for url in all_sitemap_links if re.search(r'post-sitemap\\d+\\.html', url)]\n",
    "        \n",
    "#         post_sitemap_links_numbered.sort(key=get_jagatplay_sitemap_number)\n",
    "        \n",
    "#         sitemap_limit = 11\n",
    "#         sitemaps_to_process = [\n",
    "#             url for url in post_sitemap_links_numbered \n",
    "#             if get_jagatplay_sitemap_number(url) <= sitemap_limit\n",
    "#         ]\n",
    "        \n",
    "#         log_to_file(f\"Menyaring sitemap... Akan memproses {len(sitemaps_to_process)} sitemap (hingga post-sitemap{sitemap_limit}).\")\n",
    "\n",
    "#         for i, sitemap_url in enumerate(sitemaps_to_process, 1):\n",
    "#             # Cek apakah sudah mencapai limit\n",
    "#             if max_links and len(all_urls_found) >= max_links:\n",
    "#                 log_to_file(f\"Sudah mencapai limit {max_links} link. Menghentikan crawling.\")\n",
    "#                 break\n",
    "                \n",
    "#             log_to_file(f\"({i}/{len(sitemaps_to_process)}) Memproses sitemap: {sitemap_url}\")\n",
    "#             try:\n",
    "#                 response_post = requests.get(sitemap_url, headers=HEADERS, timeout=15, verify=False)\n",
    "#                 response_post.raise_for_status()\n",
    "\n",
    "#                 # Gunakan lxml-xml dengan fallback ke html.parser\n",
    "#                 try:\n",
    "#                     soup_post = BeautifulSoup(response_post.content, \"lxml-xml\")\n",
    "#                 except:\n",
    "#                     soup_post = BeautifulSoup(response_post.content, \"html.parser\")\n",
    "                \n",
    "#                 article_links = []\n",
    "#                 url_blocks = soup_post.find_all('url')\n",
    "#                 for block in url_blocks:\n",
    "#                     loc_tag = block.find('loc')\n",
    "#                     if loc_tag:\n",
    "#                         article_links.append(loc_tag.text)\n",
    "                \n",
    "#                 # Batasi jumlah URL yang diambil jika sudah mendekati limit\n",
    "#                 if max_links:\n",
    "#                     remaining_links = max_links - len(all_urls_found)\n",
    "#                     if remaining_links < len(article_links):\n",
    "#                         article_links = article_links[:remaining_links]\n",
    "#                         log_to_file(f\"  -> Membatasi ke {remaining_links} link untuk mencapai target {max_links}\")\n",
    "                \n",
    "#                 log_to_file(f\"  -> Ditemukan {len(article_links)} link artikel.\")\n",
    "#                 all_urls_found.extend(article_links)\n",
    "                \n",
    "#                 # Cek apakah sudah mencapai limit setelah menambah URL\n",
    "#                 if max_links and len(all_urls_found) >= max_links:\n",
    "#                     log_to_file(f\"  -> Target {max_links} link tercapai!\")\n",
    "#                     break\n",
    "\n",
    "#             except requests.exceptions.RequestException as e:\n",
    "#                 log_to_file(f\"  -> ERROR: Gagal mengambil atau memproses {sitemap_url}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         log_to_file(f\"FATAL ERROR: Gagal mengambil sitemap index utama. Proses dihentikan. Error: {e}\")\n",
    "#         return\n",
    "\n",
    "#     if all_urls_found:\n",
    "#         unique_urls = sorted(list(set(all_urls_found)))\n",
    "#         log_to_file(f\"Menyimpan total {len(unique_urls)} link unik ke file {URL_OUTPUT_FILE}...\")\n",
    "#         try:\n",
    "#             with open(URL_OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "#                 for url in unique_urls:\n",
    "#                     f.write(f\"{PORTAL_NAME};{url}\\n\")\n",
    "#             log_to_file(\"Penyimpanan link berhasil.\")\n",
    "#         except IOError as e:\n",
    "#             log_to_file(f\"ERROR: Gagal menulis ke file {URL_OUTPUT_FILE}: {e}\")\n",
    "#     else:\n",
    "#         log_to_file(\"Tidak ada link baru yang ditemukan pada sesi crawling ini.\")\n",
    "    \n",
    "#     log_to_file(f\"Total link yang didapat pada sesi ini: {len(all_urls_found)}\")\n",
    "#     log_to_file(f\"Total link unik yang disimpan: {len(unique_urls) if all_urls_found else 0}\")\n",
    "#     log_to_file(\"===== Proses Crawling Selesai =====\\n\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Panggil dengan parameter max_links\n",
    "#     # Ubah nilai MAX_LINKS sesuai kebutuhan, atau set None untuk mengambil semua\n",
    "#     crawl_jagatplay(max_links=MAX_LINKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e109c7",
   "metadata": {},
   "source": [
    "## **Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup, NavigableString, Comment\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import time\n",
    "# import re\n",
    "# import urllib3\n",
    "\n",
    "# # Nonaktifkan pesan peringatan SSL\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# # --- Konfigurasi ---\n",
    "# CRAWLED_URL_FILE = \"../data/crawled_urls.txt\"\n",
    "# OUTPUT_CSV_FILE = \"../data/scraped_articles.csv\"\n",
    "# LOG_FILE = \"../data/scrape_logs.txt\"\n",
    "\n",
    "# PORTALS_TO_SCRAPE = [\n",
    "#     \"Gamebrott\",\n",
    "#     \"Kotakgame\",\n",
    "#     \"Indogamers\",\n",
    "#     \"Jagatplay\"\n",
    "# ]\n",
    "\n",
    "# os.makedirs(os.path.dirname(OUTPUT_CSV_FILE), exist_ok=True)\n",
    "# HEADERS = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# def log_to_file(message):\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "#     print(log_entry.strip())\n",
    "#     with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "#         f.write(log_entry)\n",
    "\n",
    "# def read_all_urls_to_scrape(filepath, portal_names):\n",
    "#     log_to_file(f\"Membaca SEMUA URL dari {filepath}...\")\n",
    "#     all_tasks = []\n",
    "#     try:\n",
    "#         with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#             for line in f:\n",
    "#                 if ';' in line:\n",
    "#                     portal, url = line.strip().split(';', 1)\n",
    "#                     if portal in portal_names:\n",
    "#                         all_tasks.append((portal, url))\n",
    "#         log_to_file(f\"Total URL target ditemukan: {len(all_tasks)}\")\n",
    "#         return all_tasks\n",
    "#     except FileNotFoundError:\n",
    "#         log_to_file(f\"ERROR: File {filepath} tidak ditemukan.\")\n",
    "#         return []\n",
    "\n",
    "# def get_already_scraped_urls(filepath):\n",
    "#     \"\"\"Membaca CSV dan mengembalikan set URL yang sudah di-scrape.\"\"\"\n",
    "#     if not os.path.exists(filepath):\n",
    "#         return set()\n",
    "#     try:\n",
    "#         df = pd.read_csv(filepath)\n",
    "#         if 'url' in df.columns:\n",
    "#             return set(df['url'])\n",
    "#         else:\n",
    "#             log_to_file(\"WARNING: Kolom 'url' tidak ditemukan di CSV. Tidak bisa melanjutkan. Harap hapus file CSV lama atau tambahkan kolom 'url'.\")\n",
    "#             return set()\n",
    "#     except pd.errors.EmptyDataError:\n",
    "#         return set()\n",
    "#     except Exception as e:\n",
    "#         log_to_file(f\"Error saat membaca CSV yang ada: {e}. Mengasumsikan file kosong.\")\n",
    "#         return set()\n",
    "\n",
    "# def extract_text_with_links(element):\n",
    "#     \"\"\"Ekstrak teks dari elemen termasuk tag <a>, lalu bersihkan HTML belakangan.\"\"\"\n",
    "#     if not element:\n",
    "#         return 'N/A'\n",
    "    \n",
    "#     # Ambil semua paragraph\n",
    "#     paragraphs = element.find_all('p')\n",
    "#     content_parts = []\n",
    "    \n",
    "#     for p in paragraphs:\n",
    "#         # Ambil teks dengan mempertahankan <a> tag sementara\n",
    "#         text = p.get_text(separator=' ', strip=True)\n",
    "#         if text:\n",
    "#             content_parts.append(text)\n",
    "    \n",
    "#     return '\\n'.join(content_parts) if content_parts else 'N/A'\n",
    "\n",
    "# def scrape_kotakgame_article(url, soup):\n",
    "#     \"\"\"Scrape artikel dari Kotakgame\"\"\"\n",
    "#     title = 'N/A'\n",
    "#     thumbnail_url = 'N/A'\n",
    "#     publish_date = 'N/A'\n",
    "#     content = 'N/A'\n",
    "    \n",
    "#     # Judul - ada di class bagiankiri > h3.judulh3\n",
    "#     bagian_kiri = soup.select_one('.bagiankiri')\n",
    "#     if bagian_kiri:\n",
    "#         title_tag = bagian_kiri.select_one('h3.judulh3')\n",
    "#         if title_tag:\n",
    "#             title = title_tag.get_text(strip=True)\n",
    "    \n",
    "#     # Thumbnail - ada di wrapimg > img src\n",
    "#     thumb_tag = soup.select_one('.wrapimg img')\n",
    "#     if thumb_tag and thumb_tag.has_attr('src'):\n",
    "#         relative_url = thumb_tag['src']\n",
    "#         thumbnail_url = f\"https://www.kotakgame.com{relative_url}\" if relative_url.startswith('/') else relative_url\n",
    "    \n",
    "#     # Tanggal - ada di boxwidget > boxcreate > span.txtcreate2\n",
    "#     date_span = soup.select_one('.boxwidget .boxcreate .txtcreate2')\n",
    "#     if date_span:\n",
    "#         publish_date = date_span.get_text(strip=True)\n",
    "    \n",
    "#     # Konten - ada di isinewsp, ambil semua <p>\n",
    "#     content_div = soup.select_one('.isinewsp')\n",
    "#     if content_div:\n",
    "#         content = extract_text_with_links(content_div)\n",
    "    \n",
    "#     return {\n",
    "#         \"judul\": title,\n",
    "#         \"konten\": content,\n",
    "#         \"tanggal_terbit\": publish_date,\n",
    "#         \"url_thumbnail\": thumbnail_url\n",
    "#     }\n",
    "\n",
    "# def scrape_indogamers_article(url, soup):\n",
    "#     title_tag = soup.select_one('h1[class*=\"style_article__title__\"]')\n",
    "#     title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "#     thumb_tag = soup.select_one('div[class*=\"style_image__article__\"] img')\n",
    "#     if thumb_tag and thumb_tag.has_attr('srcset'):\n",
    "#         last_url_part = thumb_tag['srcset'].split(',')[-1].strip()\n",
    "#         relative_url = last_url_part.split(' ')[0]\n",
    "#         thumbnail_url = f\"https://indogamers.com{relative_url}\"\n",
    "#     else: thumbnail_url = 'N/A'\n",
    "#     date_container = soup.select_one('div[class*=\"style_author__box__\"]')\n",
    "#     publish_date = 'N/A'\n",
    "#     if date_container:\n",
    "#         all_spans = date_container.find_all('span')\n",
    "#         for span in all_spans:\n",
    "#             span_text = span.get_text(strip=True)\n",
    "#             if re.search(r'\\b(Senin|Selasa|Rabu|Kamis|Jumat|Sabtu|Minggu)\\b', span_text):\n",
    "#                 publish_date = span_text.split(',')[0].strip()\n",
    "#                 break\n",
    "#     # content_div = soup.find('article[class*=style_content__article__]')\n",
    "#     content_div = soup.find('article', class_=re.compile(r'style_content__article___'))\n",
    "#     content = content_div.decode_contents() if content_div else 'N/A'\n",
    "#     # content = 'N/A'\n",
    "#     # if content_div:\n",
    "#     #     paragraphs = content_div.find_all('p')\n",
    "#     #     content = '\\n'.join([p.get_text(strip=True) for p in paragraphs if not p.has_attr('class') or 'caption' not in ''.join(p['class'])])\n",
    "#     return {\"judul\": title, \"konten\": content, \"tanggal_terbit\": publish_date, \"url_thumbnail\": thumbnail_url}\n",
    "\n",
    "# def scrape_gamebrott_article(url, soup):\n",
    "#     \"\"\"Scrape artikel dari Gamebrott\"\"\"\n",
    "#     title = 'N/A'\n",
    "#     thumbnail_url = 'N/A'\n",
    "#     publish_date = 'N/A'\n",
    "#     content = 'N/A'\n",
    "    \n",
    "#     # Judul - class post-wrapper > h1.jeg_post_title\n",
    "#     post_wrapper = soup.select_one('.post-wrapper')\n",
    "#     if post_wrapper:\n",
    "#         title_tag = post_wrapper.select_one('h1.jeg_post_title')\n",
    "#         if title_tag:\n",
    "#             title = title_tag.get_text(strip=True)\n",
    "    \n",
    "#     # Thumbnail - class jeg_featured.featured_image > thumbnail-container > img src\n",
    "#     thumb_tag = soup.select_one('.jeg_featured.featured_image .thumbnail-container img')\n",
    "#     if thumb_tag and thumb_tag.has_attr('src'):\n",
    "#         thumbnail_url = thumb_tag['src']\n",
    "    \n",
    "#     # Tanggal - div.jeg_meta_container > div.jeg_meta_date > a\n",
    "#     date_tag = soup.select_one('.jeg_meta_container .jeg_meta_date a')\n",
    "#     if date_tag:\n",
    "#         publish_date = date_tag.get_text(strip=True)\n",
    "    \n",
    "#     # Konten - div.entry-content.no-share > div.content-inner.jeg_link_underline > p\n",
    "#     content_div = soup.select_one('.entry-content.no-share .content-inner.jeg_link_underline')\n",
    "#     if content_div:\n",
    "#         content = extract_text_with_links(content_div)\n",
    "    \n",
    "#     return {\n",
    "#         \"judul\": title,\n",
    "#         \"konten\": content,\n",
    "#         \"tanggal_terbit\": publish_date,\n",
    "#         \"url_thumbnail\": thumbnail_url\n",
    "#     }\n",
    "\n",
    "# def scrape_jagatplay_article(url, soup):\n",
    "#     \"\"\"Scrape artikel dari Jagatplay\"\"\"\n",
    "#     title = 'N/A'\n",
    "#     thumbnail_url = 'N/A'\n",
    "#     publish_date = 'N/A'\n",
    "#     content = 'N/A'\n",
    "    \n",
    "#     # Cari div#mainContent dulu\n",
    "#     main_content = soup.select_one('div#mainContent')\n",
    "#     if not main_content:\n",
    "#         main_content = soup  # Fallback ke soup utama\n",
    "    \n",
    "#     # Judul - div.jgpost__header > h1\n",
    "#     header = main_content.select_one('.jgpost__header')\n",
    "#     if header:\n",
    "#         title_tag = header.select_one('h1')\n",
    "#         if title_tag:\n",
    "#             title = title_tag.get_text(strip=True)\n",
    "    \n",
    "#     # Thumbnail - class jgpost__feat-img, ambil background url dari style\n",
    "#     feat_img = main_content.select_one('.jgpost__feat-img')\n",
    "#     if feat_img and feat_img.has_attr('style'):\n",
    "#         style = feat_img['style']\n",
    "#         match = re.search(r\"url\\(['\\\"]?(.*?)['\\\"]?\\)\", style)\n",
    "#         if match:\n",
    "#             thumbnail_url = match.group(1)\n",
    "    \n",
    "#     # Tanggal - div.jgpost__content > div.jgauthor.breakout > div.jgauthor__posted > div\n",
    "#     author_posted = main_content.select_one('.jgpost__content .jgauthor.breakout .jgauthor__posted')\n",
    "#     if author_posted:\n",
    "#         # Cari div yang berisi tanggal (biasanya div terakhir atau yang tidak punya tag <a>)\n",
    "#         divs = author_posted.find_all('div', recursive=False)\n",
    "#         for div in divs:\n",
    "#             if not div.find('a'):  # Div tanpa link biasanya berisi tanggal\n",
    "#                 publish_date = div.get_text(strip=True)\n",
    "#                 break\n",
    "    \n",
    "#     # Konten - div.jgpost__content > p\n",
    "#     content_div = main_content.select_one('.jgpost__content')\n",
    "#     if content_div:\n",
    "#         content = extract_text_with_links(content_div)\n",
    "    \n",
    "#     return {\n",
    "#         \"judul\": title,\n",
    "#         \"konten\": content,\n",
    "#         \"tanggal_terbit\": publish_date,\n",
    "#         \"url_thumbnail\": thumbnail_url\n",
    "#     }\n",
    "\n",
    "# def main():\n",
    "#     log_to_file(\"===== Memulai Proses Scraping Skala Penuh (Mode Resume + Real-time Save) =====\")\n",
    "    \n",
    "#     all_tasks = read_all_urls_to_scrape(CRAWLED_URL_FILE, PORTALS_TO_SCRAPE)\n",
    "    \n",
    "#     header = ['id_dokumen', 'sumber', 'url', 'judul', 'konten', 'tanggal_terbit', 'url_thumbnail']\n",
    "    \n",
    "#     already_scraped_urls = get_already_scraped_urls(OUTPUT_CSV_FILE)\n",
    "#     if already_scraped_urls:\n",
    "#         log_to_file(f\"Ditemukan {len(already_scraped_urls)} URL yang sudah diproses. Akan melanjutkan.\")\n",
    "    \n",
    "#     doc_id_counter = len(already_scraped_urls) + 1\n",
    "    \n",
    "#     if not os.path.exists(OUTPUT_CSV_FILE) or not already_scraped_urls:\n",
    "#         log_to_file(f\"File {OUTPUT_CSV_FILE} tidak ada atau kosong. Membuat file baru dengan header.\")\n",
    "#         pd.DataFrame(columns=header).to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "#         doc_id_counter = 1\n",
    "    \n",
    "#     total_urls_to_process = len(all_tasks)\n",
    "#     newly_scraped_count = 0\n",
    "    \n",
    "#     for i, (portal, url) in enumerate(all_tasks):\n",
    "#         # Lewati URL yang sudah ada\n",
    "#         if url in already_scraped_urls:\n",
    "#             continue\n",
    "            \n",
    "#         log_to_file(f\"  ({i+1}/{total_urls_to_process}) Scraping: {url}\")\n",
    "        \n",
    "#         try:\n",
    "#             response = requests.get(url, headers=HEADERS, timeout=15, verify=False)\n",
    "#             if response.status_code != 200:\n",
    "#                 log_to_file(f\"    -> Gagal mengakses (Status: {response.status_code})\")\n",
    "#                 continue\n",
    "            \n",
    "#             soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#             data = None\n",
    "#             if portal == \"Gamebrott\":\n",
    "#                 data = scrape_gamebrott_article(url, soup)\n",
    "#             elif portal == \"Kotakgame\":\n",
    "#                 data = scrape_kotakgame_article(url, soup)\n",
    "#             elif portal == \"Indogamers\":\n",
    "#                 data = scrape_indogamers_article(url, soup)\n",
    "#             elif portal == \"Jagatplay\":\n",
    "#                 data = scrape_jagatplay_article(url, soup)\n",
    "            \n",
    "#             if data:\n",
    "#                 data['id_dokumen'] = f\"doc_{doc_id_counter:05d}\"\n",
    "#                 data['sumber'] = portal\n",
    "#                 data['url'] = url\n",
    "                \n",
    "#                 df_row = pd.DataFrame([data])\n",
    "#                 df_row = df_row[header]\n",
    "                \n",
    "#                 df_row.to_csv(OUTPUT_CSV_FILE, mode='a', index=False, header=False)\n",
    "                \n",
    "#                 log_to_file(f\"    -> Berhasil: {data['judul'][:50]}...\")\n",
    "                \n",
    "#                 doc_id_counter += 1\n",
    "#                 newly_scraped_count += 1\n",
    "\n",
    "#             time.sleep(0.5)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             log_to_file(f\"    -> Terjadi error kritis saat scraping {url}: {e}\")\n",
    "\n",
    "#     log_to_file(f\"Scraping selesai. Total {newly_scraped_count} artikel BARU berhasil disimpan ke {OUTPUT_CSV_FILE}.\")\n",
    "#     log_to_file(f\"Total artikel keseluruhan: {doc_id_counter - 1}\")\n",
    "#     log_to_file(f\"Proses selesai.\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d951d",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f690435",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b79c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_article = pd.read_csv('../data/scraped_articles.csv')\n",
    "df_article.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistik Deskriptif Untuk Dataframe Articles\")\n",
    "print(df_article.describe())\n",
    "\n",
    "print(\"Tipe Data Dataframe Articles\")\n",
    "print(df_article.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d704",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbca2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = df_article.dropna()\n",
    "print(f\"Jumlah baris sebelum dihandling: {df_article.shape[0]}\")\n",
    "print(f\"Jumlah baris setelah dihandling: {df_null.shape[0]}\")\n",
    "\n",
    "df_null.isnull().sum()\n",
    "df_article = df_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4dfc5",
   "metadata": {},
   "source": [
    "### Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b81c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47ed4a",
   "metadata": {},
   "source": [
    "### Date Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sources = df_article['sumber'].unique()\n",
    "sample_rows = []\n",
    "\n",
    "for source in unique_sources:\n",
    "    sample = df_article[df_article['sumber'] == source].head(1)\n",
    "    sample_rows.append(sample)\n",
    "\n",
    "if sample_rows:\n",
    "    temp_df = pd.concat(sample_rows)\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(temp_df[['sumber', 'tanggal_terbit']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb56deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_article \n",
    "\n",
    "# Dictionary bulan Inggris ke Indonesia\n",
    "bulan_dict = {\n",
    "    'January': 'Januari', 'February': 'Februari', 'March': 'Maret', 'April': 'April',\n",
    "    'May': 'Mei', 'June': 'Juni', 'July': 'Juli', 'August': 'Agustus', 'September': 'September',\n",
    "    'October': 'Oktober', 'November': 'November', 'December':'Desember',\n",
    "    'Jan': 'Januari', 'Feb': 'Februari', 'Mar': 'Maret', 'Apr': 'April', 'May': 'Mei',\n",
    "    'Jun': 'Juni', 'Jul': 'Juli', 'Aug': 'Agustus', 'Sep': 'September', 'Oct': 'Oktober',\n",
    "    'Nov': 'November', 'Dec':'Desember','Agu':'Agustus','Okt':'Oktober','Des':'Desember'\n",
    "}\n",
    "\n",
    "# Pembersihan dan konversi\n",
    "def convert_date(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'N/A'\n",
    "    \n",
    "    original_text = text.strip()\n",
    "\n",
    "    if '|' in original_text:\n",
    "        text = original_text.split('|', 1)[-1].strip()\n",
    "    \n",
    "    if re.search(r'(?i)\\b\\d+\\s+(Hari|Jam)\\s+yang\\s+lalu\\b', text):\n",
    "        return '20 November 2025'\n",
    "    \n",
    "    # Buang hari & jam jika ada\n",
    "    text = re.sub(r'(?i)\\b(Minggu|Senin|Selasa|Rabu|Kamis|Jumat|Sabtu),?\\s*', '', text)\n",
    "    text = re.sub(r'\\b(Minggu|Senin|Selasa|Rabu|Kamis|Jumat|Sabtu)\\s*', '', text)\n",
    "    text = re.sub(r'\\d{2}:\\d{2}$', '', text)\n",
    "    \n",
    "    # Ubah 'July 31, 2012' ke '31 July 2012'\n",
    "    m = re.match(r'([A-Za-z]+) (\\d{1,2}), (\\d{4})', text.strip())\n",
    "    if m:\n",
    "        text = f\"{m.group(2)} {m.group(1)} {m.group(3)}\"\n",
    "    \n",
    "    # Ubah ke list lalu ganti bulan\n",
    "    parts = text.strip().split()\n",
    "    if len(parts) >= 3:\n",
    "        hari, bulan_raw, tahun = parts[0], parts[1], parts[2]\n",
    "    else:\n",
    "        return text.strip()\n",
    "    # Ganti bulan\n",
    "    bulan = bulan_dict.get(bulan_raw, bulan_raw)\n",
    "    # Format ulang\n",
    "    return f\"{hari} {bulan} {tahun}\"\n",
    "\n",
    "df_article['tanggal_terbit_normalized'] = df_article['tanggal_terbit'].apply(convert_date)\n",
    "\n",
    "unique_sources = df_article['sumber'].unique()\n",
    "sample_rows = []\n",
    "\n",
    "for source in unique_sources:\n",
    "    sample = df_article[df_article['sumber'] == source].head(1)\n",
    "    sample_rows.append(sample)\n",
    "\n",
    "if sample_rows:\n",
    "    temp_df = pd.concat(sample_rows)\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(temp_df[['sumber', 'tanggal_terbit', 'tanggal_terbit_normalized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_article_cleaned = df_article.drop('tanggal_terbit', axis=1)\n",
    "df_article_cleaned = df_article_cleaned.rename(columns={'tanggal_terbit_normalized': 'tanggal_terbit'})\n",
    "\n",
    "# Kamus untuk mengubah tanggal_terbit dari format object menjadi datetime\n",
    "ID_TO_EN_MAP = {\n",
    "    'Januari': 'January',\n",
    "    'Februari': 'February',\n",
    "    'Maret': 'March',\n",
    "    'April': 'April',\n",
    "    'Mei': 'May',\n",
    "    'Juni': 'June',\n",
    "    'Juli': 'July',\n",
    "    'Agustus': 'August',\n",
    "    'September': 'September',\n",
    "    'Oktober': 'October',\n",
    "    'November': 'November',\n",
    "    'Desember': 'December'\n",
    "}\n",
    "\n",
    "def convert_indo_date_to_datetime(date_str):\n",
    "    \"\"\"Mengubah string tanggal Indo (15 Agustus 2025) ke datetime object.\"\"\"\n",
    "    if not isinstance(date_str, str) or date_str == 'N/A':\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Terjemahkan nama bulan di dalam string ke Inggris\n",
    "    # Contoh: \"15 Agustus 2025\" -> \"15 August 2025\"\n",
    "    date_str_en = date_str\n",
    "    for id_month, en_month in ID_TO_EN_MAP.items():\n",
    "        if id_month in date_str:\n",
    "            date_str_en = date_str.replace(id_month, en_month)\n",
    "            break\n",
    "            \n",
    "    # Sekarang Pandas bisa membacanya dengan mudah\n",
    "    try:\n",
    "        return pd.to_datetime(date_str_en, format='%d %B %Y')\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# 2. Terapkan ke kolom baru 'timestamp'\n",
    "print(\"Sedang membuat kolom timestamp...\")\n",
    "df_article_cleaned['timestamp'] = df_article_cleaned['tanggal_terbit'].apply(convert_indo_date_to_datetime)\n",
    "\n",
    "# 3. Cek Hasilnya\n",
    "print(\"\\nCek tipe data:\")\n",
    "print(df_article_cleaned.dtypes)\n",
    "\n",
    "print(\"\\nContoh data:\")\n",
    "# Tampilkan kolom tanggal (string indo) dan timestamp (datetime) berdampingan\n",
    "display(df_article_cleaned[['tanggal_terbit', 'timestamp']].head())\n",
    "\n",
    "# Opsional: Cek apakah masih ada NaT (selain yang memang N/A)\n",
    "jumlah_nat = df_article_cleaned['timestamp'].isna().sum()\n",
    "print(f\"\\nJumlah baris yang gagal dikonversi (NaT): {jumlah_nat}\")\n",
    "timestamp_nat = df_article_cleaned['timestamp'].isna()\n",
    "failed_rows = df_article_cleaned[timestamp_nat]\n",
    "\n",
    "if not failed_rows.empty:\n",
    "    print(f\"\\nDitemukan {len(failed_rows)} baris yang gagal dikonversi menjadi datetime (NaT).\")\n",
    "    print(\"Berikut adalah sampel dari baris-baris yang gagal tersebut:\")\n",
    "    \n",
    "    # 'tanggal_terbit' adalah kolom yang paling penting untuk dianalisis\n",
    "    display(failed_rows[['sumber', 'tanggal_terbit', 'timestamp']])\n",
    "    \n",
    "    print(\"\\n--- Analisis Frekuensi Format Tanggal yang Gagal ---\")\n",
    "    print(\"Berikut adalah format-format tanggal unik yang paling sering menyebabkan kegagalan:\")\n",
    "    \n",
    "    display(failed_rows['tanggal_terbit'].value_counts().head(20))\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSelamat! Tidak ada baris dengan nilai NaT di kolom 'timestamp'.\")\n",
    "\n",
    "print(df_article_cleaned.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "TODAY = datetime(2025, 11, 10)\n",
    "\n",
    "ID_TO_EN_MAP = {\n",
    "    'Januari': 'January', 'Februari': 'February', 'Maret': 'March', 'April': 'April',\n",
    "    'Mei': 'May', 'Juni': 'June', 'Juli': 'July', 'Agustus': 'August',\n",
    "    'September': 'September', 'Oktober': 'October', 'November': 'November', 'Desember': 'December'\n",
    "}\n",
    "\n",
    "def convert_to_datetime(date_str):\n",
    "    if not isinstance(date_str, str) or date_str.lower() == 'n/a':\n",
    "        return pd.NaT\n",
    "    \n",
    "    match = re.search(r'(\\d+)\\s+Hari yang', date_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        days_ago = int(match.group(1))\n",
    "        calculated_date = TODAY - timedelta(days=days_ago)\n",
    "        return calculated_date\n",
    "    \n",
    "    date_str_en = date_str\n",
    "    for id_month, en_month in ID_TO_EN_MAP.items():\n",
    "        if id_month in date_str:\n",
    "            date_str_en = date_str.replace(id_month, en_month)\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(date_str_en, format='%d %B %Y')\n",
    "    except Exception:\n",
    "        # Jika gagal, kembalikan NaT\n",
    "        print(f\"Gagal mem-parsing tanggal absolut: '{date_str}'\")\n",
    "        return pd.NaT\n",
    "\n",
    "print(\"--- DataFrame SEBELUM konversi tanggal relatif ---\")\n",
    "df_article_cleaned.head()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Terapkan fungsi konversi baru untuk membuat kolom 'timestamp'\n",
    "print(\"Mengonversi semua format tanggal ke tipe datetime...\")\n",
    "df_article_cleaned['timestamp'] = df_article_cleaned['tanggal_terbit'].apply(convert_to_datetime)\n",
    "\n",
    "def format_to_indonesian_str(dt_object):\n",
    "    if pd.isna(dt_object):\n",
    "        return 'N/A'\n",
    "    english_date_str = dt_object.strftime('%d %B %Y')\n",
    "    for en_month, id_month in ID_TO_EN_MAP.items():\n",
    "        english_date_str = english_date_str.replace(en_month, id_month)\n",
    "    return english_date_str\n",
    "\n",
    "# Timpa kolom 'tanggal_terbit' yang lama dengan format yang sudah konsisten\n",
    "df_article_cleaned['tanggal_terbit'] = df_article_cleaned['timestamp'].apply(format_to_indonesian_str)\n",
    "\n",
    "print(\"Konversi selesai.\")\n",
    "print(\"\\n--- DataFrame SETELAH konversi ---\")\n",
    "df_article_cleaned.head()\n",
    "print(\"\\nTipe data akhir:\")\n",
    "df_article_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article = df_article_cleaned\n",
    "df_article.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a25ec",
   "metadata": {},
   "source": [
    "### Content Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4b17b",
   "metadata": {},
   "source": [
    "#### Create Whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f22ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Daftar kata kerja/umum yang HARUS DIKELUARKAN dari WHITELIST agar bisa di-stemming\n",
    "WORDS_TO_EXCLUDE_FROM_WHITELIST = set([\n",
    "    \"bawa\", \"beli\", \"bepergian\", \"beredar\", \"berencana\", \"berhasil\", \"berkurang\", \"bermain\", \"beruntunnya\", \n",
    "    \"diam\", \"dibatalkan\", \"dicap\", \"diduga\", \"digelar\", \"dihapus\", \"dihentikan\", \"dikembangkan\", \"dikenalkan\", \n",
    "    \"dilengkapi\", \"dirilis\", \"dirumorkan\", \"disebar\", \"diskon\", \"ditangkap\", \"ditolak\", \"ditunda\", \"diumumkan\", \n",
    "    \"diungkap\", \"habiskan\", \"hadir\", \"hadirkan\", \"kecanduan\", \"kecipratan\", \"kejar\", \"kelarkan\", \"kesalahan\", \n",
    "    \"ketahuan\", \"ketahui\", \"keuntungan\", \"melengkapi\", \"meluncur\", \"memainkannya\", \"memanjat\", \"memilih\",\n",
    "    \"menanggapinya\", \"menawarkan\", \"mencapai\", \"mencekam\", \"mengalami\", \"mengambil\", \"mengatasi\", \"mengerti\", \n",
    "    \"menghabiskan\", \"menghilang\", \"meningkat\", \"menjaga\", \"merawat\", \"merusak\", \"muncul\", \"paham\", \"pakai\", \n",
    "    \"pamer\", \"pamerkan\", \"perbincangan\", \"percaya\", \"perdana\", \"performa\", \"perilisan\", \"perlihatkan\", \n",
    "    \"perusahaan\", \"terbakar\", \"terbang\", \"tercepat\", \"terhentikan\", \"terinspirasi\", \"terjual\", \"terlaris\", \n",
    "    \"tetapkan\", \"tambahkan\", \"tanggapi\", \"umumkan\",\n",
    "    \n",
    "    # Tambahan lain yang bersifat umum (dari contoh sebelumnya)\n",
    "    \"cara\", \"tips\", \"trik\", \"terbaru\", \"akan\", \"bikin\", \"terbaik\", \"terlengkap\", \"wajib\", \"dibuka\", \"dimainkan\", \n",
    "    \"gratis\", \"ketagihan\", \"seru\", \"lupa\", \"coba\", \"dicoba\", \"keren\", \"mainkan\", \"banget\", \"lengkap\",\n",
    "    \"pembayaran\", \"pemecatan\", \"pemerintahan\", \"pengalaman\", \"tanggal\", \"director\", \"nggak\", \"orang\", \"dirumorkan\",\n",
    "    \"ps\", \"dirilis\"\n",
    "])\n",
    "\n",
    "\n",
    "def clean_for_ngram(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "df_article['judul_clean'] = df_article['judul'].apply(clean_for_ngram)\n",
    "\n",
    "# Menggunakan Stopwords Gabungan (Indonesia dan Inggris)\n",
    "STOPWORDS_COMBINED = set(stopwords.words('indonesian')) | set(stopwords.words('english'))\n",
    "\n",
    "def get_top_ngrams(corpus, n=2, top_k=500):\n",
    "    \"\"\"Fungsi untuk mendapatkan N-gram yang paling sering muncul.\"\"\"\n",
    "    \n",
    "    # Inisialisasi CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words=list(STOPWORDS_COMBINED))\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    word_counts = X.sum(axis=0)\n",
    "    word_freq = [(word, word_counts[0, idx]) \n",
    "                 for word, idx in vectorizer.vocabulary_.items()]\n",
    "    \n",
    "    word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "    return word_freq[:top_k]\n",
    "\n",
    "# Ekstrak N-gram\n",
    "top_bigrams = get_top_ngrams(df_article['judul_clean'], n=2, top_k=500)\n",
    "top_trigrams = get_top_ngrams(df_article['judul_clean'], n=3, top_k=500)\n",
    "\n",
    "print(\"### Top 20 Bigram ###\")\n",
    "for phrase, count in top_bigrams[:20]:\n",
    "    print(f\"'{phrase}' (Count: {count})\")\n",
    "\n",
    "print(\"\\n### Top 20 Trigram ###\")\n",
    "for phrase, count in top_trigrams[:20]:\n",
    "    print(f\"'{phrase}' (Count: {count})\")\n",
    "\n",
    "# 1. Kumpulkan semua kata kandidat dari N-gram\n",
    "potential_whitelist_words = set()\n",
    "\n",
    "for phrase, count in top_bigrams:\n",
    "    for word in phrase.split():\n",
    "        potential_whitelist_words.add(word)\n",
    "\n",
    "for phrase, count in top_trigrams:\n",
    "    for word in phrase.split():\n",
    "        potential_whitelist_words.add(word)\n",
    "        \n",
    "# 2. Tambahkan Jargon/Akronim Hardcoded (sebelum filtering)\n",
    "potential_whitelist_words.update([\n",
    "    \"the\", \"of\", \"and\", \"or\", \"in\", \"with\", \n",
    "    \"buff\", \"nerf\", \"meta\", \"patch\", \"dlc\", \"rts\", \"fps\", \"moba\", \"rpg\",\n",
    "    \"pc\", \"ps5\", \"xbox\", \"nintendo\", \"steam\", \"mobile\", \"e3\" # Penting untuk akronim/entitas\n",
    "])\n",
    "\n",
    "print(f\"\\nTotal kata unik (token) sebelum filtering: {len(potential_whitelist_words)}\")\n",
    "\n",
    "# 3. FILTERING FINAL: Hanya kata yang TIDAK termasuk WORDS_TO_ALLOW_STEMMING\n",
    "final_whitelist_words = [\n",
    "    word for word in potential_whitelist_words if word not in WORDS_TO_EXCLUDE_FROM_WHITELIST\n",
    "]\n",
    "\n",
    "# Nama file yang akan digunakan untuk menyimpan whitelist\n",
    "FILE_PATH = '../data/whitelist.txt'\n",
    "\n",
    "with open(FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "    for word in final_whitelist_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"\\n✅ Whitelist bersih (total {len(final_whitelist_words)} kata) berhasil disimpan ke '{FILE_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bed402",
   "metadata": {},
   "source": [
    "#### Create Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92764462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import os \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# =======================================================\n",
    "# 1. FUNGSI DAN MUAT KOMPONEN\n",
    "# =======================================================\n",
    "\n",
    "# 🚨 Penambahan try-except untuk download NLTK stopwords\n",
    "try:\n",
    "    stopwords.words('indonesian')\n",
    "except LookupError:\n",
    "    print(\"⏳ NLTK stopwords Bahasa Indonesia belum diunduh. Sedang mengunduh...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"✅ Pengunduhan NLTK stopwords selesai.\")\n",
    "\n",
    "def load_words_set(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Peringatan: File {file_path} tidak ditemukan. Menggunakan set kosong.\")\n",
    "        return set()\n",
    "\n",
    "# Asumsi lokasi file Whitelist\n",
    "WHITELIST_FILE_PATH = '../data/whitelist.txt'\n",
    "WHITELIST = load_words_set(WHITELIST_FILE_PATH)\n",
    "\n",
    "# Menggunakan Stoplist Bahasa Indonesia dari NLTK\n",
    "NLTK_ID_STOPWORDS = set(stopwords.words('indonesian'))\n",
    "STANDARD_STOPWORDS = NLTK_ID_STOPWORDS \n",
    "\n",
    "# --- Kata-kata yang TIDAK BOLEH ADA di Final Stoplist ---\n",
    "WORDS_TO_ALLOW_STEMMING = set([\n",
    "    \"bawa\", \"beli\", \"bepergian\", \"beredar\", \"berencana\", \"berhasil\", \"berkurang\", \"bermain\", \"beruntunnya\", \n",
    "    \"diam\", \"dibatalkan\", \"dicap\", \"diduga\", \"digelar\", \"dihapus\", \"dihentikan\", \"dikembangkan\", \"dikenalkan\", \n",
    "    \"dilengkapi\", \"dirilis\", \"dirumorkan\", \"disebar\", \"diskon\", \"ditangkap\", \"ditolak\", \"ditunda\", \"diumumkan\", \n",
    "    \"diungkap\", \"habiskan\", \"hadir\", \"hadirkan\", \"kecanduan\", \"kecipratan\", \"kejar\", \"kelarkan\", \"kesalahan\", \n",
    "    \"ketahuan\", \"ketahui\", \"keuntungan\", \"melengkapi\", \"meluncur\", \"memainkannya\", \"memanjat\", \"memilih\",\n",
    "    \"menanggapinya\", \"menawarkan\", \"mencapai\", \"mencekam\", \"mengalami\", \"mengambil\", \"mengatasi\", \"mengerti\", \n",
    "    \"menghabiskan\", \"menghilang\", \"meningkat\", \"menjaga\", \"merawat\", \"merusak\", \"muncul\", \"paham\", \"pakai\", \n",
    "    \"pamer\", \"pamerkan\", \"perbincangan\", \"percaya\", \"perdana\", \"performa\", \"perilisan\", \"perlihatkan\", \n",
    "    \"perusahaan\", \"terbakar\", \"terbang\", \"tercepat\", \"terhentikan\", \"terinspirasi\", \"terjual\", \"terlaris\", \n",
    "    \"tetapkan\", \"tambahkan\", \"tanggapi\", \"umumkan\"\n",
    "])\n",
    "\n",
    "\n",
    "CURATED_ADDITIONAL_STOPWORDS = set([\n",
    "    \"baca\", \"juga\", \"berita\", \"lain\", \"kembali\", \"lanjut\", \n",
    "    \"sob\", \"gaes\", \"yuk\", \"dunia\", \"maya\", \"segera\", \"download\", \n",
    "    \"jangan\", \"lupa\", \"contoh\", \"terkait\", \"halaman\", \"akhir\", \"artikel\"\n",
    "])\n",
    "\n",
    "# =======================================================\n",
    "# 2. PEMBENTUKAN FINAL STOPLIST\n",
    "# =======================================================\n",
    "\n",
    "# 1. Gabungkan semua kata yang ingin dihapus (STANDAR NLTK ID + CURATED NOISE)\n",
    "all_stopwords_to_remove = STANDARD_STOPWORDS | CURATED_ADDITIONAL_STOPWORDS\n",
    "\n",
    "# 2. Kurangi dengan WHITELIST (untuk melindungi entitas/jargon)\n",
    "FINAL_STOPLIST = all_stopwords_to_remove - WHITELIST\n",
    "\n",
    "# 3. KOREKSI TAMBAHAN: Pastikan semua kata kerja yang diizinkan untuk stemming TIDAK masuk ke stoplist\n",
    "FINAL_STOPLIST = FINAL_STOPLIST - WORDS_TO_ALLOW_STEMMING\n",
    "\n",
    "print(f\"\\nTotal Stopwords Kandidat (NLTK ID + Tambahan): {len(all_stopwords_to_remove)}\")\n",
    "print(f\"Total Kata Kunci di Whitelist: {len(WHITELIST)}\")\n",
    "print(f\"Total Kata Kerja yang Diizinkan Stemming: {len(WORDS_TO_ALLOW_STEMMING)}\")\n",
    "print(f\"✅ Total kata di FINAL STOPLIST: {len(FINAL_STOPLIST)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- PENYIMPANAN FINAL STOPLIST KE FILE TEKS ---\n",
    "FINAL_STOPLIST_FILE_PATH = '../data/final_stopwords.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(FINAL_STOPLIST_FILE_PATH)):\n",
    "    os.makedirs(os.path.dirname(FINAL_STOPLIST_FILE_PATH))\n",
    "\n",
    "with open(FINAL_STOPLIST_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(list(FINAL_STOPLIST)): \n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"✅ FINAL STOPLIST berhasil disimpan ke '{FINAL_STOPLIST_FILE_PATH}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20430d3",
   "metadata": {},
   "source": [
    "#### Start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "df_article\n",
    "\n",
    "WHITELIST_FILE_PATH = \"../data/whitelist.txt\"\n",
    "FINAL_STOPLIST_PATH = \"../data/final_stopwords.txt\"\n",
    "CHECKPOINT_FILE_PATH = \"../data/preprocessing_checkpoint.txt\"\n",
    "OUTPUT_CSV_FILE = \"../data/processed_data.csv\"\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def load_file(file_path):\n",
    "    \"\"\"Memuat set kata dari file teks.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\" Peringatan: File {file_path} tidak ditemukan. Menggunakan set kosong.\")\n",
    "        return set()\n",
    "    \n",
    "WHITELIST = load_words_set(WHITELIST_FILE_PATH)\n",
    "FINAL_STOPLIST = load_words_set(FINAL_STOPLIST_FILE_PATH)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<[^>]*>', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    tokens_filtered = [word for word in tokens if word not in FINAL_STOPLIST]\n",
    "\n",
    "    stemmed_tokens = []\n",
    "    for word in tokens_filtered:\n",
    "        if any(char.isalpha() for char in word) and any(char.isdigit() for char in word):\n",
    "            stemmed_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word.isdigit():\n",
    "            stemmed_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in WHITELIST:\n",
    "            stemmed_tokens.append(word)\n",
    "            continue # Tambahkan continue agar tidak jatuh ke stemming di 'else'\n",
    "        \n",
    "        stemmed_tokens.append(stemmer.stem(word))\n",
    "    \n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "if 'konten_processed' not in df_article.columns:\n",
    "    df_article['konten_processed'] = np.nan\n",
    "\n",
    "total_rows = len(df_article)\n",
    "\n",
    "start_index = 0\n",
    "if os.path.exists(CHECKPOINT_FILE_PATH):\n",
    "    with open(CHECKPOINT_FILE_PATH, 'r') as f:\n",
    "        try:\n",
    "            start_index = int(f.read().strip())\n",
    "        except ValueError:\n",
    "            start_index = 0\n",
    "\n",
    "if start_index >= total_rows:\n",
    "    print(\"Preprocessing selesai\")\n",
    "else:\n",
    "    print(f\"\\n--- Memulai Proses Preprocessing Kolom 'konten_separated' ---\")\n",
    "    print(f\"Melanjutkan dari baris ke-{start_index} dari {total_rows}...\")\n",
    "\n",
    "    for i in tqdm(range(start_index, total_rows),\n",
    "                  initial=start_index,\n",
    "                  total=total_rows,\n",
    "                  desc=\"Preprocessing Konten\"):\n",
    "    \n",
    "        if pd.isna(df_article.iloc[i]['konten_processed']):\n",
    "            text_to_process = df_article.iloc[i]['konten']\n",
    "            processed_text = preprocess_text(text_to_process)\n",
    "            df_article.iloc[i, df_article.columns.get_loc('konten_processed')] = processed_text\n",
    "\n",
    "            if (i + 1) % 1000 == 0 or i == total_rows - 1:\n",
    "                with open(CHECKPOINT_FILE_PATH, 'w') as f:\n",
    "                    f.write(str(i + 1))\n",
    "    \n",
    "    print(\"SELESAI\")\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE_PATH) and df_article['konten_processed'].notna().all():\n",
    "    os.remove(CHECKPOINT_FILE_PATH)\n",
    "    print(\"File checkpoint dihapus.\")\n",
    "\n",
    "if 'konten' in df_article.columns and len(df_article) > 0:\n",
    "    print(\"\\n--- Contoh Perbandingan Hasil ---\")\n",
    "    \n",
    "    for i in range(min(5, total_rows)):\n",
    "        print(f\"\\nBaris ke-{i}:\")\n",
    "        print(f\"Konten Asli: {str(df_article.iloc[i]['konten'])[:100]}...\")\n",
    "        print(f\"Konten Diproses: {df_article.iloc[i]['konten_processed']}\")\n",
    "else:\n",
    "    print(\"DataFrame sampel kosong atau kolom 'konten' tidak ada.\")\n",
    "\n",
    "# Ekstrak DataFrame yang sudah diproses ke CSV baru\n",
    "df_article.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "print(f\"\\nDataFrame telah diekstrak ke '{OUTPUT_CSV_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4874f47",
   "metadata": {},
   "source": [
    "### INDEXING AND MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MODEL_DIR = \"../models/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "BM25_MODEL_PATH = os.path.join(MODEL_DIR, 'bm25_model.pkl')\n",
    "CORPUS_DF_PATH = os.path.join(MODEL_DIR, 'df_corpus.pkl')\n",
    "\n",
    "SBERT_MODEL_PATH = os.path.join(MODEL_DIR, 'sbert_model.pkl')\n",
    "SBERT_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'sbert_embeddings.npy')\n",
    "\n",
    "df = pd.read_csv('../data/processed_data.csv')\n",
    "df = df.drop_duplicates(subset=['konten_processed'], keep='first')\n",
    "\n",
    "# BM25\n",
    "\n",
    "# Tokenizing corpus\n",
    "tokenized_corpus = df['konten_processed'].apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "print(\"BM25 Indexing Selesai.\")\n",
    "print(f\"Total dokumen terindeks oleh BM25: {len(tokenized_corpus)}\")\n",
    "\n",
    "print(\"\\nEXPORTING BM25 MODEL\")\n",
    "try:\n",
    "    with open(BM25_MODEL_PATH, \"wb\") as f:\n",
    "        pickle.dump(bm25, f)\n",
    "    print(f\"Model BM25 berhasil disimpan ke: {BM25_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan BM25: {e}\")\n",
    "\n",
    "try:\n",
    "    df.to_pickle(CORPUS_DF_PATH)\n",
    "    print(f\"DataFrame Corpus berhasil disimpan ke: {CORPUS_DF_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan DataFrame: {e}\")\n",
    "\n",
    "\n",
    "# SBERT\n",
    "model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "sbert_model = SentenceTransformer(model_name)\n",
    "\n",
    "corpus_texts = df['konten'].astype(str).tolist()\n",
    "\n",
    "corpus_embeddings = sbert_model.encode(\n",
    "    corpus_texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "\n",
    "corpus_embeddings = corpus_embeddings.cpu().numpy()\n",
    "print(\"S-BERT Indexing (Embeddings) Selesai.\")\n",
    "print(\"\\nEXPORTING SBERT MODEL\")\n",
    "\n",
    "try:\n",
    "    np.save(SBERT_EMBEDDINGS_PATH, corpus_embeddings)\n",
    "    print(f\"Corpus Embeddings berhasil disimpan ke: {SBERT_EMBEDDINGS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan Embeddings: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(SBERT_MODEL_PATH, 'wb') as f:\n",
    "        pickle.dump(model_name, f)\n",
    "    print(f\"Nama Model SBERT berhasil disimpan ke: {SBERT_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saat menyimpan nama model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da0350",
   "metadata": {},
   "source": [
    "Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0957e6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Sebelum cleaning: 2029 baris\n",
      "Setelah drop missing/konten pendek: 2020 baris\n",
      "Setelah drop duplikat: 1822 baris\n",
      "Membersihkan HTML dari kolom 'konten'...\n",
      "\n",
      "SELESAI! Dataset pkl bersih disimpan di:\n",
      "./df_corpus_clean.pkl\n",
      "\n",
      "SELESAI! Dataset csv bersih disimpan di:\n",
      "./df_corpus_clean.csv\n",
      "Total dokumen akhir: 1822\n",
      "Kolom yang tersedia: ['id_dokumen', 'sumber', 'url', 'judul', 'konten', 'tanggal_terbit', 'url_thumbnail', 'konten_clean', 'judul_clean', 'doc_id']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 1. Load dataset mentah\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../data/scraped_articles.csv')\n",
    "\n",
    "print(f\"Sebelum cleaning: {len(df)} baris\")\n",
    "\n",
    "# 2. Hapus baris yang kolom pentingnya kosong\n",
    "# Sesuaikan nama kolom sesuai datasetmu (biasanya: 'judul', 'konten', 'tanggal', dll.)\n",
    "important_cols = ['konten']                    # minimal wajib ada konten\n",
    "optional_cols  = ['judul', 'tanggal', 'url']  # tambahkan kalau ada\n",
    "\n",
    "# Drop baris yang semua important_cols NaN\n",
    "df.dropna(subset=important_cols, how='all', inplace=True)\n",
    "\n",
    "# Drop baris yang kontennya terlalu pendek atau hanya whitespace (sering terjadi saat scraping gagal)\n",
    "df = df[df['konten'].notna()]                                      # pastikan konten tidak NaN\n",
    "df = df[df['konten'].apply(lambda x: len(str(x).strip()) > 50)]    # minimal 50 karakter\n",
    "\n",
    "print(f\"Setelah drop missing/konten pendek: {len(df)} baris\")\n",
    "\n",
    "# 3. Hapus duplikat berdasarkan konten atau URL (pilih salah satu atau gabung)\n",
    "# Pilih yang paling cocok dengan datamu\n",
    "if 'url' in df.columns:\n",
    "    df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
    "else:\n",
    "    df.drop_duplicates(subset=['konten'], keep='first', inplace=True)\n",
    "\n",
    "print(f\"Setelah drop duplikat: {len(df)} baris\")\n",
    "\n",
    "# 4. Bersihkan tag HTML dan karakter aneh\n",
    "def clean_html(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Hapus tag HTML\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "    # Bersihkan whitespace berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Membersihkan HTML dari kolom 'konten'...\")\n",
    "df['konten_clean'] = df['konten'].apply(clean_html)\n",
    "\n",
    "# Opsional: bersihkan juga kolom judul kalau ada\n",
    "if 'judul' in df.columns:\n",
    "    df['judul_clean'] = df['judul'].apply(clean_html)\n",
    "else:\n",
    "    df['judul_clean'] = \"\"  # buat kolom kosong biar konsisten\n",
    "\n",
    "# 5. Reset index dan buat kolom ID yang permanen (penting untuk ground truth!)\n",
    "df = df.reset_index(drop=True)\n",
    "df['doc_id'] = df.index  # ID dari 0 sampai N-1, ini yang akan kamu pakai di GROUND_TRUTH\n",
    "\n",
    "# 6. Simpan hasil cleaning\n",
    "output_path_pkl = './df_corpus_clean.pkl'\n",
    "output_path_csv = './df_corpus_clean.csv'\n",
    "df.to_pickle(output_path_pkl)\n",
    "df.to_csv(output_path_csv)\n",
    "print(f\"\\nSELESAI! Dataset pkl bersih disimpan di:\\n{output_path_pkl}\")\n",
    "print(f\"\\nSELESAI! Dataset csv bersih disimpan di:\\n{output_path_csv}\")\n",
    "print(f\"Total dokumen akhir: {len(df)}\")\n",
    "print(\"Kolom yang tersedia:\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b74246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba7016a76364fe88bd01d441b7ac530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 26/1822 [02:57<3:24:39,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Selesai! File tersimpan:\n",
      "- queries.csv\n",
      "- groundtruth.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 📌 IMPORT LIBRARY\n",
    "# ==========================\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==========================\n",
    "# 🔐 SETUP GEMINI API KEY\n",
    "# ==========================\n",
    "genai.configure(api_key=\"AIzaSyAxgvosCQzZsQ6xrb51Dmi3XThz0bGz6yQ\")\n",
    "\n",
    "# ==========================\n",
    "# 🤖 MODEL PEMROSES TEKS\n",
    "# ==========================\n",
    "# SBERT untuk similarity dokumen\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ==========================\n",
    "# 📂 LOAD DATASET\n",
    "# Pastikan berisi kolom:\n",
    "# id_dokumen, judul, konten\n",
    "# ==========================\n",
    "df = pd.read_csv(\"df_corpus_clean.csv\")\n",
    "\n",
    "# ==========================\n",
    "# 🧹 GABUNG TEKS UNTUK AMBIL KONTEXT\n",
    "# ==========================\n",
    "df[\"text\"] = df[\"judul\"].astype(str) + \". \" + df[\"konten\"].astype(str)\n",
    "\n",
    "# ==========================\n",
    "# 🔢 PARAMETER GENERASI QUERY\n",
    "# ==========================\n",
    "N_QUERY_PER_DOC = 2       # per dokumen di-generate 2 query\n",
    "MAX_QUERY = 50            # maksimal 50 query total\n",
    "SBERT_THRESHOLD = 0.60    # threshold cosine similarity\n",
    "MIN_RELEVANT = 3\n",
    "MAX_RELEVANT = 5\n",
    "\n",
    "# ==========================\n",
    "# 🤖 FUNGSI: GENERATE QUERY DARI GEMINI\n",
    "# ==========================\n",
    "def generate_query_from_text(text):\n",
    "    prompt = f\"\"\"\n",
    "Buat 2 query pencarian manusia (maks 7 kata) berdasarkan teks berita berikut.\n",
    "❌ Tidak boleh menyalin judul.\n",
    "⭕ Query seperti pengguna ingin mencari informasi di Google.\n",
    "🌐 Boleh campuran Indonesia + Inggris sederhana.\n",
    "⚠️ Tidak boleh berupa ringkasan kalimat lengkap.\n",
    "\n",
    "Teks:\n",
    "{text[:500]}\n",
    "\"\"\"\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    queries = [q.strip(\"-• \") for q in response.text.split(\"\\n\") if len(q.strip()) > 2]\n",
    "    queries = [q for q in queries if 2 <= len(q.split()) <= 7]  # filter sesuai aturan\n",
    "    return queries[:N_QUERY_PER_DOC]\n",
    "\n",
    "# ==========================\n",
    "# 🔍 HITUNG SBERT EMBEDDINGS SEKALI SAJA\n",
    "# ==========================\n",
    "embeddings = sbert_model.encode(df[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# ==========================\n",
    "# 🧠 PROSES GENERATE QUERY + GROUND TRUTH\n",
    "# ==========================\n",
    "query_list = []\n",
    "gt_list = []\n",
    "\n",
    "query_counter = 0\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if query_counter >= MAX_QUERY:\n",
    "        break\n",
    "    \n",
    "    queries = generate_query_from_text(row[\"text\"])\n",
    "    doc_embedding = embeddings[idx].reshape(1, -1)\n",
    "\n",
    "    # Cari dokumen serupa\n",
    "    sims = cosine_similarity(doc_embedding, embeddings)[0]\n",
    "    similar_idx = sims.argsort()[::-1]  # sorted descending\n",
    "\n",
    "    # Ambil relevan selain dirinya jika perlu\n",
    "    relevant_docs = []\n",
    "    for i in similar_idx:\n",
    "        doc_id_val = df.loc[int(i), \"id_dokumen\"]\n",
    "        if sims[i] < SBERT_THRESHOLD and len(relevant_docs) >= MAX_RELEVANT:\n",
    "            continue\n",
    "        relevant_docs.append(str(doc_id_val))\n",
    "        if len(relevant_docs) >= MAX_RELEVANT:\n",
    "            break\n",
    "\n",
    "    # Jika relevan < MIN_RELEVANT → paksa ambil top-N\n",
    "    if len(relevant_docs) < MIN_RELEVANT:\n",
    "        relevant_docs = [str(df.loc[int(i), \"id_dokumen\"]) for i in similar_idx[:MAX_RELEVANT]]\n",
    "\n",
    "    # simpan untuk setiap query\n",
    "    for q in queries:\n",
    "        qid = f\"Q{query_counter+1:03d}\"\n",
    "        query_list.append([qid, q])\n",
    "        for d in relevant_docs:\n",
    "            gt_list.append([qid, q, d, 1])  # MASUKKAN_API_KEYMU_DI_SINIelevan selalu bernilai 1\n",
    "        \n",
    "        query_counter += 1\n",
    "        if query_counter >= MAX_QUERY:\n",
    "            break\n",
    "\n",
    "# ==========================\n",
    "# 💾 EXPORT CSV\n",
    "# ==========================\n",
    "df_queries = pd.DataFrame(query_list, columns=[\"query_id\", \"query\"])\n",
    "df_gt = pd.DataFrame(gt_list, columns=[\"query_id\", \"query\", \"doc_id\", \"relevance\"])\n",
    "\n",
    "df_queries.to_csv(\"queries.csv\", index=False)\n",
    "df_gt.to_csv(\"groundtruth.csv\", index=False)\n",
    "\n",
    "print(\"🎉 Selesai! File tersimpan:\")\n",
    "print(\"- queries.csv\")\n",
    "print(\"- groundtruth.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b749adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# =======================================================\n",
    "# 1. PATH DAN MUAT ASET MODEL\n",
    "# =======================================================\n",
    "\n",
    "MODEL_DIR = '../models/'\n",
    "BM25_MODEL_PATH = os.path.join(MODEL_DIR, 'bm25_model.pkl')\n",
    "CORPUS_DF_PATH = os.path.join(MODEL_DIR, 'df_corpus.pkl')\n",
    "SBERT_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'sbert_embeddings.npy')\n",
    "SBERT_MODEL_PATH = os.path.join(MODEL_DIR, 'sbert_model.pkl')\n",
    "\n",
    "ASSETS = {}\n",
    "SBERT_MODEL = None\n",
    "BM25_MODEL = None\n",
    "CORPUS_DF = None\n",
    "SBERT_EMBEDDINGS = None\n",
    "\n",
    "def load_evaluation_assets():\n",
    "    \"\"\"Memuat semua aset indexing yang sudah diekspor.\"\"\"\n",
    "    global SBERT_MODEL, BM25_MODEL, CORPUS_DF, SBERT_EMBEDDINGS\n",
    "    try:\n",
    "        # Muat Aset BM25\n",
    "        with open(BM25_MODEL_PATH, 'rb') as f:\n",
    "            BM25_MODEL = pickle.load(f)\n",
    "        ASSETS['corpus_df'] = pd.read_pickle(CORPUS_DF_PATH)\n",
    "        CORPUS_DF = ASSETS['corpus_df']\n",
    "        \n",
    "        # Muat Aset S-BERT\n",
    "        SBERT_EMBEDDINGS = np.load(SBERT_EMBEDDINGS_PATH)\n",
    "        with open(SBERT_MODEL_PATH, 'rb') as f:\n",
    "            model_name = pickle.load(f)\n",
    "        SBERT_MODEL = SentenceTransformer(model_name)\n",
    "        \n",
    "        print(\"✅ Semua aset berhasil dimuat untuk evaluasi.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR saat memuat aset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Memuat aset\n",
    "try:\n",
    "    load_evaluation_assets()\n",
    "except Exception:\n",
    "    # Menghentikan script jika aset gagal dimuat\n",
    "    exit()\n",
    "\n",
    "# =======================================================\n",
    "# 2. DEFINISI FUNGSI PENCARIAN\n",
    "# =======================================================\n",
    "\n",
    "tokenized_corpus = CORPUS_DF['konten_processed'].apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "def get_bm25_ranking(query_tokens, top_k):\n",
    "    \"\"\"Mengembalikan daftar indeks dokumen yang diperingkat oleh BM25.\"\"\"\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    doc_scores = bm25.get_scores(query_tokens)\n",
    "    ranked_indices = np.argsort(doc_scores)[::-1]\n",
    "    return ranked_indices[:top_k]\n",
    "\n",
    "def get_sbert_ranking(query, top_k):\n",
    "    \"\"\"Mengembalikan daftar indeks dokumen yang diperingkat oleh S-BERT.\"\"\"\n",
    "    query_embedding = SBERT_MODEL.encode(query, convert_to_tensor=False)\n",
    "    similarities = cosine_similarity(query_embedding.reshape(1, -1), SBERT_EMBEDDINGS)[0]\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    return ranked_indices[:top_k]\n",
    "\n",
    "# =======================================================\n",
    "# 3. FUNGSI PERHITUNGAN METRIK (HELPER)\n",
    "# =======================================================\n",
    "\n",
    "# Precision\n",
    "def calculate_precision(ranked_indices, true_relevant_set, k):\n",
    "    retrieved_k = set(ranked_indices[:k])\n",
    "    relevant_retrieved = retrieved_k.intersection(true_relevant_set)\n",
    "    return len(relevant_retrieved) / k\n",
    "\n",
    "# Recall\n",
    "def calculate_recall(ranked_indices, true_relevant_set):\n",
    "    retrieved_all = set(ranked_indices)\n",
    "    relevant_retrieved = retrieved_all.intersection(true_relevant_set)\n",
    "    total_relevant = len(true_relevant_set)\n",
    "    return len(relevant_retrieved) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "# F1-score\n",
    "\n",
    "# MAP\n",
    "def calculate_average_precision(ranked_indices, true_relevant_set):\n",
    "    if not true_relevant_set:\n",
    "        return 0.0\n",
    "    \n",
    "    hits = 0\n",
    "    sum_precisions = 0.0\n",
    "    for i, doc_index in enumerate(ranked_indices):\n",
    "        if doc_index in true_relevant_set:\n",
    "            hits += 1\n",
    "            precision_at_k = hits / (i + 1)\n",
    "            sum_precisions += precision_at_k\n",
    "            \n",
    "    return sum_precisions / len(true_relevant_set)\n",
    "\n",
    "# =======================================================\n",
    "# 4. DATA GROUND TRUTH (MANUAL INPUT)\n",
    "# =======================================================\n",
    "\n",
    "EVAL_QUERIES = [\n",
    "    # 5 Query Awal (Contoh)\n",
    "    \"spesifikasi PC untuk bermain game terbaru\",\n",
    "    \"build hero arlott mobile legends\",\n",
    "    \"review Elden Ring Shadow of the Erdtree\",\n",
    "    \"daftar game RPG terbaik di Android 2025\",\n",
    "    \"trailer game yang akan rilis tahun depan\",\n",
    "\n",
    "    # 25 Query Tambahan (Terstruktur)\n",
    "    \n",
    "    # A. Literal & Jargon (Menguji BM25)\n",
    "    \"Kode redeem Genshin Impact bulan ini\",\n",
    "    \"Tanggal rilis resmi Final Fantasy XVI\",\n",
    "    \"Harga dan spesifikasi PC minimal Cyberpunk 2077\",\n",
    "    \"Patch note buff dan nerf hero Mobile Legends\",\n",
    "    \"Jadwal playoff MPL ID Season 13\",\n",
    "    \"Rumor kebocoran GTA VI trailer kedua\",\n",
    "    \"Review monitor gaming 144Hz terbaik 2025\",\n",
    "    \"Developer Elden Ring siapkan DLC baru\",\n",
    "    \"Game indie horor terbaru di Steam\",\n",
    "    \"Cara mendapatkan skin legend Chou\",\n",
    "\n",
    "    # B. Semantik & Kontekstual (Menguji S-BERT)\n",
    "    \"Permainan video yang bikin kecanduan\",\n",
    "    \"Rekomendasi game yang cocok dimainkan bersama teman\",\n",
    "    \"Apa saja game bertema survival pasca-apokaliptik\",\n",
    "    \"Dampak negatif kecanduan game online pada remaja\",\n",
    "    \"Game dengan grafis paling realistis untuk PS5\",\n",
    "    \"Kisah cerita karakter utama di game RPG\",\n",
    "    \"Game simulasi kehidupan di dunia terbuka (open world)\",\n",
    "    \"Perangkat keras apa yang dibutuhkan untuk VR gaming\",\n",
    "    \"Cara kerja sistem anti-cheat di eSports\",\n",
    "    \"Pengaruh teknologi AI pada pengembangan karakter game\",\n",
    "\n",
    "    # C. Gabungan & Entitas Kompleks\n",
    "    \"Pro player Mobile Legends yang pensiun dini\",\n",
    "    \"Game yang dibatalkan rilis karena masalah internal\",\n",
    "    \"Perbandingan performa Xbox Series X dan PS5\",\n",
    "    \"Alasan mengapa game tertentu mengalami review bombing\",\n",
    "    \"Turnamen esports terbesar di Asia Tenggara\"\n",
    "]\n",
    "\n",
    "GROUND_TRUTH = {\n",
    "    \"spesifikasi PC untuk bermain game terbaru\": {20, 55, 103, 500},\n",
    "    \"buff hero arlott mobile legends\": {38},\n",
    "    \"review Elden Ring Shadow of the Erdtree\": {10, 11, 12, 13, 14},\n",
    "    \"daftar game RPG terbaik di Android 2025\": {30, 31, 32},\n",
    "    \"trailer game yang akan rilis tahun depan\": {40, 41, 42, 43, 44},\n",
    "    # ... Tambahkan 25 query lainnya dengan SET of relevant indices\n",
    "}\n",
    "\n",
    "# =======================================================\n",
    "# 5. EKSEKUSI EVALUASI UTAMA\n",
    "# =======================================================\n",
    "\n",
    "RESULTS = defaultdict(lambda: defaultdict(list))\n",
    "K_VALUES = [1, 3, 5, 10] # Evaluasi Precision di K=1, K=3, K=5, K=10\n",
    "TOP_N = 50 # Ambil 50 hasil teratas untuk perhitungan Recall dan MAP\n",
    "\n",
    "for query in EVAL_QUERIES:\n",
    "    # Pastikan query ada di ground truth\n",
    "    if query not in GROUND_TRUTH:\n",
    "        print(f\"Melewati query '{query}' karena tidak ada Ground Truth.\")\n",
    "        continue\n",
    "        \n",
    "    true_relevant_set = GROUND_TRUTH[query]\n",
    "    \n",
    "    # Preprocessing query untuk BM25\n",
    "    bm25_query_tokens = str(query).lower().split()\n",
    "    \n",
    "    # 1. Jalankan BM25\n",
    "    bm25_ranked = get_bm25_ranking(bm25_query_tokens, TOP_N)\n",
    "    \n",
    "    # 2. Jalankan S-BERT\n",
    "    sbert_ranked = get_sbert_ranking(query, TOP_N)\n",
    "    \n",
    "    # --- Hitung Metrik ---\n",
    "    for model_name, ranked_list in [('BM25', bm25_ranked), ('S-BERT', sbert_ranked)]:\n",
    "        \n",
    "        # Recall dan MAP dihitung berdasarkan TOP_N (50)\n",
    "        recall = calculate_recall(ranked_list, true_relevant_set)\n",
    "        ap = calculate_average_precision(ranked_list, true_relevant_set)\n",
    "        \n",
    "        RESULTS[model_name]['Recall'].append(recall)\n",
    "        RESULTS[model_name]['MAP'].append(ap)\n",
    "        \n",
    "        # Precision@K\n",
    "        for k in K_VALUES:\n",
    "            precision_k = calculate_precision(ranked_list, true_relevant_set, k)\n",
    "            RESULTS[model_name][f'P@{k}'].append(precision_k)\n",
    "\n",
    "# =======================================================\n",
    "# 6. RINGKASAN HASIL\n",
    "# =======================================================\n",
    "\n",
    "final_metrics = {}\n",
    "for model_name in RESULTS.keys():\n",
    "    summary = {}\n",
    "    for metric, scores in RESULTS[model_name].items():\n",
    "        # Hitung Mean dari semua skor query\n",
    "        summary[f'Mean {metric}'] = np.mean(scores)\n",
    "    final_metrics[model_name] = summary\n",
    "\n",
    "# Konversi ke DataFrame untuk tampilan\n",
    "df_results = pd.DataFrame.from_dict(final_metrics, orient='index')\n",
    "\n",
    "print(\"\\n=======================================================\")\n",
    "print(\"          RINGKASAN EVALUASI (BM25 vs S-BERT)          \")\n",
    "print(\"=======================================================\")\n",
    "# Tampilkan hasil\n",
    "df_styled = df_results.style.format(\"{:.4f}\")\n",
    "print(df_styled.to_string())\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/scraped_articles.csv')\n",
    "\n",
    "df.head(50)[['id_dokumen', 'judul']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a744f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from tqdm.auto import tqdm\n",
    "# import re\n",
    "\n",
    "# # =======================================================\n",
    "# # 1. PATH DAN MUAT ASET MODEL\n",
    "# # =======================================================\n",
    "\n",
    "# MODEL_DIR = '../models/'\n",
    "# BM25_MODEL_PATH = os.path.join(MODEL_DIR, 'bm25_model.pkl')\n",
    "# CORPUS_DF_PATH = os.path.join(MODEL_DIR, 'df_corpus.pkl')\n",
    "# SBERT_EMBEDDINGS_PATH = os.path.join(MODEL_DIR, 'sbert_embeddings.npy')\n",
    "# SBERT_MODEL_PATH = os.path.join(MODEL_DIR, 'sbert_model.pkl')\n",
    "\n",
    "# ASSETS = {}\n",
    "# SBERT_MODEL = None\n",
    "# BM25_MODEL = None\n",
    "# CORPUS_DF = None\n",
    "# SBERT_EMBEDDINGS = None\n",
    "\n",
    "# def load_search_assets():\n",
    "#     \"\"\"Memuat semua aset indexing yang sudah diekspor.\"\"\"\n",
    "#     global SBERT_MODEL, BM25_MODEL, CORPUS_DF, SBERT_EMBEDDINGS, ASSETS\n",
    "#     try:\n",
    "#         # Muat Aset BM25\n",
    "#         with open(BM25_MODEL_PATH, 'rb') as f:\n",
    "#             BM25_MODEL = pickle.load(f)\n",
    "#         ASSETS['corpus_df'] = pd.read_pickle(CORPUS_DF_PATH)\n",
    "#         CORPUS_DF = ASSETS['corpus_df']\n",
    "        \n",
    "#         # Muat Aset S-BERT\n",
    "#         SBERT_EMBEDDINGS = np.load(SBERT_EMBEDDINGS_PATH)\n",
    "#         with open(SBERT_MODEL_PATH, 'rb') as f:\n",
    "#             model_name = pickle.load(f)\n",
    "#         SBERT_MODEL = SentenceTransformer(model_name)\n",
    "        \n",
    "#         print(\"✅ Semua aset berhasil dimuat untuk pengujian IR.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ ERROR saat memuat aset: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Memuat aset\n",
    "# try:\n",
    "#     load_search_assets()\n",
    "# except Exception:\n",
    "#     print(\"Script dihentikan karena kegagalan memuat model/data.\")\n",
    "#     exit()\n",
    "\n",
    "# # Kolom konten yang diproses untuk BM25 (perlu tokenized list)\n",
    "# TOKENIZED_CORPUS = CORPUS_DF['konten_processed'].apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "# # =======================================================\n",
    "# # 2. DEFINISI FUNGSI PENCARIAN\n",
    "# # =======================================================\n",
    "\n",
    "# def search_bm25(query_tokens, top_k=5):\n",
    "#     \"\"\"Fungsi pencarian menggunakan BM25, menggunakan model yang sudah dimuat.\"\"\"\n",
    "    \n",
    "#     # Preprocessing Query (di sini diasumsikan query_tokens sudah lowercase dan terpisah)\n",
    "    \n",
    "#     bm25_model = BM25_MODEL\n",
    "    \n",
    "#     doc_scores = bm25_model.get_scores(query_tokens)\n",
    "#     ranked_indices = np.argsort(doc_scores)[::-1]\n",
    "\n",
    "#     # Akses DataFrame menggunakan iloc dengan array index list\n",
    "#     results = CORPUS_DF.iloc[ranked_indices[:top_k]].copy() \n",
    "#     results['score'] = doc_scores[ranked_indices[:top_k]]\n",
    "#     results['algorithm'] = 'BM25'\n",
    "\n",
    "#     return results\n",
    "\n",
    "# def search_sbert(query, top_k=5):\n",
    "#     \"\"\"Fungsi pencarian berbasis semantik menggunakan S-BERT.\"\"\"\n",
    "    \n",
    "#     query_embedding = SBERT_MODEL.encode(query, convert_to_tensor=False)\n",
    "    \n",
    "#     # Hitung Cosine Similarity\n",
    "#     similarities = cosine_similarity(query_embedding.reshape(1, -1), SBERT_EMBEDDINGS)[0]\n",
    "    \n",
    "#     ranked_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "#     results = CORPUS_DF.iloc[ranked_indices[:top_k]].copy()\n",
    "#     results['score'] = similarities[ranked_indices[:top_k]]\n",
    "#     results['algorithm'] = 'S-BERT'\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # =======================================================\n",
    "# # 3. UJI COBA DAN PERBANDINGAN\n",
    "# # =======================================================\n",
    "\n",
    "# # Daftar Query Uji Coba (Ambil 10 Query yang Representatif)\n",
    "# EVAL_QUERIES = [\n",
    "#     \"build arlott mobile legends\", \n",
    "#     \"game yang mirip final fantasy dan elden ring\", \n",
    "#     \"spesifikasi minimum pc untuk rtx 4080\",\n",
    "#     \"game survival open world\",\n",
    "#     \"trailer sekiro shadows die twice\",\n",
    "#     \"pengumuman game di gamescom\"\n",
    "# ]\n",
    "\n",
    "# TOP_K = 5 # Menampilkan Top 5 hasil\n",
    "\n",
    "# for query in EVAL_QUERIES:\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"🔎 QUERY: {query}\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Preprocessing query untuk BM25\n",
    "#     bm25_query_tokens = str(query).lower().split()\n",
    "    \n",
    "#     # --- Uji Coba BM25 ---\n",
    "#     # 💡 KOREKSI: Hanya lewati query_tokens dan top_k\n",
    "#     bm25_results = search_bm25(bm25_query_tokens, top_k=TOP_K) \n",
    "#     print(\"\\n--- HASIL BM25 (Literal) ---\")\n",
    "#     bm25_results['rank'] = range(1, TOP_K + 1)\n",
    "    \n",
    "#     display_bm25 = bm25_results[['rank', 'judul', 'sumber', 'score']].style.format({\"score\": \"{:.4f}\"})\n",
    "#     print(display_bm25.to_string())\n",
    "\n",
    "#     # --- Uji Coba S-BERT ---\n",
    "#     # 💡 KOREKSI: Hanya lewati query string dan top_k\n",
    "#     sbert_results = search_sbert(query, top_k=TOP_K)\n",
    "#     print(\"\\n--- HASIL S-BERT (Semantik) ---\")\n",
    "#     sbert_results['rank'] = range(1, TOP_K + 1)\n",
    "\n",
    "#     display_sbert = sbert_results[['rank', 'judul', 'sumber', 'score']].style.format({\"score\": \"{:.4f}\"})\n",
    "#     print(display_sbert.to_string())\n",
    "    \n",
    "#     print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5243e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ queries.csv dibuat\n",
      "✔ groundtruth.csv dibuat\n",
      "Selesai — 50 query dan ground truth siap dipakai evaluasi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Query Generator (50 queries campuran Indo + Inggris)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "GAME_TERMS = [\n",
    "    \"ps5\", \"playstation\", \"xbox\", \"nintendo switch\", \"genshin impact\",\n",
    "    \"mlbb\", \"mobile legends\", \"ff\", \"free fire\", \"valorant\", \"dota 2\",\n",
    "    \"fifa\", \"efootball\", \"pubg mobile\", \"elden ring\", \"gta 6\", \"minecraft\"\n",
    "]\n",
    "\n",
    "ACTION_TERMS = [\n",
    "    \"harga\", \"review\", \"update\", \"patch\", \"rilis\", \"release date\", \n",
    "    \"turnamen\", \"event\", \"bug\", \"fix\", \"leak\", \"specs\", \"performance\",\n",
    "    \"benchmark\", \"fitur\", \"hero\", \"character\", \"meta\", \"new mode\"\n",
    "]\n",
    "\n",
    "CONTEXT_TERMS = [\n",
    "    \"indonesia\", \"global\", \"2024\", \"2025\", \"official\", \n",
    "    \"trailer\", \"news\", \"info\", \"ranking\"\n",
    "]\n",
    "\n",
    "def generate_query():\n",
    "    a = random.choice(ACTION_TERMS)\n",
    "    b = random.choice(GAME_TERMS)\n",
    "    c = random.choice(CONTEXT_TERMS)\n",
    "    return f\"{a} {b} {c}\".strip()\n",
    "\n",
    "QUERIES = [generate_query() for _ in range(50)]\n",
    "\n",
    "queries_df = pd.DataFrame({\n",
    "    \"query_id\": [f\"Q{str(i+1).zfill(3)}\" for i in range(50)],\n",
    "    \"query\": QUERIES\n",
    "})\n",
    "\n",
    "queries_df.to_csv(\"queries.csv\", index=False)\n",
    "print(\"✔ queries.csv dibuat\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. SEARCH FUNCTIONS (pakai fungsi BM25 & SBERT milik kamu)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def search_bm25(query, top_k=30):\n",
    "    tokens = query.lower().split()\n",
    "    bm25_model = BM25_MODEL\n",
    "    \n",
    "    scores = bm25_model.get_scores(tokens)\n",
    "    ranked = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    df = CORPUS_DF.iloc[ranked].copy()\n",
    "    df[\"score_bm25\"] = scores[ranked]\n",
    "    return df, ranked, scores\n",
    "\n",
    "def search_sbert(query, top_k=30):\n",
    "    emb = SBERT_MODEL.encode(query, convert_to_tensor=False)\n",
    "    sims = cosine_similarity(emb.reshape(1,-1), SBERT_EMBEDDINGS)[0]\n",
    "    ranked = np.argsort(sims)[::-1][:top_k]\n",
    "    \n",
    "    df = CORPUS_DF.iloc[ranked].copy()\n",
    "    df[\"score_sbert\"] = sims[ranked]\n",
    "    return df, ranked, sims\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. GROUNDTRUTH BUILDER (top-10 relevan)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "records = []\n",
    "\n",
    "for qid, qtext in zip(queries_df[\"query_id\"], queries_df[\"query\"]):\n",
    "    \n",
    "    bm25_df, bm25_idx, bm25_scores = search_bm25(qtext, top_k=30)\n",
    "    sbert_df, sbert_idx, sbert_scores = search_sbert(qtext, top_k=30)\n",
    "    \n",
    "    # Gabungkan pool\n",
    "    union_idx = list(dict.fromkeys(list(bm25_idx) + list(sbert_idx)))\n",
    "    \n",
    "    # Hitung skor gabungan normalisasi 0-1\n",
    "    bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)\n",
    "    sbert_norm = (sbert_scores - sbert_scores.min()) / (sbert_scores.max() - sbert_scores.min() + 1e-9)\n",
    "    \n",
    "    combined_scores = {\n",
    "        idx: (bm25_norm[idx] + sbert_norm[idx]) / 2\n",
    "        for idx in union_idx\n",
    "    }\n",
    "    \n",
    "    # Pilih 10 relevan\n",
    "    top10 = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top10_ids = set([x[0] for x in top10])\n",
    "    \n",
    "    # Isi groundtruth (biner)\n",
    "    for idx in union_idx:\n",
    "        doc_id = CORPUS_DF.iloc[idx][\"id_dokumen\"]\n",
    "        relevance = 1 if idx in top10_ids else 0\n",
    "        \n",
    "        records.append({\n",
    "            \"query_id\": qid,\n",
    "            \"doc_index\": int(idx),\n",
    "            \"doc_id\": doc_id,\n",
    "            \"relevance\": relevance\n",
    "        })\n",
    "\n",
    "groundtruth_df = pd.DataFrame(records)\n",
    "groundtruth_df.to_csv(\"groundtruth.csv\", index=False)\n",
    "\n",
    "print(\"✔ groundtruth.csv dibuat\")\n",
    "print(\"Selesai — 50 query dan ground truth siap dipakai evaluasi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
